{"function": "    def handle(self, **options):\n        everyone = Group.objects.get(name='everyone')\n        anonymous = User.objects.get(pk=settings.ANONYMOUS_USER_ID)\n\n        for x in Activity.objects.all():\n            if not x.admin_only:\n                assign_perm('newsfeed.read_activity', anonymous, x)\n                assign_perm('newsfeed.read_activity', everyone, x)", "label": 1}
{"function": "def _resize_image(im, width, height, padding):\n    # resize\n    h = im.shape[0]\n    w = im.shape[1]\n    if w != width or h != height:\n        # resize image\n        if not padding:\n            # trimming mode\n            if float(h) / w > float(height) / width:\n                target_h = int(float(w) / width * height)\n                im = im[(h - target_h) // 2:h -\n                        (h - target_h) // 2, ::]\n            else:\n                target_w = int(float(h) / height * width)\n                im = im[::, (w - target_w) // 2:w -\n                        (w - target_w) // 2]\n        else:\n            # padding mode\n            if float(h) / w < float(height) / width:\n                target_h = int(float(height) / width * w)\n                pad = (((target_h - h) // 2, target_h -\n                        (target_h - h) // 2 - h), (0, 0))\n            else:\n                target_w = int(float(width) / height * h)\n                pad = ((0, 0), ((target_w - w) // 2,\n                                target_w - (target_w - w) // 2 - w))\n            pad = pad + ((0, 0),)\n            im = np.pad(im, pad, 'constant')\n        im = imresize(im, (width, height))\n\n    x = np.array(im, dtype=np.uint8).transpose((2, 0, 1))\n    return x", "label": 1}
{"function": "def _create_train_cache(archive, output, names, args):\n    # Read label and wordnet_id\n    wid2ind = np.loadtxt(fname=LABEL_WORDNETID, dtype=object, delimiter=\",\")\n\n    def _get_label(wordnet_id):\n        for item in wid2ind:\n            if item[1] == wordnet_id:\n                return item[0]\n\n    images0 = []\n    print(\"Count image in TAR\")\n    pbar = tqdm.tqdm(total=len(names), unit='%')\n    for name in names:\n        category = os.path.splitext(name)[0]\n        marchive = tarfile.open(fileobj=archive.extractfile(name))\n        for mname in marchive.getnames():\n            if re.match(r'{}_[0-9]+\\.JPEG'.format(category), mname):\n                images0.append((_get_label(name[:9]), name, marchive, mname))\n            else:\n                print('Invalid file {} includes in tar file'.format(mname))\n                exit(-1)\n        pbar.update(1)\n    pbar.close()\n\n    # Thinning\n    images = []\n    for i, image in enumerate(images0):\n        if i % args.thinning == 0:\n            images.append(image)\n\n    def _load_func(index):\n        y, name, marchive, mname = images[index]\n        im = imread(marchive.extractfile(mname), num_channels=3)\n        x = _resize_image(im, args.width, args.height, args.mode == 'padding')\n        return x, np.array([y]).astype(np.int32)\n\n    from nnabla.utils.data_source import DataSourceWithFileCache\n    from nnabla.utils.data_source_implements import SimpleDataSource\n    from nnabla.logger import logger\n\n    logger.info('Num of data : {}'.format(len(images)))\n    shuffle = True\n    if args.shuffle == 'False':\n        shuffle = False\n    source = SimpleDataSource(_load_func, len(images), shuffle, rng=None)\n    DataSourceWithFileCache(\n        source, cache_dir=output, shuffle=args.shuffle)", "label": 1}
{"function": "def _create_validation_cache(archive, output, names, ground_truth, args):\n    images0 = sorted(names)\n\n    # Thinning\n    images = []\n    for i, image in enumerate(images0):\n        if i % args.thinning == 0:\n            images.append(image)\n\n    def _load_func(index):\n        y, name = ground_truth[index], images[index]\n        im = imread(archive.extractfile(name), num_channels=3)\n        x = _resize_image(im, args.width, args.height, args.mode == 'padding')\n        return x, np.array([y]).astype(np.int32)\n\n    from nnabla.utils.data_source import DataSourceWithFileCache\n    from nnabla.utils.data_source_implements import SimpleDataSource\n    from nnabla.logger import logger\n\n    logger.info('Num of data : {}'.format(len(images)))\n    shuffle = False\n    if args.shuffle == 'True':\n        shuffle = True\n    source = SimpleDataSource(_load_func, len(images), shuffle, rng=None)\n    DataSourceWithFileCache(\n        source, cache_dir=output, shuffle=args.shuffle)", "label": 1}
{"function": "def _progress(state, progress=0.0):\n    global _pbar\n    global _prev_progress\n\n    if state is None:\n        if _pbar is not None:\n            _pbar.close()\n        _pbar = None\n        _prev_progress = None\n    else:\n        if _pbar is None:\n            _pbar = tqdm.tqdm(desc=state, total=100, unit='%')\n        else:\n            if _prev_progress is None:\n                _prev_progress = 0\n            update = int((progress - _prev_progress) * 100)\n            if update > 0:\n                _pbar.update(update)\n                _prev_progress = progress", "label": 1}
{"function": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('input', type=str, nargs='+',\n                        help='Source file or directory.')\n    parser.add_argument('output', type=str,\n                        help='Destination directory.')\n    parser.add_argument('-W', '--width', type=int, default=320,\n                        help='width of output image (default:320)')\n    parser.add_argument('-H', '--height', type=int, default=320,\n                        help='height of output image (default:320)')\n    parser.add_argument('-m', '--mode', default='trimming',\n                        choices=['trimming', 'padding'],\n                        help='shaping mode (trimming or padding)  (default:trimming)')\n    parser.add_argument('-S', '--shuffle', choices=['True', 'False'],\n                        help='shuffle mode if not specified, train:True, val:False.' +\n                        ' Otherwise specified value will be used for both.')\n    parser.add_argument('-N', '--file-cache-size', type=int, default=100,\n                        help='num of data in cache file (default:100)')\n    parser.add_argument('-C', '--cache-type', default='npy',\n                        choices=['h5', 'npy'],\n                        help='cache format (h5 or npy) (default:npy)')\n    parser.add_argument('--thinning', type=int, default=1,\n                        help='Thinning rate')\n\n    args = parser.parse_args()\n    ############################################################################\n    # Analyze tar\n    # If it consists only of members corresponding to regular expression\n    # 'n[0-9]{8}\\.tar', it is judged as train data archive.\n    # If it consists only of members corresponding to regular expression\n    # 'ILSVRC2012_val_[0-9]{8}\\.JPEG', it is judged as validation data archive.\n\n    archives = {'train': None, 'val': None}\n    for inputarg in args.input:\n        print('Checking input file [{}]'.format(inputarg))\n        archive = tarfile.open(inputarg)\n        is_train = False\n        is_val = False\n        names = []\n        for name in archive.getnames():\n            if re.match(r'n[0-9]{8}\\.tar', name):\n                if is_val:\n                    print('Train data {} includes in validation tar'.format(name))\n                    exit(-1)\n                is_train = True\n            elif re.match(r'ILSVRC2012_val_[0-9]{8}\\.JPEG', name):\n                if is_train:\n                    print('Validation data {} includes in train tar'.format(name))\n                    exit(-1)\n                is_val = True\n            else:\n                print('Invalid member {} includes in tar file'.format(name))\n                exit(-1)\n            names.append(name)\n        if is_train:\n            if archives['train'] is None:\n                archives['train'] = (archive, names)\n            else:\n                print('Please specify only 1 training tar archive.')\n                exit(-1)\n        if is_val:\n            if archives['val'] is None:\n                archives['val'] = (archive, names)\n            else:\n                print('Please specify only 1 validation tar archive.')\n                exit(-1)\n\n    # Read label of validation data, (Use ascending label of wordnet_id)\n    validation_ground_truth = []\n    g_file = VALIDATION_DATA_LABEL\n    with open(g_file, 'r') as f:\n        for l in f.readlines():\n            validation_ground_truth.append(int(l.rstrip()))\n\n    ############################################################################\n    # Prepare logging\n    tmpdir = tempfile.mkdtemp()\n    logfilename = os.path.join(tmpdir, 'nnabla.log')\n\n    # Temporarily chdir to tmpdir just before importing nnabla to reflect nnabla.conf.\n    cwd = os.getcwd()\n    os.chdir(tmpdir)\n    with open('nnabla.conf', 'w') as f:\n        f.write('[LOG]\\n')\n        f.write('log_file_name = {}\\n'.format(logfilename))\n        f.write('log_file_format = %(funcName)s : %(message)s\\n')\n        f.write('log_console_level = CRITICAL\\n')\n\n    from nnabla.config import nnabla_config\n    os.chdir(cwd)\n\n    ############################################################################\n    # Data iterator setting\n    nnabla_config.set('DATA_ITERATOR',\n                      'cache_file_format', '.' + args.cache_type)\n    nnabla_config.set('DATA_ITERATOR',\n                      'data_source_file_cache_size', str(args.file_cache_size))\n    nnabla_config.set('DATA_ITERATOR',\n                      'data_source_file_cache_num_of_threads', '1')\n\n    if not os.path.isdir(args.output):\n        os.makedirs(args.output)\n\n    ############################################################################\n    # Prepare status monitor\n    from nnabla.utils.progress import configure_progress\n    configure_progress(None, _progress)\n\n    ############################################################################\n    # Converter\n\n    try:\n        if archives['train'] is not None:\n            from nnabla.logger import logger\n            logger.info('StartCreatingCache')\n            archive, names = archives['train']\n            output = os.path.join(args.output, 'train')\n            if not os.path.isdir(output):\n                os.makedirs(output)\n            _create_train_cache(archive, output, names, args)\n        if archives['val'] is not None:\n            from nnabla.logger import logger\n            logger.info('StartCreatingCache')\n            archive, names = archives['val']\n            output = os.path.join(args.output, 'val')\n            if not os.path.isdir(output):\n                os.makedirs(output)\n            _create_validation_cache(\n                archive, output, names, validation_ground_truth, args)\n    except KeyboardInterrupt:\n        shutil.rmtree(tmpdir, ignore_errors=True)\n\n        # Even if CTRL-C is pressed, it does not stop if there is a running\n        # thread, so it sending a signal to itself.\n        os.kill(os.getpid(), 9)\n\n    ############################################################################\n    # Finish\n    _finish = True\n    shutil.rmtree(tmpdir, ignore_errors=True)", "label": 1}
{"function": "    def _get_label(wordnet_id):\n        for item in wid2ind:\n            if item[1] == wordnet_id:\n                return item[0]", "label": 1}
{"function": "    def _load_func(index):\n        y, name, marchive, mname = images[index]\n        im = imread(marchive.extractfile(mname), num_channels=3)\n        x = _resize_image(im, args.width, args.height, args.mode == 'padding')\n        return x, np.array([y]).astype(np.int32)", "label": 1}
{"function": "    def _load_func(index):\n        y, name = ground_truth[index], images[index]\n        im = imread(archive.extractfile(name), num_channels=3)\n        x = _resize_image(im, args.width, args.height, args.mode == 'padding')\n        return x, np.array([y]).astype(np.int32)", "label": 1}
{"function": "def mixer():\n    from mixer.backend.marshmallow import Mixer\n    return Mixer(required=True)", "label": 1}
{"function": "def normalize_image(x):\n    \"\"\"Rescale image pixels to span range [0, 1]\n    \"\"\"\n    ma = float(x.max().cpu().data)\n    mi = float(x.min().cpu().data)\n    d = ma - mi if ma != mi else 1e5\n    return (x - mi) / d", "label": 1}
{"function": "def sec_to_hm(t):\n    \"\"\"Convert time in seconds to time in hours, minutes and seconds\n    e.g. 10239 -> (2, 50, 39)\n    \"\"\"\n    t = int(t)\n    s = t % 60\n    t //= 60\n    m = t % 60\n    t //= 60\n    return t, m, s", "label": 1}
{"function": "def sec_to_hm_str(t):\n    \"\"\"Convert time in seconds to a nice string\n    e.g. 10239 -> '02h50m39s'\n    \"\"\"\n    h, m, s = sec_to_hm(t)\n    return \"{:02d}h{:02d}m{:02d}s\".format(h, m, s)", "label": 1}
{"function": "def download_model_if_doesnt_exist(model_name):\n    \"\"\"If pretrained kitti model doesn't exist, download and unzip it\n    \"\"\"\n    # values are tuples of (<google cloud URL>, <md5 checksum>)\n    download_paths = {\n        \"mono_640x192\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip\",\n             \"a964b8356e08a02d009609d9e3928f7c\"),\n        \"stereo_640x192\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip\",\n             \"3dfb76bcff0786e4ec07ac00f658dd07\"),\n        \"mono+stereo_640x192\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip\",\n             \"c024d69012485ed05d7eaa9617a96b81\"),\n        \"mono_no_pt_640x192\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip\",\n             \"9c2f071e35027c895a4728358ffc913a\"),\n        \"stereo_no_pt_640x192\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip\",\n             \"41ec2de112905f85541ac33a854742d1\"),\n        \"mono+stereo_no_pt_640x192\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip\",\n             \"46c3b824f541d143a45c37df65fbab0a\"),\n        \"mono_1024x320\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip\",\n             \"0ab0766efdfeea89a0d9ea8ba90e1e63\"),\n        \"stereo_1024x320\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip\",\n             \"afc2f2126d70cf3fdf26b550898b501a\"),\n        \"mono+stereo_1024x320\":\n            (\"https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip\",\n             \"cdc5fc9b23513c07d5b19235d9ef08f7\"),\n        }\n\n    if not os.path.exists(\"models\"):\n        os.makedirs(\"models\")\n\n    model_path = os.path.join(\"models\", model_name)\n\n    def check_file_matches_md5(checksum, fpath):\n        if not os.path.exists(fpath):\n            return False\n        with open(fpath, 'rb') as f:\n            current_md5checksum = hashlib.md5(f.read()).hexdigest()\n        return current_md5checksum == checksum\n\n    # see if we have the model already downloaded...\n    if not os.path.exists(os.path.join(model_path, \"encoder.pth\")):\n\n        model_url, required_md5checksum = download_paths[model_name]\n\n        if not check_file_matches_md5(required_md5checksum, model_path + \".zip\"):\n            print(\"-> Downloading pretrained model to {}\".format(model_path + \".zip\"))\n            urllib.request.urlretrieve(model_url, model_path + \".zip\")\n\n        if not check_file_matches_md5(required_md5checksum, model_path + \".zip\"):\n            print(\"   Failed to download a file which matches the checksum - quitting\")\n            quit()\n\n        print(\"   Unzipping model...\")\n        with zipfile.ZipFile(model_path + \".zip\", 'r') as f:\n            f.extractall(model_path)\n\n        print(\"   Model unzipped to {}\".format(model_path))", "label": 1}
{"function": "    def check_file_matches_md5(checksum, fpath):\n        if not os.path.exists(fpath):\n            return False\n        with open(fpath, 'rb') as f:\n            current_md5checksum = hashlib.md5(f.read()).hexdigest()\n        return current_md5checksum == checksum", "label": 1}
{"function": "    def setUp(self):\n        super().setUp()\n        self.server = TaskResourcesMixin()\n        self.server.sessions = {}\n        self.server.resource_handshakes = {}\n        self.server.client = self.client\n        self.server.node = LocalNode(**dt_p2p_factory.Node().to_dict())\n        self.server.resource_manager.storage.get_dir.return_value = self.tempdir\n        self.ecc = cryptography.ECCx(None)\n        self.task_id = msg_helpers.fake_golem_uuid(self.public_key)", "label": 1}
{"function": "    def public_key(self):\n        return msg_utils.encode_hex(self.ecc.raw_pubkey)", "label": 1}
{"function": "    def key_id(self):\n        return self.public_key[2:]", "label": 1}
{"function": "    def test_start_handshake(self, mock_share, mock_timer, *_):\n        self.server.start_handshake(\n            key_id=self.key_id,\n            task_id=self.task_id,\n        )\n        self.assertIn(\n            self.key_id,\n            self.server.resource_handshakes,\n        )\n        mock_timer.assert_called_once_with(self.key_id)\n        mock_share.assert_called_once_with(self.key_id)", "label": 1}
{"function": "    def test_start_handshake_nonce_callback(self, mock_queue, *_):\n        deferred = Deferred()\n        self.server.resource_manager.add_file.return_value = deferred\n        self.server.start_handshake(\n            key_id=self.key_id,\n            task_id=self.task_id,\n        )\n\n        exception = False\n\n        def exception_on_error(error):\n            nonlocal exception\n            exception = error\n\n        deferred.addErrback(exception_on_error)\n        deferred.callback(('result', None))\n\n        if exception:\n            raise Exception(exception)\n\n        mock_queue.assert_called_once_with(\n            node_id=self.key_id, msg=mock.ANY, timeout=mock.ANY)", "label": 1}
{"function": "        def _seed_current_object(current_value):\n            if isinstance(current_value, int):  # acceptable behaviour\n                return False\n            elif isinstance(current_value, np.random.RandomState):\n                raise ValueError(\n                    \"Models initialized with a RandomState object are not \"\n                    \"supported. Please seed with an integer. \"\n                )\n            elif current_value is not None:\n                raise ValueError(\n                    \"Models should be seeded with int or None (this should never \" \"happen). \"\n                )\n            else:\n                return True", "label": 1}
{"function": "        def _prediction_to_probabilities(y: np.ndarray, classes: List[Any]) -> np.ndarray:\n            \"\"\"Transforms predicted probabilities to match with OpenML class indices.\n\n            Parameters\n            ----------\n            y : np.ndarray\n                Predicted probabilities (possibly omitting classes if they were not present in the\n                training data).\n            model_classes : list\n                List of classes known_predicted by the model, ordered by their index.\n\n            Returns\n            -------\n            np.ndarray\n            \"\"\"\n            # y: list or numpy array of predictions\n            # model_classes: sklearn classifier mapping from original array id to\n            # prediction index id\n            if not isinstance(classes, list):\n                raise ValueError(\"please convert model classes to list prior to \" \"calling this fn\")\n            result = np.zeros((len(y), len(classes)), dtype=np.float32)\n            for obs, prediction_idx in enumerate(y):\n                result[obs][prediction_idx] = 1.0\n            return result", "label": 1}
{"function": "        def get_flow_dict(_flow):\n            flow_map = {_flow.name: _flow.flow_id}\n            for subflow in _flow.components:\n                flow_map.update(get_flow_dict(_flow.components[subflow]))\n            return flow_map", "label": 1}
{"function": "        def extract_parameters(_flow, _flow_dict, component_model, _main_call=False, main_id=None):\n            def is_subcomponent_specification(values):\n                # checks whether the current value can be a specification of\n                # subcomponents, as for example the value for steps parameter\n                # (in Pipeline) or transformers parameter (in\n                # ColumnTransformer). These are always lists/tuples of lists/\n                # tuples, size bigger than 2 and an OpenMLFlow item involved.\n                if not isinstance(values, (tuple, list)):\n                    return False\n                for item in values:\n                    if not isinstance(item, (tuple, list)):\n                        return False\n                    if len(item) < 2:\n                        return False\n                    if not isinstance(item[1], (openml.flows.OpenMLFlow, str)):\n                        if (\n                            isinstance(item[1], str)\n                            and item[1] in SKLEARN_PIPELINE_STRING_COMPONENTS\n                        ):\n                            pass\n                        else:\n                            return False\n                return True\n\n            # _flow is openml flow object, _param dict maps from flow name to flow\n            # id for the main call, the param dict can be overridden (useful for\n            # unit tests / sentinels) this way, for flows without subflows we do\n            # not have to rely on _flow_dict\n            exp_parameters = set(_flow.parameters)\n            if (\n                isinstance(component_model, str)\n                and component_model in SKLEARN_PIPELINE_STRING_COMPONENTS\n            ):\n                model_parameters = set()\n            else:\n                model_parameters = set([mp for mp in component_model.get_params(deep=False)])\n            if len(exp_parameters.symmetric_difference(model_parameters)) != 0:\n                flow_params = sorted(exp_parameters)\n                model_params = sorted(model_parameters)\n                raise ValueError(\n                    \"Parameters of the model do not match the \"\n                    \"parameters expected by the \"\n                    \"flow:\\nexpected flow parameters: \"\n                    \"%s\\nmodel parameters: %s\" % (flow_params, model_params)\n                )\n            exp_components = set(_flow.components)\n            if (\n                isinstance(component_model, str)\n                and component_model in SKLEARN_PIPELINE_STRING_COMPONENTS\n            ):\n                model_components = set()\n            else:\n                _ = set([mp for mp in component_model.get_params(deep=False)])\n                model_components = set(\n                    [\n                        mp\n                        for mp in component_model.get_params(deep=True)\n                        if \"__\" not in mp and mp not in _\n                    ]\n                )\n            if len(exp_components.symmetric_difference(model_components)) != 0:\n                is_problem = True\n                if len(exp_components - model_components) > 0:\n                    # If an expected component is not returned as a component by get_params(),\n                    # this means that it is also a parameter -> we need to check that this is\n                    # actually the case\n                    difference = exp_components - model_components\n                    component_in_model_parameters = []\n                    for component in difference:\n                        if component in model_parameters:\n                            component_in_model_parameters.append(True)\n                        else:\n                            component_in_model_parameters.append(False)\n                    is_problem = not all(component_in_model_parameters)\n                if is_problem:\n                    flow_components = sorted(exp_components)\n                    model_components = sorted(model_components)\n                    raise ValueError(\n                        \"Subcomponents of the model do not match the \"\n                        \"parameters expected by the \"\n                        \"flow:\\nexpected flow subcomponents: \"\n                        \"%s\\nmodel subcomponents: %s\" % (flow_components, model_components)\n                    )\n\n            _params = []\n            for _param_name in _flow.parameters:\n                _current = OrderedDict()\n                _current[\"oml:name\"] = _param_name\n\n                current_param_values = self.model_to_flow(component_model.get_params()[_param_name])\n\n                # Try to filter out components (a.k.a. subflows) which are\n                # handled further down in the code (by recursively calling\n                # this function)!\n                if isinstance(current_param_values, openml.flows.OpenMLFlow):\n                    continue\n\n                if is_subcomponent_specification(current_param_values):\n                    # complex parameter value, with subcomponents\n                    parsed_values = list()\n                    for subcomponent in current_param_values:\n                        # scikit-learn stores usually tuples in the form\n                        # (name (str), subcomponent (mixed), argument\n                        # (mixed)). OpenML replaces the subcomponent by an\n                        # OpenMLFlow object.\n                        if len(subcomponent) < 2 or len(subcomponent) > 3:\n                            raise ValueError(\"Component reference should be \" \"size {2,3}. \")\n\n                        subcomponent_identifier = subcomponent[0]\n                        subcomponent_flow = subcomponent[1]\n                        if not isinstance(subcomponent_identifier, str):\n                            raise TypeError(\n                                \"Subcomponent identifier should be of type string, \"\n                                \"but is {}\".format(type(subcomponent_identifier))\n                            )\n                        if not isinstance(subcomponent_flow, (openml.flows.OpenMLFlow, str)):\n                            if (\n                                isinstance(subcomponent_flow, str)\n                                and subcomponent_flow in SKLEARN_PIPELINE_STRING_COMPONENTS\n                            ):\n                                pass\n                            else:\n                                raise TypeError(\n                                    \"Subcomponent flow should be of type flow, but is {}\".format(\n                                        type(subcomponent_flow)\n                                    )\n                                )\n\n                        current = {\n                            \"oml-python:serialized_object\": COMPONENT_REFERENCE,\n                            \"value\": {\n                                \"key\": subcomponent_identifier,\n                                \"step_name\": subcomponent_identifier,\n                            },\n                        }\n                        if len(subcomponent) == 3:\n                            if not isinstance(subcomponent[2], list) and not isinstance(\n                                subcomponent[2], OrderedDict\n                            ):\n                                raise TypeError(\n                                    \"Subcomponent argument should be list or OrderedDict\"\n                                )\n                            current[\"value\"][\"argument_1\"] = subcomponent[2]\n                        parsed_values.append(current)\n                    parsed_values = json.dumps(parsed_values)\n                else:\n                    # vanilla parameter value\n                    parsed_values = json.dumps(current_param_values)\n\n                _current[\"oml:value\"] = parsed_values\n                if _main_call:\n                    _current[\"oml:component\"] = main_id\n                else:\n                    _current[\"oml:component\"] = _flow_dict[_flow.name]\n                _params.append(_current)\n\n            for _identifier in _flow.components:\n                subcomponent_model = component_model.get_params()[_identifier]\n                _params.extend(\n                    extract_parameters(\n                        _flow.components[_identifier], _flow_dict, subcomponent_model\n                    )\n                )\n            return _params", "label": 1}
{"function": "            def flatten_all(list_):\n                \"\"\" Flattens arbitrary depth lists of lists (e.g. [[1,2],[3,[1]]] -> [1,2,3,1]). \"\"\"\n                for el in list_:\n                    if isinstance(el, (list, tuple)) and len(el) > 0:\n                        yield from flatten_all(el)\n                    else:\n                        yield el", "label": 1}
{"function": "            def is_subcomponent_specification(values):\n                # checks whether the current value can be a specification of\n                # subcomponents, as for example the value for steps parameter\n                # (in Pipeline) or transformers parameter (in\n                # ColumnTransformer). These are always lists/tuples of lists/\n                # tuples, size bigger than 2 and an OpenMLFlow item involved.\n                if not isinstance(values, (tuple, list)):\n                    return False\n                for item in values:\n                    if not isinstance(item, (tuple, list)):\n                        return False\n                    if len(item) < 2:\n                        return False\n                    if not isinstance(item[1], (openml.flows.OpenMLFlow, str)):\n                        if (\n                            isinstance(item[1], str)\n                            and item[1] in SKLEARN_PIPELINE_STRING_COMPONENTS\n                        ):\n                            pass\n                        else:\n                            return False\n                return True", "label": 1}
{"function": "def test_mammals():\n  \"\"\"Test module mammals.py by downloading\n   mammals.csv and testing shape of\n   extracted data has 62 rows and 2 columns\n  \"\"\"\n  test_path = tempfile.mkdtemp()\n  x_train, metadata = mammals(test_path)\n  try:\n    assert x_train.shape == (62, 2)\n  except:\n    shutil.rmtree(test_path)\n    raise()", "label": 1}
{"function": "\tdef validate(self):\n\t\tsanctioned_doc = frappe.db.exists('Sanctioned Loan Amount', {'applicant': self.applicant, 'company': self.company})\n\n\t\tif sanctioned_doc and sanctioned_doc != self.name:\n\t\t\tfrappe.throw(_(\"Sanctioned Loan Amount already exists for {0} against company {1}\").format(\n\t\t\t\tfrappe.bold(self.applicant), frappe.bold(self.company)\n\t\t\t))", "label": 1}
{"function": "    def __init__(self, framework):\n        self.framework = framework", "label": 1}
{"function": "    def value(self, t):\n        \"\"\"Generates the value given a timestep (based on schedule's logic).\n\n        Args:\n            t (int): The time step. This could be a tf.Tensor.\n\n        Returns:\n            any: The calculated value depending on the schedule and `t`.\n        \"\"\"\n        if self.framework in [\"tf2\", \"tf\", \"tfe\"]:\n            return self._tf_value_op(t)\n        return self._value(t)", "label": 1}
{"function": "    def cmd_sync(self):\n        \"\"\"Sync the X display. Should only be used for development\"\"\"\n        self.conn.flush()", "label": 1}
{"function": "    def cmd_to_screen(self, n):\n        \"\"\"Warp focus to screen n, where n is a 0-based screen number\n\n        Examples\n        ========\n\n            to_screen(0)\n        \"\"\"\n        return self.focus_screen(n)", "label": 1}
{"function": "    def cmd_next_screen(self):\n        \"\"\"Move to next screen\"\"\"\n        return self.focus_screen(\n            (self.screens.index(self.current_screen) + 1) % len(self.screens)\n        )", "label": 1}
{"function": "    def cmd_prev_screen(self):\n        \"\"\"Move to the previous screen\"\"\"\n        return self.focus_screen(\n            (self.screens.index(self.current_screen) - 1) % len(self.screens)\n        )", "label": 1}
{"function": "    def cmd_windows(self):\n        \"\"\"Return info for each client window\"\"\"\n        return [\n            i.info() for i in self.windows_map.values()\n            if not isinstance(i, window.Internal)\n        ]", "label": 1}
{"function": "    def cmd_internal_windows(self):\n        \"\"\"Return info for each internal window (bars, for example)\"\"\"\n        return [\n            i.info() for i in self.windows_map.values()\n            if isinstance(i, window.Internal)\n        ]", "label": 1}
{"function": "    def cmd_qtile_info(self):\n        \"\"\"Returns a dictionary of info on the Qtile instance\"\"\"\n        return {}", "label": 1}
{"function": "    def cmd_shutdown(self):\n        \"\"\"Quit Qtile\"\"\"\n        self.stop()", "label": 1}
{"function": "    def cmd_switch_groups(self, groupa, groupb):\n        \"\"\"Switch position of groupa to groupb\"\"\"\n        if groupa not in self.groups_map or groupb not in self.groups_map:\n            return\n\n        indexa = self.groups.index(self.groups_map[groupa])\n        indexb = self.groups.index(self.groups_map[groupb])\n\n        self.groups[indexa], self.groups[indexb] = \\\n            self.groups[indexb], self.groups[indexa]\n        hook.fire(\"setgroup\")\n\n        # update window _NET_WM_DESKTOP\n        for group in (self.groups[indexa], self.groups[indexb]):\n            for w in group.windows:\n                w.group = group", "label": 1}
{"function": "    def find_window(self, wid):\n        window = self.windows_map.get(wid)\n        if window:\n            if not window.group.screen:\n                self.current_screen.set_group(window.group)\n            window.group.focus(window, False)", "label": 1}
{"function": "    def alter_column(self, table_name, column_name,\n                        nullable=None,\n                        server_default=False,\n                        name=None,\n                        type_=None,\n                        autoincrement=None,\n                        existing_type=None,\n                        existing_server_default=False,\n                        existing_nullable=None,\n                        existing_autoincrement=None,\n                        schema=None\n    ):\n        \"\"\"Issue an \"alter column\" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        \"do the right thing\", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        \"schema\" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don't support them natively.\n        The ``existing_server_default`` argument is\n        used in this case as well to remove a previous\n        constraint.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column's nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column's default value.\n         Set to ``None`` to have the default removed.\n        :param name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n        :param type_: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column's type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don't otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLlchemy \"schema\" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL's system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        \"\"\"\n\n        compiler = self.impl.dialect.statement_compiler(\n                            self.impl.dialect,\n                            None\n                        )\n        def _count_constraint(constraint):\n            return not isinstance(constraint, sa_schema.PrimaryKeyConstraint) and \\\n                (not constraint._create_rule or\n                    constraint._create_rule(compiler))\n\n        if existing_type and type_:\n            t = self._table(table_name,\n                        sa_schema.Column(column_name, existing_type),\n                        schema=schema\n                    )\n            for constraint in t.constraints:\n                if _count_constraint(constraint):\n                    self.impl.drop_constraint(constraint)\n\n        self.impl.alter_column(table_name, column_name,\n            nullable=nullable,\n            server_default=server_default,\n            name=name,\n            type_=type_,\n            schema=schema,\n            autoincrement=autoincrement,\n            existing_type=existing_type,\n            existing_server_default=existing_server_default,\n            existing_nullable=existing_nullable,\n            existing_autoincrement=existing_autoincrement\n        )\n\n        if type_:\n            t = self._table(table_name,\n                        sa_schema.Column(column_name, type_),\n                        schema=schema\n                    )\n            for constraint in t.constraints:\n                if _count_constraint(constraint):\n                    self.impl.add_constraint(constraint)", "label": 1}
{"function": "    def add_column(self, table_name, column, schema=None):\n        \"\"\"Issue an \"add column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column('organization',\n                Column('name', String())\n            )\n\n        The provided :class:`~sqlalchemy.schema.Column` object can also\n        specify a :class:`~sqlalchemy.schema.ForeignKey`, referencing\n        a remote table name.  Alembic will automatically generate a stub\n        \"referenced\" table and emit a second ALTER statement in order\n        to add the constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column('organization',\n                Column('account_id', INTEGER, ForeignKey('accounts.id'))\n            )\n\n        Note that this statement uses the :class:`~sqlalchemy.schema.Column`\n        construct as is from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the column add\n            op.add_column('account',\n                Column('timestamp', TIMESTAMP, server_default=func.now())\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        \"\"\"\n\n        t = self._table(table_name, column, schema=schema)\n        self.impl.add_column(\n            table_name,\n            column,\n            schema=schema\n        )\n        for constraint in t.constraints:\n            if not isinstance(constraint, sa_schema.PrimaryKeyConstraint):\n                self.impl.add_constraint(constraint)", "label": 1}
{"function": "    def drop_column(self, table_name, column_name, **kw):\n        \"\"\"Issue a \"drop column\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column('organization', 'account_id')\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec's a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec's a separate DROP CONSTRAINT for that default.\n\n        \"\"\"\n\n        self.impl.drop_column(\n            table_name,\n            self._column(column_name, NULLTYPE),\n            **kw\n        )", "label": 1}
{"function": "    def create_foreign_key(self, name, source, referent, local_cols,\n                           remote_cols, onupdate=None, ondelete=None,\n                           source_schema=None, referent_schema=None):\n        \"\"\"Issue a \"create foreign key\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_foreign_key(\n                        \"fk_user_address\", \"address\",\n                        \"user\", [\"user_id\"], [\"id\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the foreign key constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         `NamingConventions <http://www.sqlalchemy.org/trac/wiki/UsageRecipes/NamingConventions>`_,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source: String name of the source table.\n        :param referent: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        \"\"\"\n\n        self.impl.add_constraint(\n                    self._foreign_key_constraint(name, source, referent,\n                            local_cols, remote_cols,\n                            onupdate=onupdate, ondelete=ondelete,\n                            source_schema=source_schema,\n                            referent_schema=referent_schema)\n                )", "label": 1}
{"function": "    def create_unique_constraint(self, name, source, local_cols,\n                                 schema=None, **kw):\n        \"\"\"Issue a \"create unique constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint(\"uq_user_name\", \"user\", [\"name\"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         `NamingConventions <http://www.sqlalchemy.org/trac/wiki/UsageRecipes/NamingConventions>`_,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source: String name of the source table. Dotted schema names are\n         supported.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT DEFERRABLE when\n         issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value> when issuing DDL\n         for this constraint.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        \"\"\"\n\n        self.impl.add_constraint(\n                    self._unique_constraint(name, source, local_cols,\n                        schema=schema, **kw)\n                )", "label": 1}
{"function": "    def create_check_constraint(self, name, source, condition,\n                                schema=None, **kw):\n        \"\"\"Issue a \"create check constraint\" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                \"ck_user_name_len\",\n                \"user\",\n                func.len(column('name')) > 5\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         `NamingConventions <http://www.sqlalchemy.org/trac/wiki/UsageRecipes/NamingConventions>`_,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source: String name of the source table.\n        :param condition: SQL expression that's the condition of the constraint.\n         Can be a string or SQLAlchemy expression language structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT DEFERRABLE when\n         issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value> when issuing DDL\n         for this constraint.\n        :param schema: Optional schema name to operate within.\n\n         ..versionadded:: 0.4.0\n\n        \"\"\"\n        self.impl.add_constraint(\n            self._check_constraint(name, source, condition, schema=schema, **kw)\n        )", "label": 1}
{"function": "    def create_table(self, name, *columns, **kw):\n        \"\"\"Issue a \"create table\" instruction using the current migration context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                'account',\n                Column('id', INTEGER, primary_key=True),\n                Column('name', VARCHAR(50), nullable=False),\n                Column('description', NVARCHAR(200))\n                Column('timestamp', TIMESTAMP, server_default=func.now())\n            )\n\n        Note that :meth:`.create_table` accepts :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify \"DEFAULT NOW\" along with the \"timestamp\" column\n            op.create_table('account',\n                Column('id', INTEGER, primary_key=True),\n                Column('timestamp', TIMESTAMP, server_default=func.now())\n            )\n\n        :param name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        self.impl.create_table(\n            self._table(name, *columns, **kw)\n        )", "label": 1}
{"function": "    def drop_table(self, name, **kw):\n        \"\"\"Issue a \"drop table\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table(\"accounts\")\n\n        :param name: Name of the table\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        \"\"\"\n        self.impl.drop_table(\n            self._table(name, **kw)\n        )", "label": 1}
{"function": "    def create_index(self, name, tablename, columns, schema=None, **kw):\n        \"\"\"Issue a \"create index\" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_index('ik_test', 't1', ['foo', 'bar'])\n\n        :param name: name of the index.\n        :param tablename: name of the owning table.\n        :param columns: a list of string column names in the\n         table.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        \"\"\"\n\n        self.impl.create_index(\n            self._index(name, tablename, columns, schema=schema, **kw)\n        )", "label": 1}
{"function": "    def drop_index(self, name, tablename=None, schema=None):\n        \"\"\"Issue a \"drop index\" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_index(\"accounts\")\n\n        :param name: name of the index.\n        :param tablename: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        \"\"\"\n        # need a dummy column name here since SQLAlchemy\n        # 0.7.6 and further raises on Index with no columns\n        self.impl.drop_index(\n            self._index(name, tablename, ['x'], schema=schema)\n        )", "label": 1}
{"function": "\tdef get_subcontexts(self, path='', get_empty=True):\n\t\t\"\"\" Return a list of Row-contexts that are direct children of the\n\t\tcontext pointed to by `path`. The list doesn't contain contexts that\n\t\thave a \"hidden\" visibility. If `get_empty` is False, then contexts\n\t\tthat have 0 total tasks are excluded from the list. In the list,\n\t\tcontexts are sorted by:\n\t\t * priority, descending\n\t\t * total number of tasks (including tasks in descendance), ascending\n\t\t\"\"\"\n\t\tif get_empty:\n\t\t\tadd_condition = ''\n\t\telse:\n\t\t\tadd_condition = 'AND total_tasks > 0'\n\t\tc = self.connection.cursor()\n\t\tc.execute(\"\"\"\n\t\t\tSELECT c.*, COUNT(own.id) as own_tasks, (\n\t\t\t\tSELECT COUNT(t.id)\n\t\t\t\tFROM Context c1\n\t\t\t\tLEFT JOIN Task t\n\t\t\t\t  ON t.context = c1.id\n\t\t\t\t AND t.start <= (datetime('now'))\n\t\t\t\t AND t.done IS NULL\n\t\t\t\tWHERE c1.path LIKE c.path||'%' \n\t\t\t) as total_tasks\n\t\t\tFROM Context c\n\t\t\tLEFT JOIN Task own\n\t\t\t  ON own.context = c.id\n\t\t\t AND own.start <= (datetime('now'))\n\t\t\t AND own.done IS NULL\n\t\t\tWHERE path LIKE ?\n\t\t\t  AND path NOT LIKE ?\n\t\t\t  AND visibility = 'normal'\n\t\t\t  {}\n\t\t\tGROUP BY c.id\n\t\t\tORDER BY\n\t\t\t  priority DESC,\n\t\t\t  total_tasks DESC\n\t\t\"\"\".format(add_condition), (\n\t\t\t'{}.%'.format(path),\n\t\t\t'{}.%.%'.format(path),\n\t\t\t)\n\t\t)\n\t\treturn c.fetchall()", "label": 1}
{"function": "\tdef get_descendants(self, path=''):\n\t\t\"\"\" Return an iterator over Row-contexts that are in the descendance\n\t\tof the context pointed to by `path`. The iterator is agnostic of\n\t\tvisibility and the contexts are sorted by their path. The first\n\t\telement of the iterator is the given context itself. \"\"\"\n\t\tc = self.connection.cursor()\n\t\tc.execute(\"\"\"\n\t\t\tSELECT c.*, COUNT(own.id) as own_tasks, (\n\t\t\t\tSELECT COUNT(t.id)\n\t\t\t\tFROM Context c1\n\t\t\t\tLEFT JOIN Task t\n\t\t\t\t  ON t.context = c1.id\n\t\t\t\t AND t.start <= (datetime('now'))\n\t\t\t\t AND t.done IS NULL\n\t\t\t\tWHERE c1.path LIKE c.path||'%' \n\t\t\t) as total_tasks\n\t\t\tFROM Context c\n\t\t\tLEFT JOIN Task own\n\t\t\t  ON own.context = c.id\n\t\t\t AND own.start <= (datetime('now'))\n\t\t\t AND own.done IS NULL\n\t\t\tWHERE path LIKE ?\n\t\t\tGROUP BY c.id\n\t\t\tORDER BY\n\t\t\t  c.path\n\t\t\"\"\", ('{}%'.format(path),))\n\t\treturn c", "label": 1}
{"function": "\tdef history(self):\n\t\t\"\"\" Return an iterator over Row-tasks which iterates over all the\n\t\ttasks in existence, sorted by their date of creation.\"\"\"\n\t\tc = self.connection.cursor()\n\t\tc.execute(\"\"\"\n\t\t\tSELECT t.*, c.path as ctx_path\n\t\t\tFROM Task t JOIN Context c\n\t\t\tON t.context = c.id\n\t\t\tORDER BY t.created\n\t\t\"\"\")\n\t\treturn c", "label": 1}
{"function": "\tdef get_greatest_id(self):\n\t\t\"\"\" Returns the greatest existing task ID, or None if there are no\n\t\ttask.\"\"\"\n\t\tc = self.connection.cursor()\n\t\tc.execute(\"\"\"\n\t\t\tSELECT MAX(id)\n\t\t\tFROM Task\n\t\t\"\"\")\n\t\trow = c.fetchone()\n\t\tif row is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn row[0]", "label": 1}
{"function": "\tdef purge(self, before):\n\t\t\"\"\" Remove all done tasks that were created before `before`. Remove\n\t\tall done tasks if `before` is None.\"\"\"\n\t\tc = self.connection.cursor()\n\t\tquery = \"\"\"\n\t\t\tDELETE FROM Task\n\t\t\tWHERE done IS NOT NULL\n\t\t\"\"\"\n\t\tif before is not None:\n\t\t\tquery += \"\"\"\n\t\t\t\tAND created < ?\n\t\t\t\"\"\"\n\t\t\tvalues = (before,)\n\t\telse:\n\t\t\tvalues = ()\n\t\tc.execute(query, values)\n\t\treturn c.rowcount", "label": 1}
{"function": "\tdef search(self, term, ctx='', done=None, before=None, after=None,\n\t\t       case=False):\n\t\toriginal = self.case_sensitive_like\n\t\tself.set_case_sensitive_like(case)\n\t\tc = self.connection.cursor()\n\t\tquery = \"\"\"\n\t\t\tSELECT t.*, c.path as ctx_path\n\t\t\tFROM Task t JOIN Context c\n\t\t\tON t.context = c.id\n\t\t\tWHERE t.title LIKE ?\n\t\t\t  AND c.path LIKE ?\n\t\t\"\"\"\n\t\tparams = ('%{}%'.format(term), '{}%'.format(ctx))\n\n\t\tif done is not None:\n\t\t\tcond = 'IS NOT NULL' if done else 'IS NULL'\n\t\t\tquery += \"\"\"\n\t\t\t\tAND t.done {}\n\t\t\t\"\"\".format(cond)\n\t\tif before is not None:\n\t\t\tquery += \"\"\"\n\t\t\t\tAND t.created < ?\n\t\t\t\"\"\"\n\t\t\tparams = params + (before,)\n\t\tif after is not None:\n\t\t\tquery += \"\"\"\n\t\t\t\tAND t.created > ?\n\t\t\t\"\"\"\n\t\t\tparams = params + (after,)\n\n\t\tc.execute(query, params)\n\t\tself.set_case_sensitive_like(original)\n\t\treturn c.fetchall()", "label": 1}
{"function": "\tdef take_editing_lock(self, tid):\n\t\t\"\"\"\n\t\tSet the column `editing` to 1 iff it's 0. Return True if the column was effectively set, False otherwise.\n\t\t\"\"\"\n\t\tc = self.connection.cursor()\n\t\tc.execute(\"\"\"\n\t\t\tUPDATE Task\n\t\t\tSET editing = 1\n\t\t\tWHERE id = ?\n\t\t\t  AND editing = 0\n\t\t\"\"\", (tid,))\n\t\ttaken = c.rowcount == 1\n\t\tself.connection.commit()\n\t\treturn taken", "label": 1}
{"function": "\tdef release_editing_lock(self, tid):\n\t\t\"\"\"\n\t\tRelease the editing lock on a task. The caller is trusted to have the\n\t\tlock and no verification is made.\n\t\t\"\"\"\n\t\tc = self.connection.cursor()\n\t\tc.execute(\"\"\"\n\t\t\tUPDATE Task\n\t\t\tSET editing = 0\n\t\t\tWHERE id = ?\n\t\t\"\"\", (tid,))\n\t\tself.connection.commit()", "label": 1}
{"function": "\tdef exit(self, save=True):\n\t\t\"\"\" Close the database and save all operations done to it if `save` is\n\t\tTrue. Write all contexts paths (NON fully-dotted) to the contexts file\n\t\tif at least one context was created or removed during operations. The\n\t\tcontexts file exists for terminal auto-completion.\"\"\"\n\t\tif save:\n\t\t\tself.connection.commit()\n\t\t\tif self.changed_contexts:\n\t\t\t\tc = self.connection.cursor()\n\t\t\t\tc.execute(\"\"\"\n\t\t\t\t\tSELECT DISTINCT path FROM Context\n\t\t\t\t\tORDER BY path\n\t\t\t\t\"\"\")\n\t\t\t\tdata_ctx = op.join(DATA_DIR, DATA_CTX_NAME)\n\t\t\t\twith open(data_ctx, 'w') as ctx_file:\n\t\t\t\t\tfor row in c:\n\t\t\t\t\t\tctx = userify_context(row[0])\n\t\t\t\t\t\tctx_file.write(ctx + '\\n')\n\t\tself.connection.close()", "label": 1}
{"function": "    def get_option_spec(cls):\n        spec = PyOptionSpec()\n        spec.addFloatOption(\n            'entropy_ratio',\n            'the entropy ratio we put on PolicyGradient',\n            0.01)\n        spec.addFloatOption(\n            'grad_clip_norm',\n            'gradient norm clipping',\n            0.0)\n        spec.addFloatOption(\n            'min_prob',\n            'mininal probability used in training',\n            1e-6)\n        spec.addFloatOption(\n            'ratio_clamp',\n            'maximum importance sampling ratio',\n            10.0)\n        spec.addStrListOption(\n            'policy_action_nodes',\n            'the entropy ratio we put on PolicyGradient',\n            ['pi,a'])\n        return spec", "label": 1}
{"function": "    def test_1d_slicing3_npm(self):\n        self.test_1d_slicing3(flags=Noflags)", "label": 1}
{"function": "    def test_1d_slicing4(self, flags=enable_pyobj_flags):\n        pyfunc = slicing_1d_usecase4\n        arraytype = types.Array(types.int32, 1, 'C')\n        argtys = (arraytype,)\n        cr = compile_isolated(pyfunc, argtys, flags=flags)\n        cfunc = cr.entry_point\n\n        a = np.arange(10, dtype='i4')\n        self.assertEqual(pyfunc(a), cfunc(a))\n\n        # Any\n        arraytype = types.Array(types.int32, 1, 'A')\n        argtys = (arraytype,)\n        cr = compile_isolated(pyfunc, argtys, flags=flags)\n        cfunc = cr.entry_point\n\n        a = np.arange(20, dtype='i4')[::2]\n        self.assertFalse(a.flags['C_CONTIGUOUS'])\n        self.assertFalse(a.flags['F_CONTIGUOUS'])\n        self.assertEqual(pyfunc(a), cfunc(a))", "label": 1}
{"function": "    def test_1d_slicing4_npm(self):\n        self.test_1d_slicing4(flags=Noflags)", "label": 1}
{"function": "    def check_1d_slicing_with_arg(self, pyfunc, flags):\n        args = list(range(-9, 10))\n\n        arraytype = types.Array(types.int32, 1, 'C')\n        argtys = (arraytype, types.int32)\n        cr = compile_isolated(pyfunc, argtys, flags=flags)\n        cfunc = cr.entry_point\n\n        a = np.arange(10, dtype='i4')\n        for arg in args:\n            self.assertEqual(pyfunc(a, arg), cfunc(a, arg))\n\n        # Any\n        arraytype = types.Array(types.int32, 1, 'A')\n        argtys = (arraytype, types.int32)\n        cr = compile_isolated(pyfunc, argtys, flags=flags)\n        cfunc = cr.entry_point\n\n        a = np.arange(20, dtype='i4')[::2]\n        self.assertFalse(a.flags['C_CONTIGUOUS'])\n        self.assertFalse(a.flags['F_CONTIGUOUS'])\n        for arg in args:\n            self.assertEqual(pyfunc(a, arg), cfunc(a, arg))", "label": 1}
{"function": "    def test_1d_slicing5(self, flags=enable_pyobj_flags):\n        pyfunc = slicing_1d_usecase5\n        self.check_1d_slicing_with_arg(pyfunc, flags)", "label": 1}
{"function": "    def test_1d_slicing5_npm(self):\n        self.test_1d_slicing5(flags=Noflags)", "label": 1}
{"function": "    def test_1d_slicing6(self, flags=enable_pyobj_flags):\n        pyfunc = slicing_1d_usecase6\n        self.check_1d_slicing_with_arg(pyfunc, flags)", "label": 1}
{"function": "    def test_1d_slicing6_npm(self):\n        self.test_1d_slicing6(flags=Noflags)", "label": 1}
{"function": "    def test_1d_slicing7(self, flags=enable_pyobj_flags):\n        pyfunc = slicing_1d_usecase7\n        self.check_1d_slicing_with_arg(pyfunc, flags)", "label": 1}
{"function": "    def test_1d_slicing7_npm(self):\n        self.test_1d_slicing7(flags=Noflags)", "label": 1}
{"function": "    def register_introspection_functions(self):\n        \"\"\"Registers the XML-RPC introspection methods in the system\n        namespace.\n\n        see http://xmlrpc.usefulinc.com/doc/reserved.html\n        \"\"\"\n\n        self.funcs.update({'system.listMethods' : self.system_listMethods,\n                      'system.methodSignature' : self.system_methodSignature,\n                      'system.methodHelp' : self.system_methodHelp})", "label": 1}
{"function": "    def register_multicall_functions(self):\n        \"\"\"Registers the XML-RPC multicall method in the system\n        namespace.\n\n        see http://www.xmlrpc.com/discuss/msgReader$1208\"\"\"\n\n        self.funcs.update({'system.multicall' : self.system_multicall})", "label": 1}
{"function": "    def _marshaled_dispatch(self, data, dispatch_method = None, path = None):\n        \"\"\"Dispatches an XML-RPC method from marshalled (XML) data.\n\n        XML-RPC methods are dispatched from the marshalled (XML) data\n        using the _dispatch method and the result is returned as\n        marshalled data. For backwards compatibility, a dispatch\n        function can be provided as an argument (see comment in\n        SimpleXMLRPCRequestHandler.do_POST) but overriding the\n        existing method through subclassing is the preferred means\n        of changing method dispatch behavior.\n        \"\"\"\n\n        try:\n            params, method = loads(data, use_builtin_types=self.use_builtin_types)\n\n            # generate response\n            if dispatch_method is not None:\n                response = dispatch_method(method, params)\n            else:\n                response = self._dispatch(method, params)\n            # wrap response in a singleton tuple\n            response = (response,)\n            response = dumps(response, methodresponse=1,\n                             allow_none=self.allow_none, encoding=self.encoding)\n        except Fault as fault:\n            response = dumps(fault, allow_none=self.allow_none,\n                             encoding=self.encoding)\n        except:\n            # report exception back to server\n            exc_type, exc_value, exc_tb = sys.exc_info()\n            response = dumps(\n                Fault(1, \"%s:%s\" % (exc_type, exc_value)),\n                encoding=self.encoding, allow_none=self.allow_none,\n                )\n\n        return response.encode(self.encoding, 'xmlcharrefreplace')", "label": 1}
{"function": "    def system_listMethods(self):\n        \"\"\"system.listMethods() => ['add', 'subtract', 'multiple']\n\n        Returns a list of the methods supported by the server.\"\"\"\n\n        methods = set(self.funcs.keys())\n        if self.instance is not None:\n            # Instance can implement _listMethod to return a list of\n            # methods\n            if hasattr(self.instance, '_listMethods'):\n                methods |= set(self.instance._listMethods())\n            # if the instance has a _dispatch method then we\n            # don't have enough information to provide a list\n            # of methods\n            elif not hasattr(self.instance, '_dispatch'):\n                methods |= set(list_public_methods(self.instance))\n        return sorted(methods)", "label": 1}
{"function": "    def system_methodSignature(self, method_name):\n        \"\"\"system.methodSignature('add') => [double, int, int]\n\n        Returns a list describing the signature of the method. In the\n        above example, the add method takes two integers as arguments\n        and returns a double result.\n\n        This server does NOT support system.methodSignature.\"\"\"\n\n        # See http://xmlrpc.usefulinc.com/doc/sysmethodsig.html\n\n        return 'signatures not supported'", "label": 1}
{"function": "    def system_methodHelp(self, method_name):\n        \"\"\"system.methodHelp('add') => \"Adds two integers together\"\n\n        Returns a string containing documentation for the specified method.\"\"\"\n\n        method = None\n        if method_name in self.funcs:\n            method = self.funcs[method_name]\n        elif self.instance is not None:\n            # Instance can implement _methodHelp to return help for a method\n            if hasattr(self.instance, '_methodHelp'):\n                return self.instance._methodHelp(method_name)\n            # if the instance has a _dispatch method then we\n            # don't have enough information to provide help\n            elif not hasattr(self.instance, '_dispatch'):\n                try:\n                    method = resolve_dotted_attribute(\n                                self.instance,\n                                method_name,\n                                self.allow_dotted_names\n                                )\n                except AttributeError:\n                    pass\n\n        # Note that we aren't checking that the method actually\n        # be a callable object of some kind\n        if method is None:\n            return \"\"\n        else:\n            return pydoc.getdoc(method)", "label": 1}
{"function": "    def system_multicall(self, call_list):\n        \"\"\"system.multicall([{'methodName': 'add', 'params': [2, 2]}, ...]) => \\\n[[4], ...]\n\n        Allows the caller to package multiple XML-RPC calls into a single\n        request.\n\n        See http://www.xmlrpc.com/discuss/msgReader$1208\n        \"\"\"\n\n        results = []\n        for call in call_list:\n            method_name = call['methodName']\n            params = call['params']\n\n            try:\n                # XXX A marshalling error in any response will fail the entire\n                # multicall. If someone cares they should fix this.\n                results.append([self._dispatch(method_name, params)])\n            except Fault as fault:\n                results.append(\n                    {'faultCode' : fault.faultCode,\n                     'faultString' : fault.faultString}\n                    )\n            except:\n                exc_type, exc_value, exc_tb = sys.exc_info()\n                results.append(\n                    {'faultCode' : 1,\n                     'faultString' : \"%s:%s\" % (exc_type, exc_value)}\n                    )\n        return results", "label": 1}
{"function": "    def _dispatch(self, method, params):\n        \"\"\"Dispatches the XML-RPC method.\n\n        XML-RPC calls are forwarded to a registered function that\n        matches the called XML-RPC method name. If no such function\n        exists then the call is forwarded to the registered instance,\n        if available.\n\n        If the registered instance has a _dispatch method then that\n        method will be called with the name of the XML-RPC method and\n        its parameters as a tuple\n        e.g. instance._dispatch('add',(2,3))\n\n        If the registered instance does not have a _dispatch method\n        then the instance will be searched to find a matching method\n        and, if found, will be called.\n\n        Methods beginning with an '_' are considered private and will\n        not be called.\n        \"\"\"\n\n        func = None\n        try:\n            # check to see if a matching function has been registered\n            func = self.funcs[method]\n        except KeyError:\n            if self.instance is not None:\n                # check for a _dispatch method\n                if hasattr(self.instance, '_dispatch'):\n                    return self.instance._dispatch(method, params)\n                else:\n                    # call instance method directly\n                    try:\n                        func = resolve_dotted_attribute(\n                            self.instance,\n                            method,\n                            self.allow_dotted_names\n                            )\n                    except AttributeError:\n                        pass\n\n        if func is not None:\n            return func(*params)\n        else:\n            raise Exception('method \"%s\" is not supported' % method)", "label": 1}
{"function": "    def accept_encodings(self):\n        r = {}\n        ae = self.headers.get(\"Accept-Encoding\", \"\")\n        for e in ae.split(\",\"):\n            match = self.aepattern.match(e)\n            if match:\n                v = match.group(3)\n                v = float(v) if v else 1.0\n                r[match.group(1)] = v\n        return r", "label": 1}
{"function": "    def is_rpc_path_valid(self):\n        if self.rpc_paths:\n            return self.path in self.rpc_paths\n        else:\n            # If .rpc_paths is empty, just assume all paths are legal\n            return True", "label": 1}
{"function": "    def export_item(self, item):\n        print(json.dumps(item))", "label": 1}
{"function": "    def close(self):\n        pass", "label": 1}
{"function": "    def __init__(self, **kwargs):\n        \"\"\"Behaviour can be customized by overriding attributes in a subclass or setting them in the constructor.\n\n        :param string kill_xpath: XPath expression for tags to remove along with their contents.\n        :param string strip_xpath: XPath expression for tags to replace with their contents.\n        :param string allow_xpath: XPath expression for tags to except from strip_xpath.\n        :param bool fix_whitespace: Normalize whitespace to a single space and ensure newlines around block elements.\n        :param dict namespaces: Namespace prefixes to register for the XPaths.\n        \"\"\"\n        # TODO: This is weird. Why don't we change to proper individual keyword arguments with class attribs as default\n        for name, value in kwargs.items():\n            if not hasattr(self, name):\n                raise TypeError('Unknown parameter: %s=%r' % (name, value))\n            setattr(self, name, value)", "label": 1}
{"function": "    def __call__(self, doc):\n        \"\"\"Clean the document.\"\"\"\n        if hasattr(doc, 'getroot'):\n            doc = doc.getroot()\n\n        if self.fix_whitespace:\n            # Ensure newlines around block elements\n            for el in doc.iterdescendants():\n                if el.tag in BLOCK_ELEMENTS:\n                    el.tail = (el.tail or '') + '\\n'\n                    previous = el.getprevious()\n                    parent = el.getparent()\n                    if previous is None:\n                        parent.text = (parent.text or '') + '\\n'\n                    else:\n                        previous.tail = (previous.tail or '') + '\\n'\n\n        # Remove elements that match kill_xpath\n        if self.kill_xpath:\n            for el in doc.xpath(self.kill_xpath, namespaces=self.namespaces):\n                #log.debug('Killing: %s' % tostring(el))\n                parent = el.getparent()\n                # We can't kill the root element!\n                if parent is None:\n                    continue\n                if el.tail:\n                    previous = el.getprevious()\n                    if previous is None:\n                        parent.text = (parent.text or '') + el.tail\n                    else:\n                        previous.tail = (previous.tail or '') + el.tail\n                parent.remove(el)\n\n        # Collect all the allowed elements\n        to_keep = [el for el in doc.xpath(self.allow_xpath, namespaces=self.namespaces)] if self.allow_xpath else []\n\n        # Replace elements that match strip_xpath with their contents\n        if self.strip_xpath:\n            for el in doc.xpath(self.strip_xpath, namespaces=self.namespaces):\n                # Skip if allowed by allow_xpath\n                if el in to_keep:\n                    continue\n                parent = el.getparent()\n                previous = el.getprevious()\n                # We can't strip the root element!\n                if parent is None:\n                    continue\n                # Append the text to previous tail (or parent text if no previous), ensuring newline if block level\n                if el.text and isinstance(el.tag, six.string_types):\n                    if previous is None:\n                        parent.text = (parent.text or '') + el.text\n                    else:\n                        previous.tail = (previous.tail or '') + el.text\n                # Append the tail to last child tail, or previous tail, or parent text, ensuring newline if block level\n                if el.tail:\n                    if len(el):\n                        last = el[-1]\n                        last.tail = (last.tail or '') + el.tail\n                    elif previous is None:\n                        parent.text = (parent.text or '') + el.tail\n                    else:\n                        previous.tail = (previous.tail or '') + el.tail\n                index = parent.index(el)\n                parent[index:index+1] = el[:]\n\n        # Collapse whitespace down to a single space or a single newline\n        if self.fix_whitespace:\n            for el in doc.iter():\n                if el.text is not None:\n                    el.text = re.sub(r'\\s*\\n\\s*', '\\n', el.text)\n                    el.text = re.sub(r'[ \\t]+', ' ', el.text)\n                    # el.text = re.sub(r'\\s+', ' ', el.text)\n                if el.tail is not None:\n                    el.tail = re.sub(r'\\s*\\n\\s*', '\\n', el.tail)\n                    el.tail = re.sub(r'[ \\t]+', ' ', el.tail)", "label": 1}
{"function": "    def clean_html(self, html):\n        \"\"\"Apply ``Cleaner`` to HTML string or document and return a cleaned string or document.\"\"\"\n        result_type = type(html)\n        if isinstance(html, six.string_types):\n            doc = html_fromstring(html)\n        else:\n            doc = copy.deepcopy(html)\n        self(doc)\n        if issubclass(result_type, six.binary_type):\n            return tostring(doc, encoding='utf-8')\n        elif issubclass(result_type, six.text_type):\n            return tostring(doc, encoding='unicode')\n        else:\n            return doc", "label": 1}
{"function": "    def clean_markup(self, markup, parser=None):\n        \"\"\"Apply ``Cleaner`` to markup string or document and return a cleaned string or document.\"\"\"\n        result_type = type(markup)\n        if isinstance(markup, six.string_types):\n            doc = fromstring(markup, parser=parser)\n        else:\n            doc = copy.deepcopy(markup)\n        self(doc)\n        if issubclass(result_type, six.binary_type):\n            return tostring(doc, encoding='utf-8')\n        elif issubclass(result_type, six.text_type):\n            return tostring(doc, encoding='unicode')\n        else:\n            return doc", "label": 1}
{"function": "def str2size(s, scale=1024):\n    \"\"\"Convert size-string.\n\n    String format: <value>[:space:]<B | K | M | ...> to bytes.\n\n    :param s: size-string\n    :param scale: base size\n    \"\"\"\n    if not s:\n        return 0\n\n    if isinstance(s, six.integer_types):\n        return s\n\n    match = re.match(r'^([\\.\\d]+)\\s*([BbKkMmGgTtPpEeZzYy]?)', s)\n    if match is None:\n        raise ValueError(_('Invalid value: %(value)s')\n                         % {'value': s})\n\n    groups = match.groups()\n    value = float(groups[0])\n    suffix = groups[1].upper() if groups[1] else 'B'\n\n    types = ('B', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    for i, t in enumerate(types):\n        if suffix == t:\n            return int(value * pow(scale, i))", "label": 1}
{"function": "def str2gib_size(s):\n    \"\"\"Covert size-string to size in gigabytes.\"\"\"\n    size_in_bytes = str2size(s)\n    return size_in_bytes // units.Gi", "label": 1}
{"function": "def get_rrmgr_cmd(src, dst, compression=None, tcp_buf_size=None,\n                  connections=None):\n    \"\"\"Returns rrmgr command for source and destination.\"\"\"\n    cmd = ['rrmgr', '-s', 'zfs']\n    if compression:\n        cmd.extend(['-c', six.text_type(compression)])\n    cmd.append('-q')\n    cmd.append('-e')\n    if tcp_buf_size:\n        cmd.extend(['-w', six.text_type(tcp_buf_size)])\n    if connections:\n        cmd.extend(['-n', six.text_type(connections)])\n    cmd.extend([src, dst])\n    return ' '.join(cmd)", "label": 1}
{"function": "def parse_nms_url(url):\n    \"\"\"Parse NMS url into normalized parts like scheme, user, host and others.\n\n    Example NMS URL:\n        auto://admin:nexenta@192.168.1.1:2000/\n\n    NMS URL parts:\n\n    .. code-block:: none\n\n        auto                True if url starts with auto://, protocol\n                            will be automatically switched to https\n                            if http not supported;\n        scheme (auto)       connection protocol (http or https);\n        user (admin)        NMS user;\n        password (nexenta)  NMS password;\n        host (192.168.1.1)  NMS host;\n        port (2000)         NMS port.\n\n    :param url: url string\n    :return: tuple (auto, scheme, user, password, host, port, path)\n    \"\"\"\n    pr = urlparse.urlparse(url)\n    scheme = pr.scheme\n    auto = scheme == 'auto'\n    if auto:\n        scheme = 'http'\n    user = 'admin'\n    password = 'nexenta'\n    if '@' not in pr.netloc:\n        host_and_port = pr.netloc\n    else:\n        user_and_password, host_and_port = pr.netloc.split('@', 1)\n        if ':' in user_and_password:\n            user, password = user_and_password.split(':')\n        else:\n            user = user_and_password\n    if ':' in host_and_port:\n        host, port = host_and_port.split(':', 1)\n    else:\n        host, port = host_and_port, '2000'\n    return auto, scheme, user, password, host, port, '/rest/nms/'", "label": 1}
{"function": "    def __init__(self):\n        pass", "label": 1}
{"function": "def call_this():\n    print('break here')", "label": 1}
{"function": "def main():\n    other.call_me_back1(call_this)\n    print('TEST SUCEEDED!')", "label": 1}
{"function": "    def get_root(self):\n        # Return first element if it exists else inf (MaxHeap) or -inf (MinHeap)\n        if self.size > 0:\n            return self.array[1]\n        else:\n            return self.array[0]", "label": 1}
{"function": "    def get_array(self):\n        # Return the heap in form of array\n        return self.array[1:]", "label": 1}
{"function": "    def parent(ind):\n        # Calculate parent position for given index\n        return ind//2", "label": 1}
{"function": "    def child_left(ind):\n        # Calculate first child position for given index\n        return ind*2", "label": 1}
{"function": "    def child_right(ind):\n        # Calculate second child position for given index\n        return ind*2 + 1", "label": 1}
{"function": "    def is_child_left(self, ind):\n        # Check if first child exists\n        return self.child_left(ind) < len(self.array)", "label": 1}
{"function": "    def is_child_right(self, ind):\n        # Check if second child exists\n        return self.child_right(ind) < len(self.array)", "label": 1}
{"function": "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels, kernel_size=None, stride=None,\n                 num_iterations=p.NUM_ROUTING_ITERATIONS, use_cuda=False):\n        super(CapsuleLayer, self).__init__()\n\n        self.num_route_nodes = num_route_nodes\n        self.num_iterations = num_iterations\n\n        self.num_capsules = num_capsules\n\n        if num_route_nodes != -1:\n            self.route_weights = nn.Parameter(torch.randn(num_capsules, num_route_nodes, in_channels, out_channels))\n        else:\n            self.capsules = nn.ModuleList(\n                [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=0) for _ in\n                 range(num_capsules)])\n\n        self.use_cuda = use_cuda", "label": 1}
{"function": "    def squash(self, tensor, dim=-1):\n        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n        scale = squared_norm / (1 + squared_norm)\n        return scale * tensor / torch.sqrt(squared_norm)", "label": 1}
{"function": "    def forward(self, x):\n        if self.num_route_nodes != -1:\n            priors = x[None, :, :, None, :] @ self.route_weights[:, None, :, :, :]\n\n            logits = Variable(torch.zeros(*priors.size()))\n            if self.use_cuda:\n                logits = logits.cuda()\n            for i in range(self.num_iterations):\n                probs = softmax(logits, dim=2)\n                outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n\n                if i != self.num_iterations - 1:\n                    delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n                    logits = logits + delta_logits\n        else:\n            outputs = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n            outputs = torch.cat(outputs, dim=-1)\n            outputs = self.squash(outputs)\n\n        return outputs", "label": 1}
{"function": "    def __init__(self, use_cuda=False, num_iterations=p.NUM_ROUTING_ITERATIONS):\n        super(CapsuleNet, self).__init__()\n        self.use_cuda = use_cuda\n        self.encoder = nn.Sequential(\n            View(dim=(-1, 2, 129, 21)),\n            # conv layer\n            nn.Conv2d(in_channels=2, out_channels=256, kernel_size=(21, 3), stride=(2, 1)),\n            Swish(),\n            # primary capsule\n            CapsuleLayer(num_capsules=8, num_route_nodes=-1, in_channels=256, out_channels=16,\n                         kernel_size=(21, 3), stride=(4, 2), num_iterations=num_iterations),\n            # class capsule\n            CapsuleLayer(num_capsules=p.NUM_LABELS, num_route_nodes=16 * 9 * 9, in_channels=8,\n                         out_channels=16, num_iterations=num_iterations)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(16 * p.NUM_LABELS, 1024),\n            Swish(),\n            nn.Linear(1024, 4096),\n            Swish(),\n            nn.Linear(4096, 5418),\n            nn.Sigmoid()\n        )", "label": 1}
{"function": "    def forward(self, x, y=None):\n        h = self.encoder(x).squeeze().transpose(0, 1)\n\n        y_hat = (h ** 2).sum(dim=-1) ** 0.5\n        y_hat = F.softmax(y_hat, dim=-1)\n\n        if y is None:\n            # In all batches, get the most active capsule.\n            _, max_length_indices = y_hat.max(dim=1)\n            y = Variable(torch.eye(p.NUM_LABELS))\n            if self.use_cuda:\n                y = y.cuda()\n            y = y.index_select(dim=0, index=max_length_indices.data)\n\n        x_hat = self.decoder((h * y[:, :, None]).view(x.size(0), -1))\n\n        return y_hat, x_hat", "label": 1}
{"function": "    def __init__(self):\n        super(CapsuleLoss, self).__init__()\n        self.reconstruction_loss = nn.MSELoss(size_average=False)", "label": 1}
{"function": "    def forward(self, x, y, y_hat, x_hat):\n        left = F.relu(0.9 - y_hat, inplace=True) ** 2\n        right = F.relu(y_hat - 0.1, inplace=True) ** 2\n\n        margin_loss = y * left + 0.5 * (1. - y) * right\n        margin_loss = margin_loss.sum()\n\n        assert torch.numel(x) == torch.numel(x_hat)\n        x = x.view(x_hat.size()[0], -1)\n        reconstruction_loss = self.reconstruction_loss(x_hat, x)\n\n        return (margin_loss + 0.0005 * reconstruction_loss) / x.size(0)", "label": 1}
{"function": "    def __init__(self, editwin):\n        self.editwin = editwin\n        self.text = editwin.text\n        # Bind the check-restore event to the function restore_event,\n        # so that we can then use activate_restore (which calls event_add)\n        # and deactivate_restore (which calls event_delete).\n        editwin.text.bind(self.RESTORE_VIRTUAL_EVENT_NAME,\n                          self.restore_event)\n        self.counter = 0\n        self.is_restore_active = 0\n        self.set_style(self.STYLE)", "label": 1}
{"function": "    def activate_restore(self):\n        if not self.is_restore_active:\n            for seq in self.RESTORE_SEQUENCES:\n                self.text.event_add(self.RESTORE_VIRTUAL_EVENT_NAME, seq)\n            self.is_restore_active = True", "label": 1}
{"function": "    def deactivate_restore(self):\n        if self.is_restore_active:\n            for seq in self.RESTORE_SEQUENCES:\n                self.text.event_delete(self.RESTORE_VIRTUAL_EVENT_NAME, seq)\n            self.is_restore_active = False", "label": 1}
{"function": "    def __str__(self):\n        return \"(%s) %s-%s\" % (self.status_code, self.error_type,\n                               self.error_message)", "label": 1}
{"function": "    def _call(api, *args, **kwargs):\n        method = WeixinAPIMethod(api, *args, **kwargs)\n        return method.execute()", "label": 1}
{"function": "        def __init__(self, api, *args, **kwargs):\n            self.api = api\n            self.as_generator = kwargs.pop(\"as_generator\", False)\n            self.return_json = kwargs.pop(\"return_json\", True)\n            self.parameters = {}\n            self._build_parameters(args, kwargs)\n            self._build_path()", "label": 1}
{"function": "        def _build_parameters(self, args, kwargs):\n            for index, value in enumerate(args):\n                if value is None:\n                    continue\n                try:\n                    self.parameters[self.accepts_parameters[index]] = encode_string(value)\n                except IndexError:\n                    raise WeixinClientError(\"Too many arguments supplied\")\n\n            for key, value in six.iteritems(kwargs):\n                if value is None:\n                    continue\n                if key in self.parameters:\n                    raise WeixinClientError(\"Parameter %s already supplied\" % key)\n                if key not in set(['json_body']):\n                    value = encode_string(value)\n                self.parameters[key] = value", "label": 1}
{"function": "        def _build_path(self):\n            for variable in re_path_template.findall(self.path):\n                name = variable.strip('{}')\n\n                try:\n                    value = quote(self.parameters[name])\n                except KeyError:\n                    raise Exception('No parameter value found for path variable: %s' % name)\n                del self.parameters[name]\n\n                self.path = self.path.replace(variable, value)\n\n            if self.api.format:\n                self.path = self.path + '.%s' % self.api.format\n            else:\n                self.path = self.path", "label": 1}
{"function": "        def _build_pagination_info(self, content_obj):\n            pass", "label": 1}
{"function": "        def _do_api_request(self, url, method='GET', body=None,\n                            json_body=None, headers=None):\n            headers = headers or {}\n            if self.signature and self.api.app_secret is not None:\n                secret = self.api.app_secret\n                signature = hmac.new(secret, sha256).hexdigest()\n                headers['X-Weixin-Forwarded-For'] = '|'.join([signature])\n            response = OAuth2Request(self.api).make_request(\n                url, method=method, body=body,\n                json_body=json_body, headers=headers)\n            status_code = response.status_code\n            try:\n                content_obj = simplejson.loads(response.content)\n            except ValueError:\n                raise WeixinClientError(\n                    'Unable to parse response, not valid JSON.',\n                    status_code=status_code)\n\n            api_responses = []\n            if status_code == 200:\n                if not self.objectify_response:\n                    return content_obj, None\n\n                if self.response_type == 'list':\n                    for entry in content_obj['data']:\n                        if self.return_json:\n                            api_responses.append(entry)\n                elif self.response_type == 'entry':\n                    data = content_obj\n                    if self.return_json:\n                        api_responses = data\n                elif self.response_type == 'empty':\n                    pass\n                return api_responses, self._build_pagination_info(content_obj)\n            else:\n                raise WeixinAPIError(\n                    status_code, content_obj['errcode'], content_obj['errmsg'])", "label": 1}
{"function": "        def _paginator_with_url(self, url, method=\"GET\", body=None, headers=None):\n            pass", "label": 1}
{"function": "        def _get_with_next_url(self, url, method=\"GET\", body=None, headers=None):\n            pass", "label": 1}
{"function": "        def execute(self):\n            url, method, body, json_body, headers = (\n                OAuth2Request(self.api).prepare_request(\n                    self.method, self.path, self.parameters,\n                    include_secret=self.include_secret))\n            if self.as_generator:\n                return self._paginator_with_url(url, method, body, headers)\n            else:\n                content, next = self._do_api_request(url, method, body,\n                                                     json_body, headers)\n            if self.paginates:\n                return content, next\n            else:\n                return content", "label": 1}
{"function": "    def b2a_uu(data: _Bytes, *, backtick: bool = ...) -> bytes: ...", "label": 1}
{"function": "    def b2a_uu(data: _Bytes) -> bytes: ...", "label": 1}
{"function": "    def b2a_base64(data: _Bytes, *, newline: bool = ...) -> bytes: ...", "label": 1}
{"function": "    def b2a_base64(data: _Bytes) -> bytes: ...", "label": 1}
{"function": "    def forward(self, z):\n        pass", "label": 1}
{"function": "    def forward(self, z):\n        a = 1.0 / (1.0 + np.exp(-z))\n        return a", "label": 1}
{"function": "    def forward(self, z):\n        shift_z = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(shift_z)\n        a = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n        return a", "label": 1}
{"function": "def is_valid_closer(string):\n    \"\"\"\n    Returns True if every opener has a valid closer\n    \"\"\"\n    openers = ['[', '(', '{']\n    closers = [']', ')', '}']\n    stack = []\n\n    for ch in string:\n        # If there is a closer, but no openers\n        if not stack and ch in closers:\n            return False\n\n        # Add any opener into the stack\n        elif ch in openers:\n            stack.append(ch)\n\n        # If the following closers do not pair with the opener popped from stack\n        # then the function will return False\n        elif ch == ']':\n            if stack.pop() != '[':\n                return False\n\n        elif ch == ')':\n            if stack.pop() != '(':\n                return False\n\n        elif ch == '}':\n            if stack.pop() != '{':\n                return False\n\n    # Nothing in our stack == True\n    return not stack", "label": 1}
{"function": "    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--ntest', type=int, default=float(\"inf\"), help='# of test examples.')\n        self.parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n        self.parser.add_argument('--aspect_ratio', type=float, default=1.0, help='aspect ratio of result images')\n        self.parser.add_argument('--phase', type=str, default='test', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--how_many', type=int, default=50, help='how many test images to run')\n        self.parser.add_argument('--file_name', type=str, default='')\n        self.parser.add_argument('--suffix', type=str, default='')\n        self.isTrain = False", "label": 1}
{"function": "    def __str__(self):\n        return \"{}/{}\".format(self.okta_domain, self.name)", "label": 1}
{"function": "def test__get_function_default_kwargs__empty():\n    \"\"\" Test if `_get_function_default_kwargs` handles an empty function. \"\"\"\n\n    def func():\n        pass\n\n    assert list(_get_function_default_kwargs(func).keys()) == []", "label": 1}
{"function": "def test__get_function_default_kwargs__kwarg():\n    \"\"\" Test if `_get_function_default_kwargs` handles a single kwarg. \"\"\"\n\n    def func(kwarg=HParam()):\n        pass\n\n    assert list(_get_function_default_kwargs(func).keys()) == ['kwarg']\n    assert all([isinstance(v, _HParam) for v in _get_function_default_kwargs(func).values()])", "label": 1}
{"function": "def test__get_function_default_kwargs__arg_kwarg():\n    \"\"\" Test if `_get_function_default_kwargs` handles a kwarg, arg, args, and kwargs. \"\"\"\n\n    def func(arg, *args, kwarg=None, **kwargs):\n        pass\n\n    assert list(_get_function_default_kwargs(func).keys()) == ['kwarg']", "label": 1}
{"function": "def test_config_operators():\n    \"\"\" Test the `log_config`, `clear_config`, `add_config` and `get_config` together. It's\n    difficult to test them alone.\n    \"\"\"\n\n    @configurable\n    def configured(arg):\n        pass\n\n    assert len(get_config()) == 0\n    add_config({configured: HParams()})\n    log_config()  # Smoke test\n    assert len(get_config()) == 1\n    clear_config()\n    assert len(get_config()) == 0", "label": 1}
{"function": "def test_merge_configs():\n    \"\"\" Test the merging of two configurations via `add_config`.\n    \"\"\"\n\n    @configurable\n    def configured(arg, arg_two):\n        pass\n\n    @configurable\n    def other_configured(arg):\n        pass\n\n    add_config({configured: HParams(arg='arg', arg_two='arg_two')})\n    add_config({other_configured: HParams()})\n    assert len(get_config()) == 2\n    assert get_config()[_get_function_signature(configured.__wrapped__)]['arg'] == 'arg'\n    add_config({configured: HParams(arg='gra')})\n    assert len(get_config()) == 2\n    assert get_config()[_get_function_signature(configured.__wrapped__)]['arg'] == 'gra'\n    assert get_config()[_get_function_signature(configured.__wrapped__)]['arg_two'] == 'arg_two'", "label": 1}
{"function": "def test_parse_hparam_args__decimal():\n    hparam_args = ['--foo', 'HParams(boo=0.01)']\n    assert parse_hparam_args(hparam_args) == {'foo': HParams(boo=0.01)}", "label": 1}
{"function": "def test_parse_hparam_args__string():\n    hparam_args = ['--foo', 'HParams(boo=\"WaveNet\")']\n    assert parse_hparam_args(hparam_args) == {'foo': HParams(boo='WaveNet')}", "label": 1}
{"function": "def test_parse_hparam_args__equals():\n    hparam_args = ['--foo=HParams(boo=1)']\n    assert parse_hparam_args(hparam_args) == {'foo': HParams(boo=1)}", "label": 1}
{"function": "def test_parse_hparam_args__nesting():\n    hparam_args = ['--moo.foo', 'HParams(boo=1)']\n    assert parse_hparam_args(hparam_args) == {'moo.foo': HParams(boo=1)}", "label": 1}
{"function": "def test_parse_hparam_args__exponent():\n    hparam_args = ['--foo', 'HParams(boo=10**-6)']\n    assert parse_hparam_args(hparam_args) == {'foo': HParams(boo=10**-6)}", "label": 1}
{"function": "    def register(self, fileobj: _FileObject, events: _EventMask, data: Any = ...) -> SelectorKey: ...", "label": 1}
{"function": "    def unregister(self, fileobj: _FileObject) -> SelectorKey: ...", "label": 1}
{"function": "    def select(self, timeout: Optional[float] = ...) -> List[Tuple[SelectorKey, _EventMask]]: ...", "label": 1}
{"function": "    def get_map(self) -> Mapping[_FileObject, SelectorKey]: ...", "label": 1}
{"function": "    def fileno(self) -> int: ...", "label": 1}
{"function": "    def register(self, fileobj: _FileObject, events: _EventMask, data: Any = ...) -> SelectorKey: ...", "label": 1}
{"function": "    def unregister(self, fileobj: _FileObject) -> SelectorKey: ...", "label": 1}
{"function": "    def select(self, timeout: Optional[float] = ...) -> List[Tuple[SelectorKey, _EventMask]]: ...", "label": 1}
{"function": "    def get_map(self) -> Mapping[_FileObject, SelectorKey]: ...", "label": 1}
{"function": "    def register(self, fileobj: _FileObject, events: _EventMask, data: Any = ...) -> SelectorKey: ...", "label": 1}
{"function": "  def test_collapsed_callchains_generator_hide_other(self):\n    perf_stream = StringIO.StringIO(PERF_SCRIPT_OUTPUT)\n    callchains = list(ipr.collapsed_callchains_generator(perf_stream,\n                                                         hide_other=True,\n                                                         hide_compiler=True,\n                                                         hide_jit=True))\n    self.assertListEqual(callchains, [\n      [\"foo\", \"BytecodeHandler:bar\", \"[interpreter]\"],\n      [\"foo\", \"BytecodeHandler:bar\", \"[interpreter]\"],\n      [\"beep\", \"BytecodeHandler:bar\", \"[interpreter]\"],\n      [\"Lost\", \"[misattributed]\"],\n      [\"[entry trampoline]\"],\n    ])", "label": 1}
{"function": "  def test_calculate_samples_count_per_callchain(self):\n    counters = ipr.calculate_samples_count_per_callchain([\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"beep\", \"BytecodeHandler:bar\"],\n      [\"hello\", \"v8::internal::Compiler\", \"[compiler]\"],\n    ])\n    self.assertItemsEqual(counters, [\n      ('BytecodeHandler:bar;foo', 2),\n      ('BytecodeHandler:bar;beep', 1),\n      ('[compiler];v8::internal::Compiler;hello', 1),\n    ])", "label": 1}
{"function": "  def test_calculate_samples_count_per_callchain(self):\n    counters = ipr.calculate_samples_count_per_callchain([\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"beep\", \"BytecodeHandler:bar\"],\n    ])\n    self.assertItemsEqual(counters, [\n      ('BytecodeHandler:bar;foo', 2),\n      ('BytecodeHandler:bar;beep', 1),\n    ])", "label": 1}
{"function": "  def test_calculate_samples_count_per_handler_show_compile(self):\n    counters = ipr.calculate_samples_count_per_handler([\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"beep\", \"BytecodeHandler:bar\"],\n      [\"hello\", \"v8::internal::Compiler\", \"[compiler]\"],\n    ])\n    self.assertItemsEqual(counters, [\n      (\"bar\", 3),\n      (\"[compiler]\", 1)\n    ])", "label": 1}
{"function": "  def test_calculate_samples_count_per_handler_(self):\n    counters = ipr.calculate_samples_count_per_handler([\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"foo\", \"BytecodeHandler:bar\"],\n      [\"beep\", \"BytecodeHandler:bar\"],\n    ])\n    self.assertItemsEqual(counters, [(\"bar\", 3)])", "label": 1}
{"function": "  def test_multiple_handlers(self):\n    perf_stream = StringIO.StringIO(\"\"\"\n        0000 foo(bar)\n        1234 BytecodeHandler:first\n        5678 a::random::call<to>(something, else)\n        9abc BytecodeHandler:second\n        def0 otherIrrelevant(stuff)\n        1111 entrypoint\n    \"\"\")\n    callchains = list(ipr.collapsed_callchains_generator(perf_stream, False))\n    self.assertListEqual(callchains, [\n      [\"foo\", \"BytecodeHandler:first\", \"[interpreter]\"],\n    ])", "label": 1}
{"function": "  def test_compiler_symbols_regex(self):\n    compiler_symbols = [\n      \"v8::internal::Parser\",\n      \"v8::internal::(anonymous namespace)::Compile\",\n      \"v8::internal::Compiler::foo\",\n    ]\n    for compiler_symbol in compiler_symbols:\n      self.assertTrue(ipr.COMPILER_SYMBOLS_RE.match(compiler_symbol))", "label": 1}
{"function": "  def test_jit_code_symbols_regex(self):\n    jit_code_symbols = [\n      \"LazyCompile:~Foo blah.js\",\n      \"Eval:*\",\n      \"Script:*Bar tmp.js\",\n    ]\n    for jit_code_symbol in jit_code_symbols:\n      self.assertTrue(ipr.JIT_CODE_SYMBOLS_RE.match(jit_code_symbol))", "label": 1}
{"function": "  def test_strip_function_parameters(self):\n    def should_match(signature, name):\n      self.assertEqual(ipr.strip_function_parameters(signature), name)\n\n    should_match(\"foo(bar)\", \"foo\"),\n    should_match(\"Foo(foomatic::(anonymous)::bar(baz))\", \"Foo\"),\n    should_match(\"v8::(anonymous ns)::bar<thing(with, parentheses)>(baz, poe)\",\n       \"v8::(anonymous ns)::bar<thing(with, parentheses)>\")", "label": 1}
{"function": "    def should_match(signature, name):\n      self.assertEqual(ipr.strip_function_parameters(signature), name)", "label": 1}
{"function": "    def encode(self, input, final=False):\n        return codecs.charmap_encode(input,self.errors,encoding_table)[0]", "label": 1}
{"function": "    def decode(self, input, final=False):\n        return codecs.charmap_decode(input,self.errors,decoding_table)[0]", "label": 1}
{"function": "def decode(x, canvas): # b * (10 + 3)\n    x = x.view(-1, 10 + 3)\n    stroke = 1 - Decoder(x[:, :10])\n    stroke = stroke.view(-1, width, width, 1)\n    color_stroke = stroke * x[:, -3:].view(-1, 1, 1, 3)\n    stroke = stroke.permute(0, 3, 1, 2)\n    color_stroke = color_stroke.permute(0, 3, 1, 2)\n    stroke = stroke.view(-1, 5, 1, width, width)\n    color_stroke = color_stroke.view(-1, 5, 3, width, width)\n    res = []\n    for i in range(5):\n        canvas = canvas * (1 - stroke[:, i]) + color_stroke[:, i]\n        res.append(canvas)\n    return canvas, res", "label": 1}
{"function": "def small2large(x):\n    # (d * d, width, width) -> (d * width, d * width)    \n    x = x.reshape(args.divide, args.divide, width, width, -1)\n    x = np.transpose(x, (0, 2, 1, 3, 4))\n    x = x.reshape(args.divide * width, args.divide * width, -1)\n    return x", "label": 1}
{"function": "def large2small(x):\n    # (d * width, d * width) -> (d * d, width, width)\n    x = x.reshape(args.divide, width, args.divide, width, 3)\n    x = np.transpose(x, (0, 2, 1, 3, 4))\n    x = x.reshape(canvas_cnt, width, width, 3)\n    return x", "label": 1}
{"function": "def smooth(img):\n    def smooth_pix(img, tx, ty):\n        if tx == args.divide * width - 1 or ty == args.divide * width - 1 or tx == 0 or ty == 0: \n            return img\n        img[tx, ty] = (img[tx, ty] + img[tx + 1, ty] + img[tx, ty + 1] + img[tx - 1, ty] + img[tx, ty - 1] + img[tx + 1, ty - 1] + img[tx - 1, ty + 1] + img[tx - 1, ty - 1] + img[tx + 1, ty + 1]) / 9\n        return img\n\n    for p in range(args.divide):\n        for q in range(args.divide):\n            x = p * width\n            y = q * width\n            for k in range(width):\n                img = smooth_pix(img, x + k, y + width - 1)\n                if q != args.divide - 1:\n                    img = smooth_pix(img, x + k, y + width)\n            for k in range(width):\n                img = smooth_pix(img, x + width - 1, y + k)\n                if p != args.divide - 1:\n                    img = smooth_pix(img, x + width, y + k)\n    return img", "label": 1}
{"function": "def save_img(res, imgid, divide=False):\n    output = res.detach().cpu().numpy() # d * d, 3, width, width    \n    output = np.transpose(output, (0, 2, 3, 1))\n    if divide:\n        output = small2large(output)\n        output = smooth(output)\n    else:\n        output = output[0]\n    output = (output * 255).astype('uint8')\n    output = cv2.resize(output, origin_shape)\n    cv2.imwrite('output/generated' + str(imgid) + '.png', output)", "label": 1}
{"function": "    def smooth_pix(img, tx, ty):\n        if tx == args.divide * width - 1 or ty == args.divide * width - 1 or tx == 0 or ty == 0: \n            return img\n        img[tx, ty] = (img[tx, ty] + img[tx + 1, ty] + img[tx, ty + 1] + img[tx - 1, ty] + img[tx, ty - 1] + img[tx + 1, ty - 1] + img[tx - 1, ty + 1] + img[tx - 1, ty - 1] + img[tx + 1, ty + 1]) / 9\n        return img", "label": 1}
{"function": "    def __init__(self, parent = None):\n        super().__init__(parent)\n        cura.CuraApplication.CuraApplication.getInstance().getPreferences().preferenceChanged.connect(self._onFavoritesChanged)\n        self._onChanged()", "label": 1}
{"function": "    def _onFavoritesChanged(self, preference_key: str) -> None:\n        \"\"\"Triggered when any preference changes, but only handles it when the list of favourites is changed. \"\"\"\n\n        if preference_key != \"cura/favorite_materials\":\n            return\n        self._onChanged()", "label": 1}
{"function": "def alert(code=None, obj=object, strFields=None, trace=None):\n    \"\"\"The alert function is used for writing alerts to the standard error stream.\n    Only the ErrorHandler class can receive alerts via the \"receiveAlert\" method.\n\n    Parameters\n    ----------\n    code: int\n        The 4 digit code for retrieving alert from AlertCatalogue\n    obj: object\n        The object related to the alert e.g., TextComponent object\n    strFields: dict\n        Dict containing relevant values for formatting messages\n    trace: sys.exec_info() traceback object\n            The traceback\n    \"\"\"\n\n    msg = AlertEntry(code, obj, strFields, trace)\n\n    # format the warning into a string for console and logging targets\n    msgAsStr = (\"Alert {code}: {msg}\\n\"\n                \"For more info see https://docs.psychopy.org/alerts/{code}.html\"\n                .format(type=msg.type,\n                        name=msg.name,\n                        code=msg.code,\n                        cat=msg.cat,\n                        msg=msg.msg,\n                        trace=msg.trace))\n    # msgAsStr = (\"Component Type: {type} | \"\n    #             \"Component Name: {name} | \"\n    #             \"Code: {code} | \"\n    #             \"Category: {cat} | \"\n    #             \"Message: {msg} | \"\n    #             \"Traceback: {trace}\".format(type=msg.type,\n    #                                         name=msg.name,\n    #                                         code=msg.code,\n    #                                         cat=msg.cat,\n    #                                         msg=msg.msg,\n    #                                         trace=msg.trace))\n\n    # if a psychopy warning instead of a file-like stderr then pass a raw str\n    if hasattr(sys.stderr, 'receiveAlert'):\n        sys.stderr.receiveAlert(msg)\n    else:\n        # For tests detecting output - change when error handler set up\n        sys.stderr.write(msgAsStr)\n        for handler in _activeAlertHandlers:\n            handler.receiveAlert(msg)", "label": 1}
{"function": "    def __init__(self):\n        self.alert = self.load()", "label": 1}
{"function": "    def alertPath(self):\n        return Path(__file__).parent / \"alertsCatalogue\"", "label": 1}
{"function": "    def alertFiles(self):\n        return list(self.alertPath.glob(\"*[0-9].*\"))", "label": 1}
{"function": "    def load(self):\n        \"\"\"Loads alert catalogue yaml files\n\n        Returns\n        -------\n        dict\n            The alerts catalogue as a Python dictionary\n        \"\"\"\n        alertDict = {}\n\n        for filePath in self.alertFiles:\n            # '{}'.format(filePath) instead of simple open(filePath,'r')\n            # is needed for Py2 support only\n            with open('{}'.format(filePath), 'r') as ymlFile:\n                entry = yaml.load(ymlFile, Loader=yaml.SafeLoader)\n                if entry is None:\n                    continue  # this might be a stub for future entry\n                ID = entry['code']\n                alertDict[ID] = entry\n                if 'url' not in entry:  # give a default URL\n                    entry['url'] = ('https://psychopy.org/alerts/{}.html'\n                                    .format(ID))\n\n        return alertDict", "label": 1}
{"function": "    def __init__(self, code, obj, strFields=None, trace=None):\n        self.label = catalog.alert[code]['label']\n        self.code = catalog.alert[code]['code']\n        self.cat = catalog.alert[code]['cat']\n        self.url = catalog.alert[code]['url']\n        self.obj = obj\n\n        if hasattr(obj, 'type'):\n            self.type = obj.type\n        else:\n            self.type = None\n\n        if hasattr(obj, \"params\"):\n            self.name = obj.params['name'].val\n        else:\n            self.name = None\n\n        if strFields:\n            self.msg = _translate(catalog.alert[code]['msg']).format(**strFields)\n        else:\n            self.msg = _translate(catalog.alert[code]['msg'])\n\n        if trace:\n            self.trace = ''.join(traceback.format_exception(\n                trace[0], trace[1], trace[2]))\n        else:\n            self.trace = None", "label": 1}
{"function": "    def __init__(self, memcached_conn, *, cookie_name=\"AIOHTTP_SESSION\",\n                 domain=None, max_age=None, path='/',\n                 secure=None, httponly=True,\n                 key_factory=lambda: uuid.uuid4().hex,\n                 encoder=json.dumps, decoder=json.loads):\n        super().__init__(cookie_name=cookie_name, domain=domain,\n                         max_age=max_age, path=path, secure=secure,\n                         httponly=httponly,\n                         encoder=encoder, decoder=decoder)\n        self._key_factory = key_factory\n        self.conn = memcached_conn", "label": 1}
{"function": "    def __init__(self, client, config, serializer, deserializer):\n\n        self._client = client\n        self._serialize = serializer\n        self._deserialize = deserializer\n        self.api_version = \"2019-09-01\"\n\n        self.config = config", "label": 1}
{"function": "    def get(\n            self, resource_group_name, ip_groups_name, expand=None, custom_headers=None, raw=False, **operation_config):\n        \"\"\"Gets the specified ipGroups.\n\n        :param resource_group_name: The name of the resource group.\n        :type resource_group_name: str\n        :param ip_groups_name: The name of the ipGroups.\n        :type ip_groups_name: str\n        :param expand: Expands resourceIds (of Firewalls/Network Security\n         Groups etc.) back referenced by the IpGroups resource.\n        :type expand: str\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: returns the direct response alongside the\n         deserialized response\n        :param operation_config: :ref:`Operation configuration\n         overrides<msrest:optionsforoperations>`.\n        :return: IpGroup or ClientRawResponse if raw=true\n        :rtype: ~azure.mgmt.network.v2019_09_01.models.IpGroup or\n         ~msrest.pipeline.ClientRawResponse\n        :raises:\n         :class:`ErrorException<azure.mgmt.network.v2019_09_01.models.ErrorException>`\n        \"\"\"\n        # Construct URL\n        url = self.get.metadata['url']\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'ipGroupsName': self._serialize.url(\"ip_groups_name\", ip_groups_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n        if expand is not None:\n            query_parameters['$expand'] = self._serialize.query(\"expand\", expand, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application/json'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct and send request\n        request = self._client.get(url, query_parameters, header_parameters)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n        if response.status_code == 200:\n            deserialized = self._deserialize('IpGroup', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized", "label": 1}
{"function": "    def _create_or_update_initial(\n            self, resource_group_name, ip_groups_name, parameters, custom_headers=None, raw=False, **operation_config):\n        # Construct URL\n        url = self.create_or_update.metadata['url']\n        path_format_arguments = {\n            'resourceGroupName': self._serialize.url(\"resource_group_name\", resource_group_name, 'str'),\n            'ipGroupsName': self._serialize.url(\"ip_groups_name\", ip_groups_name, 'str'),\n            'subscriptionId': self._serialize.url(\"self.config.subscription_id\", self.config.subscription_id, 'str')\n        }\n        url = self._client.format_url(url, **path_format_arguments)\n\n        # Construct parameters\n        query_parameters = {}\n        query_parameters['api-version'] = self._serialize.query(\"self.api_version\", self.api_version, 'str')\n\n        # Construct headers\n        header_parameters = {}\n        header_parameters['Accept'] = 'application/json'\n        header_parameters['Content-Type'] = 'application/json; charset=utf-8'\n        if self.config.generate_client_request_id:\n            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())\n        if custom_headers:\n            header_parameters.update(custom_headers)\n        if self.config.accept_language is not None:\n            header_parameters['accept-language'] = self._serialize.header(\"self.config.accept_language\", self.config.accept_language, 'str')\n\n        # Construct body\n        body_content = self._serialize.body(parameters, 'IpGroup')\n\n        # Construct and send request\n        request = self._client.put(url, query_parameters, header_parameters, body_content)\n        response = self._client.send(request, stream=False, **operation_config)\n\n        if response.status_code not in [200, 201]:\n            raise models.ErrorException(self._deserialize, response)\n\n        deserialized = None\n\n        if response.status_code == 200:\n            deserialized = self._deserialize('IpGroup', response)\n        if response.status_code == 201:\n            deserialized = self._deserialize('IpGroup', response)\n\n        if raw:\n            client_raw_response = ClientRawResponse(deserialized, response)\n            return client_raw_response\n\n        return deserialized", "label": 1}
{"function": "    def test_fetch(self, n='test_fetch'):\n        c = compat.Consumer(self.connection, queue=n, exchange=n,\n                            routing_key='rkey')\n        assert c.fetch() is None\n        assert c.fetch(no_ack=True) is None\n        assert 'basic_get' in c.backend\n\n        callback_called = [False]\n\n        def receive(payload, message):\n            callback_called[0] = True\n\n        c.backend.to_deliver.append('42')\n        payload = c.fetch().payload\n        assert payload == '42'\n        c.backend.to_deliver.append('46')\n        c.register_callback(receive)\n        assert c.fetch(enable_callbacks=True).payload == '46'\n        assert callback_called[0]", "label": 1}
{"function": "    def test_discard_all_filterfunc_not_supported(self, n='xjf21j21'):\n        c = compat.Consumer(self.connection, queue=n, exchange=n,\n                            routing_key='rkey')\n        with pytest.raises(NotImplementedError):\n            c.discard_all(filterfunc=lambda x: x)\n        c.close()", "label": 1}
{"function": "    def test_wait(self, n='test_wait'):\n\n        class C(compat.Consumer):\n\n            def iterconsume(self, limit=None):\n                yield from range(limit)\n\n        c = C(self.connection,\n              queue=n, exchange=n, routing_key='rkey')\n        assert c.wait(10) == list(range(10))\n        c.close()", "label": 1}
{"function": "    def test_iterqueue(self, n='test_iterqueue'):\n        i = [0]\n\n        class C(compat.Consumer):\n\n            def fetch(self, limit=None):\n                z = i[0]\n                i[0] += 1\n                return z\n\n        c = C(self.connection,\n              queue=n, exchange=n, routing_key='rkey')\n        assert list(c.iterqueue(limit=10)) == list(range(10))\n        c.close()", "label": 1}
{"function": "    def setup(self):\n        self.connection = Connection(transport=Transport)", "label": 1}
{"function": "    def test_providing_channel(self):\n        chan = Mock(name='channel')\n        cs = compat.ConsumerSet(self.connection, channel=chan)\n        assert cs._provided_channel\n        assert cs.backend is chan\n\n        cs.cancel = Mock(name='cancel')\n        cs.close()\n        chan.close.assert_not_called()", "label": 1}
{"function": "    def test_iterconsume(self, _iterconsume, n='test_iterconsume'):\n        c = compat.Consumer(self.connection, queue=n, exchange=n)\n        cs = compat.ConsumerSet(self.connection, consumers=[c])\n        cs.iterconsume(limit=10, no_ack=True)\n        _iterconsume.assert_called_with(c.connection, cs, True, 10)", "label": 1}
{"function": "    def test_revive(self, n='test_revive'):\n        c = compat.Consumer(self.connection, queue=n, exchange=n)\n        cs = compat.ConsumerSet(self.connection, consumers=[c])\n\n        with self.connection.channel() as c2:\n            cs.revive(c2)\n            assert cs.backend is c2", "label": 1}
{"function": "    def test_constructor(self, prefix='0daf8h21'):\n        dcon = {'%s.xyx' % prefix: {'exchange': '%s.xyx' % prefix,\n                                    'routing_key': 'xyx'},\n                '%s.xyz' % prefix: {'exchange': '%s.xyz' % prefix,\n                                    'routing_key': 'xyz'}}\n        consumers = [compat.Consumer(self.connection, queue=prefix + str(i),\n                                     exchange=prefix + str(i))\n                     for i in range(3)]\n        c = compat.ConsumerSet(self.connection, consumers=consumers)\n        c2 = compat.ConsumerSet(self.connection, from_dict=dcon)\n\n        assert len(c.queues) == 3\n        assert len(c2.queues) == 2\n\n        c.add_consumer(compat.Consumer(self.connection,\n                                       queue=prefix + 'xaxxxa',\n                                       exchange=prefix + 'xaxxxa'))\n        assert len(c.queues) == 4\n        for cq in c.queues:\n            assert cq.channel is c.channel\n\n        c2.add_consumer_from_dict(\n            '%s.xxx' % prefix,\n            exchange='%s.xxx' % prefix,\n            routing_key='xxx',\n        )\n        assert len(c2.queues) == 3\n        for c2q in c2.queues:\n            assert c2q.channel is c2.channel\n\n        c.discard_all()\n        assert c.channel.called.count('queue_purge') == 4\n        c.consume()\n\n        c.close()\n        c2.close()\n        assert 'basic_cancel' in c.channel\n        assert 'close' in c.channel\n        assert 'close' in c2.channel", "label": 1}
{"function": "        def receive(payload, message):\n            callback_called[0] = True", "label": 1}
{"function": "    def test_repo_setfilters_wrong_filters_syntax(self, shell):\n        \"\"\"\n        Test do_repo_setfilters with wrong filters syntax\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_setfilters = MagicMock()\n        shell.client.channel.software.setRepoFilters = MagicMock()\n        mprint = MagicMock()\n        logger = MagicMock()\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_setfilters(shell, \"repo foo bar\")\n\n        assert out is 1\n        assert not mprint.called\n        assert not shell.client.channel.software.setRepoFilters.called\n        assert not shell.help_repo_setfilters.called\n        assert logger.error.called\n\n        assert_expect(logger.error.call_args_list, \"Each filter must start with + or -\")", "label": 1}
{"function": "    def test_repo_setfilters(self, shell):\n        \"\"\"\n        Test do_repo_setfilters with wrong filters syntax\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_setfilters = MagicMock()\n        shell.client.channel.software.setRepoFilters = MagicMock()\n        mprint = MagicMock()\n        logger = MagicMock()\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_setfilters(shell, \"repo +emacs -vim\")\n\n        assert out is 0\n        assert not mprint.called\n        assert not shell.help_repo_setfilters.called\n        assert not logger.error.called\n        assert shell.client.channel.software.setRepoFilters.call_count == 1\n        assert shell.client.channel.software.setRepoFilters.called\n\n        assert_args_expect(shell.client.channel.software.setRepoFilters.call_args_list,\n                           [((shell.session, 'repo',\n                              [{'filter': 'emacs', 'flag': '+'},\n                               {'filter': 'vim', 'flag': '-'}]), {})])", "label": 1}
{"function": "    def test_repo_clearfilters_noargs(self, shell):\n        \"\"\"\n        Test do_repo_clearfilters with no arguments.\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_clearfilters = MagicMock()\n        shell.client.channel.software.clearRepoFilters = MagicMock()\n        mprint = MagicMock()\n        logger = MagicMock()\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_clearfilters(shell, \"\")\n\n        assert out is 1\n        assert not mprint.called\n        assert not logger.error.called\n        assert not shell.client.channel.software.clearRepoFilters.called\n        assert shell.help_repo_clearfilters.called", "label": 1}
{"function": "    def test_repo_clearfilters_not_interactive(self, shell):\n        \"\"\"\n        Test do_repo_clearfilters not interactive.\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_clearfilters = MagicMock()\n        shell.client.channel.software.clearRepoFilters = MagicMock()\n        shell.user_confirm = MagicMock(return_value=False)\n        mprint = MagicMock()\n        logger = MagicMock()\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_clearfilters(shell, \"repo --yes\")\n\n        assert out is 0\n        assert not mprint.called\n        assert not shell.help_repo_clearfilters.called\n        assert not logger.error.called\n        assert shell.client.channel.software.clearRepoFilters.called", "label": 1}
{"function": "    def test_repo_clearfilters_interactive(self, shell):\n        \"\"\"\n        Test do_repo_clearfilters interactive.\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_clearfilters = MagicMock()\n        shell.client.channel.software.clearRepoFilters = MagicMock()\n        shell.user_confirm = MagicMock(return_value=True)\n        mprint = MagicMock()\n        logger = MagicMock()\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_clearfilters(shell, \"repo\")\n\n        assert out is 0\n        assert not mprint.called\n        assert not shell.help_repo_clearfilters.called\n        assert not logger.error.called\n        assert shell.client.channel.software.clearRepoFilters.called", "label": 1}
{"function": "    def test_repo_delete_noargs(self, shell):\n        \"\"\"\n        Test do_repo_delete no arguments.\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_delete = MagicMock()\n        shell.client.channel.software.removeRepo = MagicMock()\n        shell.do_repo_list = MagicMock()\n        mprint = MagicMock()\n        logger = MagicMock()\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_delete(shell, \"\")\n\n        assert out is 1\n        assert not mprint.called\n        assert not logger.error.called\n        assert not shell.client.channel.software.removeRepo.called\n        assert shell.help_repo_delete.called", "label": 1}
{"function": "    def test_repo_rename_noargs(self, shell):\n        \"\"\"\n        Test do_repo_rename no arguments.\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_rename = MagicMock()\n        shell.client.channel.software.getRepoDetails = MagicMock()\n        shell.do_repo_list = MagicMock()\n        mprint = MagicMock()\n        logger = MagicMock()\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_rename(shell, \"\")\n\n        assert out is 1\n        assert not mprint.called\n        assert not logger.error.called\n        assert not shell.client.channel.software.getRepoDetails.called\n        assert shell.help_repo_rename.called", "label": 1}
{"function": "    def test_repo_updateurl_noargs(self, shell):\n        \"\"\"\n        Test do_repo_updateurl no arguments.\n\n        :param shell:\n        :return:\n        \"\"\"\n        for arg in [\"\", \"repo\", \"http://foo\", \"http://bar\"]:\n            shell.help_repo_updateurl = MagicMock()\n            shell.client.channel.software.updateRepUrl = MagicMock()\n            shell.do_repo_list = MagicMock()\n            mprint = MagicMock()\n            logger = MagicMock()\n\n            with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                    patch(\"spacecmd.repo.logging\", logger) as lgr:\n                out = spacecmd.repo.do_repo_updateurl(shell, \"\")\n\n            assert out is 1\n            assert not mprint.called\n            assert not logger.error.called\n            assert not shell.client.channel.software.updateRepUrl.called\n            assert shell.help_repo_updateurl.called", "label": 1}
{"function": "    def test_repo_updatessl_interactive(self, shell):\n        \"\"\"\n        Test do_repo_updatessl interactive.\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_rename = MagicMock()\n        shell.client.channel.software.updateRepoSsl = MagicMock()\n        shell.do_repo_list = MagicMock()\n        mprint = MagicMock()\n        logger = MagicMock()\n        prompter = MagicMock(side_effect=[\"name\", \"ca\", \"cert\", \"key\"])\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.prompt_user\", prompter) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_updatessl(shell, \"\")\n\n        assert out is 0\n        assert not mprint.called\n        assert not logger.error.called\n        assert shell.client.channel.software.updateRepoSsl.called\n\n        assert_args_expect(shell.client.channel.software.updateRepoSsl.call_args_list,\n                           [((shell.session, \"name\", \"ca\", \"cert\", \"key\"), {})])", "label": 1}
{"function": "    def test_repo_updatessl_non_interactive(self, shell):\n        \"\"\"\n        Test do_repo_updatessl non-interactive.\n\n        :param shell:\n        :return:\n        \"\"\"\n        shell.help_repo_rename = MagicMock()\n        shell.client.channel.software.updateRepoSsl = MagicMock()\n        shell.do_repo_list = MagicMock()\n        mprint = MagicMock()\n        logger = MagicMock()\n        prompter = MagicMock(return_value=\"\")\n\n        with patch(\"spacecmd.repo.print\", mprint) as prn, \\\n                patch(\"spacecmd.repo.prompt_user\", prompter) as prn, \\\n                patch(\"spacecmd.repo.logging\", logger) as lgr:\n            out = spacecmd.repo.do_repo_updatessl(shell, \"--name name --ca ca --cert cert --key key\")\n\n        assert out is 0\n        assert not mprint.called\n        assert not logger.error.called\n        assert shell.client.channel.software.updateRepoSsl.called\n\n        assert_args_expect(shell.client.channel.software.updateRepoSsl.call_args_list,\n                           [((shell.session, \"name\", \"ca\", \"cert\", \"key\"), {})])", "label": 1}
{"function": "    def assignment_callback(offsets: Mapping[Partition, int]) -> None:\n        assignment_callback.called = True\n        assert inner_consumer.tell() == {partition: 0}\n        assert consumer.tell() == {partition: 0}", "label": 1}
{"function": "    def __init__(self, kv_store, local_g, partition_book):\n        self._kv_store = kv_store\n        self._graph = local_g\n        self.partition_book = partition_book\n        self._roles = {}", "label": 1}
{"function": "    def roles(self):\n        \"\"\"Roles of the client processes\"\"\"\n        return self._roles", "label": 1}
{"function": "    def kv_store(self):\n        \"\"\"Get data store.\"\"\"\n        return self._kv_store", "label": 1}
{"function": "    def kv_store(self, kv_store):\n        self._kv_store = kv_store", "label": 1}
{"function": "    def graph(self):\n        \"\"\"Get graph data.\"\"\"\n        return self._graph", "label": 1}
{"function": "    def graph(self, graph):\n        self._graph = graph", "label": 1}
{"function": "def enable_debugging():\n    set_log_level(logging.DEBUG)", "label": 1}
{"function": "def set_log_level(level):\n    logger.setLevel(level)\n    handler.setLevel(level)", "label": 1}
{"function": "    def __init__(self, lam_over_diam=None, lam=None, diam=None, obscuration=0., flux=1.,\n                 scale_unit=None, gsparams=None):\n        from .angle import arcsec, radians, AngleUnit\n\n        self._obscuration = float(obscuration)\n        self._flux = float(flux)\n        self._gsparams = GSParams.check(gsparams)\n\n        # Parse arguments: either lam_over_diam in arbitrary units, or lam in nm and diam in m.\n        # If the latter, then get lam_over_diam in units of scale_unit, as specified in\n        # docstring.\n        if lam_over_diam is not None:\n            if lam is not None or diam is not None:\n                raise GalSimIncompatibleValuesError(\n                    \"If specifying lam_over_diam, then do not specify lam or diam\",\n                    lam_over_diam=lam_over_diam, lam=lam, diam=diam)\n            self._lod = float(lam_over_diam)\n        else:\n            if lam is None or diam is None:\n                raise GalSimIncompatibleValuesError(\n                    \"If not specifying lam_over_diam, then specify lam AND diam\",\n                    lam_over_diam=lam_over_diam, lam=lam, diam=diam)\n            # In this case we're going to use scale_unit, so parse it in case of string input:\n            if isinstance(scale_unit, str):\n                scale_unit = AngleUnit.from_name(scale_unit)\n            elif scale_unit is None:\n                scale_unit = arcsec\n            self._lod = (1.e-9*float(lam)/float(diam))*(radians/scale_unit)", "label": 1}
{"function": "  def testDelegateUploadFileToObjectZipped(self, mock_unlink):\n    mock_stream = mock.Mock()\n    mock_stream.close = mock.Mock()\n    mock_upload_url = mock.Mock()\n    mock_upload_url.object_name = 'Sample'\n\n    def DelegateUpload():\n      return 'a', 'b'\n\n    elapsed_time, uploaded_object = _DelegateUploadFileToObject(\n        DelegateUpload, mock_upload_url, mock_stream, True, False, False, None)\n    # Ensure results are passed through.\n    self.assertEqual(elapsed_time, 'a')\n    self.assertEqual(uploaded_object, 'b')\n    # Ensure the file was unlinked.\n    self.assertTrue(mock_unlink.called)\n    # Ensure close was called.\n    self.assertTrue(mock_stream.close.called)", "label": 1}
{"function": "  def testDelegateUploadFileToObjectGzipEncoded(self, mock_lock):\n    mock_stream = mock.Mock()\n    mock_stream.close = mock.Mock()\n\n    def DelegateUpload():\n      # Ensure the lock was aquired before the delegate was called.\n      self.assertTrue(mock_lock.__enter__.called)\n      return 'a', 'b'\n\n    elapsed_time, uploaded_object = _DelegateUploadFileToObject(\n        DelegateUpload, 'url', mock_stream, False, True, False, None)\n    # Ensure results are passed through.\n    self.assertEqual(elapsed_time, 'a')\n    self.assertEqual(uploaded_object, 'b')\n    # Ensure close was called.\n    self.assertTrue(mock_stream.close.called)\n    # Ensure the lock was released.\n    self.assertTrue(mock_lock.__exit__.called)", "label": 1}
{"function": "  def testDelegateUploadFileToObjectGzipEncodedComposite(self, mock_lock):\n    mock_stream = mock.Mock()\n    mock_stream.close = mock.Mock()\n\n    def DelegateUpload():\n      # Ensure the lock was not aquired before the delegate was called.\n      self.assertFalse(mock_lock.__enter__.called)\n      return 'a', 'b'\n\n    elapsed_time, uploaded_object = _DelegateUploadFileToObject(\n        DelegateUpload, 'url', mock_stream, False, True, True, None)\n    # Ensure results are passed through.\n    self.assertEqual(elapsed_time, 'a')\n    self.assertEqual(uploaded_object, 'b')\n    # Ensure close was called.\n    self.assertTrue(mock_stream.close.called)\n    # Ensure the lock was released.\n    self.assertFalse(mock_lock.__exit__.called)", "label": 1}
{"function": "  def testContainsWildcardMatchesNotObject(self, mock_CreateWildcardIterator,\n                                           mock_gsutil_api):\n    storage_url = StorageUrlFromString('gs://test/helloworld')\n    mock_CreateWildcardIterator.return_value = iter(\n        [BucketListingPrefix(storage_url)])\n    (exp_url, have_existing_dst_container) = ExpandUrlToSingleBlr(\n        'gs://test/hello*/', mock_gsutil_api, 'project_id', False,\n        CreateOrGetGsutilLogger('copy_test'))\n\n    self.assertTrue(have_existing_dst_container)\n    self.assertEqual(exp_url, storage_url)", "label": 1}
{"function": "  def testContainsWildcardMatchesObject(self, mock_CreateWildcardIterator,\n                                        mock_gsutil_api):\n    storage_url = StorageUrlFromString('gs://test/helloworld')\n    mock_CreateWildcardIterator.return_value = iter(\n        [BucketListingObject(storage_url)])\n    (exp_url, have_existing_dst_container) = ExpandUrlToSingleBlr(\n        'gs://test/hello*/', mock_gsutil_api, 'project_id', False,\n        CreateOrGetGsutilLogger('copy_test'))\n\n    self.assertFalse(have_existing_dst_container)\n    self.assertEqual(exp_url, storage_url)", "label": 1}
{"function": "  def testContainsWildcardMultipleMatches(self, mock_CreateWildcardIterator,\n                                          mock_gsutil_api):\n    mock_CreateWildcardIterator.return_value = iter([\n        BucketListingObject(StorageUrlFromString('gs://test/helloworld')),\n        BucketListingObject(StorageUrlFromString('gs://test/helloworld2'))\n    ])\n    with self.assertRaises(CommandException):\n      ExpandUrlToSingleBlr('gs://test/hello*/', mock_gsutil_api, 'project_id',\n                           False, CreateOrGetGsutilLogger('copy_test'))", "label": 1}
{"function": "  def testContainsWildcardNoMatches(self, mock_CreateWildcardIterator,\n                                    mock_gsutil_api):\n    mock_CreateWildcardIterator.return_value = iter([])\n    with self.assertRaises(CommandException):\n      ExpandUrlToSingleBlr('gs://test/hello*/', mock_gsutil_api, 'project_id',\n                           False, CreateOrGetGsutilLogger('copy_test'))", "label": 1}
{"function": "  def testLocalFileDirectory(self, mock_StorageUrlFromString, mock_gsutil_api):\n    mock_storage_url = mock.Mock()\n    mock_storage_url.isFileUrl.return_value = True\n    mock_storage_url.IsDirectory.return_value = True\n    mock_StorageUrlFromString.return_value = mock_storage_url\n    (exp_url, have_existing_dst_container) = ExpandUrlToSingleBlr(\n        '/home/test', mock_gsutil_api, 'project_id', False,\n        CreateOrGetGsutilLogger('copy_test'))\n\n    self.assertTrue(have_existing_dst_container)\n    self.assertEqual(exp_url, mock_storage_url)", "label": 1}
{"function": "  def testLocalFileNotDirectory(self, mock_StorageUrlFromString,\n                                mock_gsutil_api):\n    mock_storage_url = mock.Mock()\n    mock_storage_url.isFileUrl.return_value = True\n    mock_storage_url.IsDirectory.return_value = False\n    mock_StorageUrlFromString.return_value = mock_storage_url\n    (exp_url, have_existing_dst_container) = ExpandUrlToSingleBlr(\n        '/home/test', mock_gsutil_api, 'project_id', False,\n        CreateOrGetGsutilLogger('copy_test'))\n\n    self.assertFalse(have_existing_dst_container)\n    self.assertEqual(exp_url, mock_storage_url)", "label": 1}
{"function": "  def testNoSlashPrefixExactMatch(self, mock_gsutil_api):\n    mock_gsutil_api.ListObjects.return_value = iter([\n        CloudApi.CsObjectOrPrefix('folder/',\n                                  CloudApi.CsObjectOrPrefixType.PREFIX)\n    ])\n    (exp_url, have_existing_dst_container) = ExpandUrlToSingleBlr(\n        'gs://test/folder', mock_gsutil_api, 'project_id', False,\n        CreateOrGetGsutilLogger('copy_test'))\n\n    self.assertTrue(have_existing_dst_container)\n    self.assertEqual(exp_url, StorageUrlFromString('gs://test/folder'))", "label": 1}
{"function": "    def deserialize(self, group):\n        \"\"\"\n        Overrides main deserialize method to store data in attributes\n        \"\"\"\n        val = group.attrs[self.name]\n        jsonData = json.loads(val)\n        result = self._registry.deserialize(self._obj_class, jsonData)\n        self.inslot.setValue(result)\n        self.dirty = False", "label": 1}
{"function": "        def doMulti(slot, index, size):\n            slot[index].notifyDirty(self.setDirty)\n            slot[index].notifyValueChanged(self.setDirty)", "label": 1}
{"function": "        def extract_index(s):\n            return int(index_capture.match(s).groups()[0])", "label": 1}
{"function": "        def extract_index(s):\n            return int(index_capture.match(s).groups()[0])", "label": 1}
{"function": "    def __init__(self, dist):\n        \"\"\"Create and initialize a new Command object.  Most importantly,\n        invokes the 'initialize_options()' method, which is the real\n        initializer and depends on the actual command being\n        instantiated.\n        \"\"\"\n        # late import because of mutual dependence between these classes\n        from distutils.dist import Distribution\n\n        if not isinstance(dist, Distribution):\n            raise TypeError(\"dist must be a Distribution instance\")\n        if self.__class__ is Command:\n            raise RuntimeError(\"Command is an abstract class\")\n\n        self.distribution = dist\n        self.initialize_options()\n\n        # Per-command versions of the global flags, so that the user can\n        # customize Distutils' behaviour command-by-command and let some\n        # commands fall back on the Distribution's behaviour.  None means\n        # \"not defined, check self.distribution's copy\", while 0 or 1 mean\n        # false and true (duh).  Note that this means figuring out the real\n        # value of each flag is a touch complicated -- hence \"self._dry_run\"\n        # will be handled by __getattr__, below.\n        # XXX This needs to be fixed.\n        self._dry_run = None\n\n        # verbose is largely ignored, but needs to be set for\n        # backwards compatibility (I think)?\n        self.verbose = dist.verbose\n\n        # Some commands define a 'self.force' option to ignore file\n        # timestamps, but methods defined *here* assume that\n        # 'self.force' exists for all commands.  So define it here\n        # just to be safe.\n        self.force = None\n\n        # The 'help' flag is just used for command-line parsing, so\n        # none of that complicated bureaucracy is needed.\n        self.help = 0\n\n        # 'finalized' records whether or not 'finalize_options()' has been\n        # called.  'finalize_options()' itself should not pay attention to\n        # this flag: it is the business of 'ensure_finalized()', which\n        # always calls 'finalize_options()', to respect/update it.\n        self.finalized = 0", "label": 1}
{"function": "    def __getattr__(self, attr):\n        if attr == 'dry_run':\n            myval = getattr(self, \"_\" + attr)\n            if myval is None:\n                return getattr(self.distribution, attr)\n            else:\n                return myval\n        else:\n            raise AttributeError(attr)", "label": 1}
{"function": "    def ensure_finalized(self):\n        if not self.finalized:\n            self.finalize_options()\n        self.finalized = 1", "label": 1}
{"function": "    def initialize_options(self):\n        \"\"\"Set default values for all the options that this command\n        supports.  Note that these defaults may be overridden by other\n        commands, by the setup script, by config files, or by the\n        command-line.  Thus, this is not the place to code dependencies\n        between options; generally, 'initialize_options()' implementations\n        are just a bunch of \"self.foo = None\" assignments.\n\n        This method must be implemented by all command classes.\n        \"\"\"\n        raise RuntimeError(\"abstract method -- subclass %s must override\"\n                           % self.__class__)", "label": 1}
{"function": "    def finalize_options(self):\n        \"\"\"Set final values for all the options that this command supports.\n        This is always called as late as possible, ie.  after any option\n        assignments from the command-line or from other commands have been\n        done.  Thus, this is the place to code option dependencies: if\n        'foo' depends on 'bar', then it is safe to set 'foo' from 'bar' as\n        long as 'foo' still has the same value it was assigned in\n        'initialize_options()'.\n\n        This method must be implemented by all command classes.\n        \"\"\"\n        raise RuntimeError(\"abstract method -- subclass %s must override\"\n                           % self.__class__)", "label": 1}
{"function": "    def dump_options(self, header=None, indent=\"\"):\n        from distutils.fancy_getopt import longopt_xlate\n        if header is None:\n            header = \"command options for '%s':\" % self.get_command_name()\n        self.announce(indent + header, level=log.INFO)\n        indent = indent + \"  \"\n        for (option, _, _) in self.user_options:\n            option = option.translate(longopt_xlate)\n            if option[-1] == \"=\":\n                option = option[:-1]\n            value = getattr(self, option)\n            self.announce(indent + \"%s = %s\" % (option, value),\n                          level=log.INFO)", "label": 1}
{"function": "    def __iadd__(self, other):\n        return self + other", "label": 1}
{"function": "    def __radd__(self, other):\n        return self + other", "label": 1}
{"function": "    def __sub__(self, other):\n        return self + (-other)", "label": 1}
{"function": "    def __isub__(self, other):\n        return self + (-other)", "label": 1}
{"function": "    def __rsub__(self, other):\n        return -(self - other)", "label": 1}
{"function": "    def __mul__(self, other):\n        if isinstance(other, Quaternion):\n            return self.__class__(array=np.dot(self._q_matrix(), other.q))\n        return self * self.__class__(other)", "label": 1}
{"function": "    def __imul__(self, other):\n        return self * other", "label": 1}
{"function": "    def __rmul__(self, other):\n        return self.__class__(other) * self", "label": 1}
{"function": "    def __matmul__(self, other):\n        if isinstance(other, Quaternion):\n            return self.q.__matmul__(other.q)\n        return self.__matmul__(self.__class__(other))", "label": 1}
{"function": "    def __imatmul__(self, other):\n        return self.__matmul__(other)", "label": 1}
{"function": "    def testCrossValidate(self, mock_init):\n        modelbridge = RandomModelBridge()\n        modelbridge.transforms = OrderedDict()\n        modelbridge.parameters = [\"x\", \"y\", \"z\"]\n        with self.assertRaises(NotImplementedError):\n            modelbridge._cross_validate([], [], [])", "label": 1}
{"function": "    def testGen(self, mock_init, mock_gen):\n        # Test with constraints\n        modelbridge = RandomModelBridge()\n        modelbridge.parameters = [\"x\", \"y\", \"z\"]\n        modelbridge.transforms = OrderedDict()\n        modelbridge.model = RandomModel()\n        observation_features, weights, best_obsf, _ = modelbridge._gen(\n            n=3,\n            search_space=self.search_space,\n            pending_observations={},\n            fixed_features=ObservationFeatures({\"z\": 3.0}),\n            optimization_config=None,\n            model_gen_options=self.model_gen_options,\n        )\n        gen_args = mock_gen.mock_calls[0][2]\n        self.assertEqual(gen_args[\"n\"], 3)\n        self.assertEqual(gen_args[\"bounds\"], [(0.0, 1.0), (1.0, 2.0), (0.0, 5.0)])\n        self.assertTrue(\n            np.array_equal(\n                gen_args[\"linear_constraints\"][0],\n                np.array([[1.0, -1, 0.0], [-1.0, 0.0, -1.0]]),\n            )\n        )\n        self.assertTrue(\n            np.array_equal(gen_args[\"linear_constraints\"][1], np.array([[0.0], [-3.5]]))\n        )\n        self.assertEqual(gen_args[\"fixed_features\"], {2: 3.0})\n        self.assertEqual(gen_args[\"model_gen_options\"], {\"option\": \"yes\"})\n        self.assertEqual(\n            observation_features[0].parameters, {\"x\": 1.0, \"y\": 2.0, \"z\": 3.0}\n        )\n        self.assertEqual(\n            observation_features[1].parameters, {\"x\": 3.0, \"y\": 4.0, \"z\": 3.0}\n        )\n        self.assertTrue(np.array_equal(weights, np.array([1.0, 2.0])))\n\n        # Test with no constraints, no fixed feature, no pending observations\n        search_space = SearchSpace(self.parameters[:2])\n        modelbridge.parameters = [\"x\", \"y\"]\n        modelbridge._gen(\n            n=3,\n            search_space=search_space,\n            pending_observations={},\n            fixed_features=ObservationFeatures({}),\n            optimization_config=None,\n            model_gen_options=self.model_gen_options,\n        )\n        gen_args = mock_gen.mock_calls[1][2]\n        self.assertEqual(gen_args[\"bounds\"], [(0.0, 1.0), (1.0, 2.0)])\n        self.assertIsNone(gen_args[\"linear_constraints\"])\n        self.assertIsNone(gen_args[\"fixed_features\"])", "label": 1}
{"function": "    def test_deduplicate(self):\n        sobol = RandomModelBridge(\n            search_space=get_discrete_search_space(),\n            model=SobolGenerator(deduplicate=True),\n            transforms=Cont_X_trans,\n        )\n        for _ in range(24):  # Search space is {[0, 3], [5, 7], {\"red\", \"panda\"}}\n            self.assertEqual(len(sobol.gen(1).arms), 1)\n        with self.assertRaises(SearchSpaceExhausted):\n            sobol.gen(1)", "label": 1}
{"function": "    def __init__(self, file_descr, landscape=False):\n        # Tamanhos em px\n        self.width = 750\n        self.widthCanhoto = 0\n        self.fontSizeTitle = 9\n        self.heightLine = 27\n        self.fontSizeValue = 12\n        self.title = 'Boleto banc\u00e1rio'\n        self.fileDescr = file_descr\n\n        if landscape:\n            raise NotImplementedError('Em desenvolvimento...')\n        else:\n            tpl = string.Template(self._load_template('head.html'))\n            self.html = tpl.substitute(title=self.title, width=self.width,\n                                       font_size_value=self.fontSizeValue,\n                                       height_line=self.heightLine,\n                                       font_size_title=self.fontSizeTitle)", "label": 1}
{"function": "    def _load_template(self, template):\n        pyboleto_dir = os.path.dirname(os.path.abspath(__file__))\n        template_path = os.path.join(pyboleto_dir, 'templates', template)\n        with open(template_path, 'r') as tpl:\n            template_content = tpl.read()\n        return template_content", "label": 1}
{"function": "    def _load_image(self, logo_image):\n        pyboleto_dir = os.path.dirname(os.path.abspath(__file__))\n        image_path = os.path.join(pyboleto_dir, 'media', logo_image)\n        return image_path", "label": 1}
{"function": "    def _drawReciboSacado(self, boletoDados):\n        \"\"\"Imprime o Recibo do Sacado para modelo de p\u00e1gina inteira\n\n        :param boletoDados: Objeto com os dados do boleto a ser preenchido.\n            Deve ser subclasse de :class:`pyboleto.data.BoletoData`\n        :type boletoDados: :class:`pyboleto.data.BoletoData`\n\n        \"\"\"\n        tpl = string.Template(self._load_template('recibo_sacado.html'))\n        tpl_data = {}\n\n        # Cabe\u00e7alho\n        tpl_data['logo_img'] = ''\n        if boletoDados.logo_image:\n            img = codecs.open(self._load_image(boletoDados.logo_image))\n            aux = img.read()\n            aux = base64.b64encode(aux)\n            img_base64 = 'data:image/jpeg;base64,{0}'.format(aux)\n            tpl_data['logo_img'] = img_base64\n        tpl_data['codigo_dv_banco'] = boletoDados.codigo_dv_banco\n\n        # Corpo\n        tpl_data['cedente'] = boletoDados.cedente\n        tpl_data['agencia_conta_cedente'] = boletoDados.agencia_conta_cedente\n        tpl_data['cedente_documento'] = boletoDados.cedente_documento\n\n        data_vencimento = boletoDados.data_vencimento\n        tpl_data['data_vencimento'] = data_vencimento.strftime('%d/%m/%Y')\n        tpl_data['sacado'] = boletoDados.sacado[0]\n        tpl_data['nosso_numero_format'] = boletoDados.format_nosso_numero()\n        tpl_data['numero_documento'] = boletoDados.numero_documento\n\n        data_documento = boletoDados.data_documento\n        tpl_data['data_documento'] = data_documento.strftime('%d/%m/%Y')\n        tpl_data['cedente_endereco'] = boletoDados.cedente_endereco\n\n        valor_doc = self._formataValorParaExibir(boletoDados.valor_documento)\n        tpl_data['valor_documento'] = valor_doc\n\n        # Demonstrativo\n        tpl_data['demonstrativo'] = ''\n        for dm in boletoDados.demonstrativo:\n            tpl_data['demonstrativo'] += '<p>{0}</p>'.format(dm)\n\n        self.html += tpl.substitute(tpl_data)", "label": 1}
{"function": "    def _drawHorizontalCorteLine(self):\n        self.html += '<hr />'", "label": 1}
{"function": "    def _drawReciboCaixa(self, boletoDados):\n        \"\"\"Imprime o Recibo do Caixa\n\n        :param boletoDados: Objeto com os dados do boleto a ser preenchido.\n            Deve ser subclasse de :class:`pyboleto.data.BoletoData`\n        :type boletoDados: :class:`pyboleto.data.BoletoData`\n\n        \"\"\"\n        tpl = string.Template(self._load_template('recibo_caixa.html'))\n        tpl_data = {}\n\n        # Cabe\u00e7alho\n        tpl_data['logo_img'] = ''\n        if boletoDados.logo_image:\n            tpl_data['logo_img'] = self._load_image(boletoDados.logo_image)\n        tpl_data['codigo_dv_banco'] = boletoDados.codigo_dv_banco\n        tpl_data['linha_digitavel'] = boletoDados.linha_digitavel\n\n        # Corpo\n        data_vencimento = boletoDados.data_vencimento\n        tpl_data['data_vencimento'] = data_vencimento.strftime('%d/%m/%Y')\n\n        # value em unicode em data.py\n        if isinstance(boletoDados.local_pagamento, unicode):\n            tpl_data['local_pagamento'] = boletoDados.local_pagamento.encode\n            ('utf-8')\n        else:\n            tpl_data['local_pagamento'] = boletoDados.local_pagamento\n        tpl_data['cedente'] = boletoDados.cedente\n        tpl_data['agencia_conta_cedente'] = boletoDados.agencia_conta_cedente\n\n        data_documento = boletoDados.data_documento\n        tpl_data['data_documento'] = data_documento.strftime('%d/%m/%Y')\n        tpl_data['numero_documento'] = boletoDados.numero_documento\n        tpl_data['especie_documento'] = boletoDados.especie_documento\n        tpl_data['aceite'] = boletoDados.aceite\n\n        data_process = boletoDados.data_processamento\n        tpl_data['data_processamento'] = data_process.strftime('%d/%m/%Y')\n        tpl_data['nosso_numero_format'] = boletoDados.format_nosso_numero()\n        tpl_data['carteira'] = boletoDados.carteira\n        tpl_data['especie'] = boletoDados.especie\n        tpl_data['quantidade'] = boletoDados.quantidade\n\n        valor = self._formataValorParaExibir(boletoDados.valor)\n        tpl_data['valor'] = valor\n\n        valor_doc = self._formataValorParaExibir(boletoDados.valor_documento)\n        tpl_data['valor_documento'] = valor_doc\n\n        # Instru\u00e7\u00f5es\n        tpl_data['instrucoes'] = ''\n        for instrucao in boletoDados.instrucoes:\n            tpl_data['instrucoes'] += '<p>{0}</p>'.format(instrucao)\n\n        # Rodap\u00e9\n        tpl_data['sacado_info'] = ''\n        for linha_sacado in boletoDados.sacado:\n            tpl_data['sacado_info'] += '<p>{0}</p>'.format(linha_sacado)\n\n        # C\u00f3digo de barras\n        tpl_data['barcode'] = self._codigoBarraI25(boletoDados.barcode)\n\n        self.html += tpl.substitute(tpl_data)", "label": 1}
{"function": "    def drawBoletoCarneDuplo(self, boletoDados1, boletoDados2=None):\n        \"\"\"Imprime um boleto tipo carn\u00ea com 2 boletos por p\u00e1gina.\n\n        :param boletoDados1: Objeto com os dados do boleto a ser preenchido.\n            Deve ser subclasse de :class:`pyboleto.data.BoletoData`\n        :param boletoDados2: Objeto com os dados do boleto a ser preenchido.\n            Deve ser subclasse de :class:`pyboleto.data.BoletoData`\n        :type boletoDados1: :class:`pyboleto.data.BoletoData`\n        :type boletoDados2: :class:`pyboleto.data.BoletoData`\n\n        \"\"\"\n        raise NotImplementedError('Em desenvolvimento')", "label": 1}
{"function": "    def repr_array(self, x, level):\n        if not x:\n            return \"array('%s')\" % x.typecode\n        header = \"array('%s', [\" % x.typecode\n        return self._repr_iterable(x, level, header, '])', self.maxarray)", "label": 1}
{"function": "    def repr_set(self, x, level):\n        if not x:\n            return 'set()'\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, '{', '}', self.maxset)", "label": 1}
{"function": "    def repr_frozenset(self, x, level):\n        if not x:\n            return 'frozenset()'\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'frozenset({', '})',\n                                   self.maxfrozenset)", "label": 1}
{"function": "    def repr_deque(self, x, level):\n        return self._repr_iterable(x, level, 'deque([', '])', self.maxdeque)", "label": 1}
{"function": "    def repr_dict(self, x, level):\n        n = len(x)\n        if n == 0: return '{}'\n        if level <= 0: return '{...}'\n        newlevel = level - 1\n        repr1 = self.repr1\n        pieces = []\n        for key in islice(_possibly_sorted(x), self.maxdict):\n            keyrepr = repr1(key, newlevel)\n            valrepr = repr1(x[key], newlevel)\n            pieces.append('%s: %s' % (keyrepr, valrepr))\n        if n > self.maxdict: pieces.append('...')\n        s = ', '.join(pieces)\n        return '{%s}' % (s,)", "label": 1}
{"function": "    def repr_str(self, x, level):\n        s = builtins.repr(x[:self.maxstring])\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = builtins.repr(x[:i] + x[len(x)-j:])\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s", "label": 1}
{"function": "    def repr_int(self, x, level):\n        s = builtins.repr(x) # XXX Hope this isn't too slow...\n        if len(s) > self.maxlong:\n            i = max(0, (self.maxlong-3)//2)\n            j = max(0, self.maxlong-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s", "label": 1}
{"function": "    def repr_instance(self, x, level):\n        try:\n            s = builtins.repr(x)\n            # Bugs in x.__repr__() can cause arbitrary\n            # exceptions -- then make up something\n        except Exception:\n            return '<%s instance at %#x>' % (x.__class__.__name__, id(x))\n        if len(s) > self.maxother:\n            i = max(0, (self.maxother-3)//2)\n            j = max(0, self.maxother-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s", "label": 1}
{"function": "        def wrapper(self):\n            key = id(self), get_ident()\n            if key in repr_running:\n                return fillvalue\n            repr_running.add(key)\n            try:\n                result = user_function(self)\n            finally:\n                repr_running.discard(key)\n            return result", "label": 1}
{"function": "def right_mark_index(pinyin_no_number: Text) -> int: ...", "label": 1}
{"function": "\tdef testbanOK(self):\n\t\tself._addDefItems()\n\t\tself.__failManager.setMaxRetry(5)\n\t\t#ticket = FailTicket('193.168.0.128', None)\n\t\tticket = self.__failManager.toBan()\n\t\tself.assertEqual(ticket.getIP(), \"193.168.0.128\")\n\t\tself.assertTrue(isinstance(ticket.getIP(), (str, IPAddr)))\n\n\t\t# finish with rudimentary tests of the ticket\n\t\t# verify consistent str\n\t\tticket_str = str(ticket)\n\t\tticket_repr = repr(ticket)\n\t\tself.assertEqual(\n\t\t\tticket_str,\n\t\t\t'FailTicket: ip=193.168.0.128 time=1167605999.0 bantime=None bancount=0 #attempts=5 matches=[]')\n\t\tself.assertEqual(\n\t\t\tticket_repr,\n\t\t\t'FailTicket: ip=193.168.0.128 time=1167605999.0 bantime=None bancount=0 #attempts=5 matches=[]')\n\t\tself.assertFalse(not ticket)\n\t\t# and some get/set-ers otherwise not tested\n\t\tticket.setTime(1000002000.0)\n\t\tself.assertEqual(ticket.getTime(), 1000002000.0)\n\t\t# and str() adjusted correspondingly\n\t\tself.assertEqual(\n\t\t\tstr(ticket),\n\t\t\t'FailTicket: ip=193.168.0.128 time=1000002000.0 bantime=None bancount=0 #attempts=5 matches=[]')", "label": 1}
{"function": "\tdef testbanNOK(self):\n\t\tself._addDefItems()\n\t\tself.__failManager.setMaxRetry(10)\n\t\tself.assertRaises(FailManagerEmpty, self.__failManager.toBan)", "label": 1}
{"function": "\tdef testWindow(self):\n\t\tself._addDefItems()\n\t\tticket = self.__failManager.toBan()\n\t\tself.assertNotEqual(ticket.getIP(), \"100.100.10.10\")\n\t\tticket = self.__failManager.toBan()\n\t\tself.assertNotEqual(ticket.getIP(), \"100.100.10.10\")\n\t\tself.assertRaises(FailManagerEmpty, self.__failManager.toBan)", "label": 1}
{"function": "\tdef testBgService(self):\n\t\tbgSvc = self.__failManager._FailManager__bgSvc\n\t\tfailManager2nd = FailManager()\n\t\t# test singleton (same object):\n\t\tbgSvc2 = failManager2nd._FailManager__bgSvc\n\t\tself.assertTrue(id(bgSvc) == id(bgSvc2))\n\t\tbgSvc2 = None\n\t\t# test service :\n\t\tself.assertTrue(bgSvc.service(True, True))\n\t\tself.assertFalse(bgSvc.service())\n\t\t# bypass threshold and time:\n\t\tfor i in range(1, bgSvc._BgService__threshold):\n\t\t\tself.assertFalse(bgSvc.service())\n\t\t# bypass time check:\n\t\tbgSvc._BgService__serviceTime = -0x7fffffff\n\t\tself.assertTrue(bgSvc.service())\n\t\t# bypass threshold and time:\n\t\tbgSvc._BgService__serviceTime = -0x7fffffff\n\t\tfor i in range(1, bgSvc._BgService__threshold):\n\t\t\tself.assertFalse(bgSvc.service())\n\t\tself.assertTrue(bgSvc.service(False, True))\n\t\tself.assertFalse(bgSvc.service(False, True))", "label": 1}
{"function": "\tdef setUp(self):\n\t\t\"\"\"Call before every test case.\"\"\"\n\t\tsuper(FailmanagerComplex, self).setUp()\n\t\tself.__failManager = FailManager()\n\t\t# down logging level for all this tests, because of extremely large failure count (several GB on heavydebug)\n\t\tself.__saved_ll = failmanager.logLevel\n\t\tfailmanager.logLevel = 3", "label": 1}
{"function": "\tdef tearDown(self):\n\t\tsuper(FailmanagerComplex, self).tearDown()\n\t\t# restore level\n\t\tfailmanager.logLevel = self.__saved_ll", "label": 1}
{"function": "\tdef _ip_range(maxips):\n\t\tclass _ip(list):\n\t\t\tdef __str__(self):\n\t\t\t\treturn '.'.join(map(str, self))\n\t\t\tdef __repr__(self):\n\t\t\t\treturn str(self)\n\t\t\tdef __key__(self):\n\t\t\t\treturn str(self)\n\t\t\tdef __hash__(self):\n\t\t\t\t#return (int)(struct.unpack('I', struct.pack(\"BBBB\",*self))[0])\n\t\t\t\treturn (int)(self[0] << 24 | self[1] << 16 | self[2] << 8 | self[3])\n\t\ti = 0\n\t\tc = [127,0,0,0]\n\t\twhile i < maxips:\n\t\t\tfor n in range(3,0,-1):\n\t\t\t\tif c[n] < 255:\n\t\t\t\t\tc[n] += 1\n\t\t\t\t\tbreak\n\t\t\t\tc[n] = 0\n\t\t\tyield (i, _ip(c))\n\t\t\ti += 1", "label": 1}
{"function": "\tdef testCheckIPGenerator(self):\n\t\tfor i, ip in self._ip_range(65536 if not unittest.F2B.fast else 1000):\n\t\t\tif i == 254:\n\t\t\t\tself.assertEqual(str(ip), '127.0.0.255')\n\t\t\telif i == 255:\n\t\t\t\tself.assertEqual(str(ip), '127.0.1.0')\n\t\t\telif i == 1000:\n\t\t\t\tself.assertEqual(str(ip), '127.0.3.233')\n\t\t\telif i == 65534:\n\t\t\t\tself.assertEqual(str(ip), '127.0.255.255')\n\t\t\telif i == 65535:\n\t\t\t\tself.assertEqual(str(ip), '127.1.0.0')", "label": 1}
{"function": "\t\t\tdef __str__(self):\n\t\t\t\treturn '.'.join(map(str, self))", "label": 1}
{"function": "\t\t\tdef __repr__(self):\n\t\t\t\treturn str(self)", "label": 1}
{"function": "    def read_image_dims(self, image_data):\n        \"\"\"Read the image dimensions.\n\n        Args:\n            image_data: string of image data.\n        Returns:\n            image_height and image_width.\n        \"\"\"\n        image = self.decode_image(image_data)\n        return image.shape[:2]", "label": 1}
{"function": "    def decode_image(self, image_data):\n        \"\"\"Decode the image data string.\n\n        Args:\n            image_data: string of image data.\n        Returns:\n            Decoded image data.\n        Raises:\n            ValueError: Value of image channels not supported.\n        \"\"\"\n        image = self._session.run(\n            self._decode, feed_dict={self._decode_data: image_data})\n        if len(image.shape) != 3 or image.shape[2] not in (1, 3):\n            raise ValueError('The image channels not supported.')\n\n        return image", "label": 1}
{"function": "    def norm2bytes(value):\n        return value.encode() if isinstance(value, str) and six.PY3 else value", "label": 1}
{"function": "def InParentNamespaceStart(builder): builder.StartObject(0)", "label": 1}
{"function": "def InParentNamespaceEnd(builder): return builder.EndObject()", "label": 1}
{"function": "    def GetRootAsInParentNamespace(cls, buf, offset):\n        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)\n        x = InParentNamespace()\n        x.Init(buf, n + offset)\n        return x", "label": 1}
{"function": "    def InParentNamespaceBufferHasIdentifier(cls, buf, offset, size_prefixed=False):\n        return flatbuffers.util.BufferHasIdentifier(buf, offset, b\"\\x4D\\x4F\\x4E\\x53\", size_prefixed=size_prefixed)", "label": 1}
{"function": "    def Init(self, buf, pos):\n        self._tab = flatbuffers.table.Table(buf, pos)", "label": 1}
{"function": "def test_coordinates_as_bytes():\n    pubkey = UmbralPrivateKey.gen_key().pubkey\n    point = pubkey.point_key\n    stamp = SignatureStamp(verifying_key=pubkey)\n\n    x, y = point.to_affine()\n    x = x.to_bytes(32, 'big')\n    y = y.to_bytes(32, 'big')\n\n    for p in (point, pubkey, stamp):\n        assert get_coordinates_as_bytes(p) == x + y\n        assert get_coordinates_as_bytes(p, x_coord=False) == y\n        assert get_coordinates_as_bytes(p, y_coord=False) == x\n        with pytest.raises(ValueError):\n            _ = get_coordinates_as_bytes(p, x_coord=False, y_coord=False)", "label": 1}
{"function": "def elbo(q, p, sample_dim=None, batch_dim=None, alpha=0.1,\n         size_average=True, reduce=True):\n    r\"\"\"Calculates an importance weighted Monte Carlo estimate of the\n    semi-supervised evidence lower bound (ELBO)\n\n    .. math:: \\frac{1}{B} \\sum_{b=1}^B\n              \\log \\left[\n                \\frac{1}{S} \\sum_{s=1}^S\n                \\frac{p(x^{(b)}, y^{(b)}, z^{(s,b)})}\n                     {q(z^{(s,b)} | x^{(b)})}\n              \\right]\n              + \\frac{\\alpha}{B} \\sum_{b=1}^B\n              \\log \\left[\n                \\frac{1}{S} \\sum_{s=1}^S\n                \\frac{q(y^{(b)}, z^{(s,b)} | x^{(b)})}\n                     {q(z^{(s,b)} | x^{(b)})}\n              \\right]\n\n    The sets of variables :math:`x`, :math:`y` and :math:`z` refer to:\n\n        :math:`x`: The set of conditioned nodes that are present in `p` but\n        are not present in `q`.\n\n        :math:`y`: The set of conditioned nodes in `q`, which may or may\n        not also be present in `q`.\n\n        :math:`z`: The set of sampled nodes present in both `q` and `p`.\n\n    Arguments:\n        q(:obj:`Trace`): The encoder trace.\n        p(:obj:`Trace`): The decoder trace.\n        sample_dim(int, optional): The dimension containing individual samples.\n        batch_dim(int, optional): The dimension containing batch items.\n        alpha(float, default 0.1): Coefficient for the ML term.\n        size_average (bool, optional): By default, the objective is averaged\n            over items in the minibatch. When set to false, the objective is\n            instead summed over the minibatch.\n        reduce (bool, optional): By default, the objective is averaged or\n           summed over items in the minibatch. When reduce is False, losses\n           are returned without averaging or summation.\n    \"\"\"\n    z = [n for n in q.sampled() if n in p]\n    log_pxyz = p.log_joint(sample_dim, batch_dim)\n    log_qz = q.log_joint(sample_dim, batch_dim, z)\n    log_qy = q.log_joint(sample_dim, batch_dim, q.conditioned())\n    log_pq = (log_pxyz - log_qz)\n    if sample_dim is None:\n        objective = log_pq + alpha * log_qy\n    else:\n        objective = log_mean_exp(log_pq, 0)\n        if not isinstance(log_qy, Number):\n            objective = objective + alpha * log_mean_exp(log_qy, 0)\n    if reduce:\n        objective = objective.mean() if size_average else objective.sum()\n    return objective", "label": 1}
{"function": "def readable_server_selector(selection):\n    return selection.with_server_descriptions(\n        [s for s in selection.server_descriptions if s.is_readable])", "label": 1}
{"function": "def writable_server_selector(selection):\n    return selection.with_server_descriptions(\n        [s for s in selection.server_descriptions if s.is_writable])", "label": 1}
{"function": "def secondary_server_selector(selection):\n    return selection.with_server_descriptions(\n        [s for s in selection.server_descriptions\n         if s.server_type == SERVER_TYPE.RSSecondary])", "label": 1}
{"function": "def arbiter_server_selector(selection):\n    return selection.with_server_descriptions(\n        [s for s in selection.server_descriptions\n         if s.server_type == SERVER_TYPE.RSArbiter])", "label": 1}
{"function": "def writable_preferred_server_selector(selection):\n    \"\"\"Like PrimaryPreferred but doesn't use tags or latency.\"\"\"\n    return (writable_server_selector(selection) or\n            secondary_server_selector(selection))", "label": 1}
{"function": "def apply_single_tag_set(tag_set, selection):\n    \"\"\"All servers matching one tag set.\n\n    A tag set is a dict. A server matches if its tags are a superset:\n    A server tagged {'a': '1', 'b': '2'} matches the tag set {'a': '1'}.\n\n    The empty tag set {} matches any server.\n    \"\"\"\n    def tags_match(server_tags):\n        for key, value in tag_set.items():\n            if key not in server_tags or server_tags[key] != value:\n                return False\n\n        return True\n\n    return selection.with_server_descriptions(\n        [s for s in selection.server_descriptions if tags_match(s.tags)])", "label": 1}
{"function": "def apply_tag_sets(tag_sets, selection):\n    \"\"\"All servers match a list of tag sets.\n\n    tag_sets is a list of dicts. The empty tag set {} matches any server,\n    and may be provided at the end of the list as a fallback. So\n    [{'a': 'value'}, {}] expresses a preference for servers tagged\n    {'a': 'value'}, but accepts any server if none matches the first\n    preference.\n    \"\"\"\n    for tag_set in tag_sets:\n        with_tag_set = apply_single_tag_set(tag_set, selection)\n        if with_tag_set:\n            return with_tag_set\n\n    return selection.with_server_descriptions([])", "label": 1}
{"function": "def secondary_with_tags_server_selector(tag_sets, selection):\n    \"\"\"All near-enough secondaries matching the tag sets.\"\"\"\n    return apply_tag_sets(tag_sets, secondary_server_selector(selection))", "label": 1}
{"function": "def member_with_tags_server_selector(tag_sets, selection):\n    \"\"\"All near-enough members matching the tag sets.\"\"\"\n    return apply_tag_sets(tag_sets, readable_server_selector(selection))", "label": 1}
{"function": "    def from_topology_description(cls, topology_description):\n        known_servers = topology_description.known_servers\n        primary = None\n        for sd in known_servers:\n            if sd.server_type == SERVER_TYPE.RSPrimary:\n                primary = sd\n                break\n\n        return Selection(topology_description,\n                         topology_description.known_servers,\n                         topology_description.common_wire_version,\n                         primary)", "label": 1}
{"function": "  def test_error(self):\n    with run_configs.get().start(want_rgb=False) as controller:\n      with self.assertRaises(remote_controller.RequestError):\n        controller.create_game(sc_pb.RequestCreateGame())  # Missing map, etc.\n\n      with self.assertRaises(protocol.ProtocolError):\n        controller.join_game(sc_pb.RequestJoinGame())  # No game to join.", "label": 1}
{"function": "  def test_replay_a_replay(self):\n    run_config = run_configs.get()\n    with run_config.start(want_rgb=False) as controller:\n      map_inst = maps.get(\"Flat64\")\n      map_data = map_inst.data(run_config)\n      interface = sc_pb.InterfaceOptions(raw=True)\n\n      # Play a quick game to generate a replay.\n      create = sc_pb.RequestCreateGame(\n          local_map=sc_pb.LocalMap(\n              map_path=map_inst.path, map_data=map_data))\n      create.player_setup.add(type=sc_pb.Participant)\n      create.player_setup.add(type=sc_pb.Computer, race=sc_common.Terran,\n                              difficulty=sc_pb.VeryEasy)\n      join = sc_pb.RequestJoinGame(race=sc_common.Terran, options=interface)\n\n      controller.create_game(create)\n      controller.join_game(join)\n      controller.step(100)\n      obs = controller.observe()\n      replay_data = controller.save_replay()\n\n      # Run through the replay the first time, verifying that it finishes, but\n      # wasn't recording a replay.\n      start_replay = sc_pb.RequestStartReplay(\n          replay_data=replay_data,\n          map_data=map_data,\n          options=interface,\n          observed_player_id=1)\n\n      controller.start_replay(start_replay)\n      controller.step(1000)\n      obs2 = controller.observe()\n      self.assertEqual(obs.observation.game_loop, obs2.observation.game_loop)\n      with self.assertRaises(protocol.ProtocolError):\n        controller.save_replay()\n\n      # Run through the replay a second time, verifying that it finishes, and\n      # *was* recording a replay.\n      start_replay.record_replay = True\n      controller.start_replay(start_replay)\n      controller.step(1000)\n      obs2 = controller.observe()\n      self.assertEqual(obs.observation.game_loop, obs2.observation.game_loop)\n      replay_data2 = controller.save_replay()\n\n      # Make sure the replay isn't too small. Variance is fine but empty is not.\n      self.assertGreater(len(replay_data2), len(replay_data) * 0.8)\n\n      # Run through the replay a third time, verifying that it finishes, but\n      # still wasn't recording a replay.\n      start_replay.record_replay = False\n      controller.start_replay(start_replay)\n      controller.step(1000)\n      obs3 = controller.observe()\n      self.assertEqual(obs.observation.game_loop, obs3.observation.game_loop)\n      with self.assertRaises(protocol.ProtocolError):\n        controller.save_replay()", "label": 1}
{"function": "def SimplePoint():\n    newpoints = []\n\n    newpoints.append([0.0, 0.0, 0.0])\n\n    return newpoints", "label": 1}
{"function": "def SimpleLine(c1=[0.0, 0.0, 0.0], c2=[2.0, 2.0, 2.0]):\n    newpoints = []\n\n    c3 = Vector(c2) - Vector(c1)\n    newpoints.append([0.0, 0.0, 0.0])\n    newpoints.append([c3[0], c3[1], c3[2]])\n\n    return newpoints", "label": 1}
{"function": "def SimpleAngle(length=1.0, angle=45.0):\n    newpoints = []\n\n    angle = radians(angle)\n    newpoints.append([length, 0.0, 0.0])\n    newpoints.append([0.0, 0.0, 0.0])\n    newpoints.append([length * cos(angle), length * sin(angle), 0.0])\n\n    return newpoints", "label": 1}
{"function": "def SimpleDistance(length=1.0, center=True):\n    newpoints = []\n\n    if center:\n        newpoints.append([-length / 2, 0.0, 0.0])\n        newpoints.append([length / 2, 0.0, 0.0])\n    else:\n        newpoints.append([0.0, 0.0, 0.0])\n        newpoints.append([length, 0.0, 0.0])\n\n    return newpoints", "label": 1}
{"function": "def SimpleCircle(sides=4, radius=1.0):\n    newpoints = []\n\n    angle = radians(360) / sides\n    newpoints.append([radius, 0, 0])\n    if radius != 0 :\n        j = 1\n        while j < sides:\n            t = angle * j\n            x = cos(t) * radius\n            y = sin(t) * radius\n            newpoints.append([x, y, 0])\n            j += 1\n\n    return newpoints", "label": 1}
{"function": "def SimpleEllipse(a=2.0, b=1.0):\n    newpoints = []\n\n    newpoints.append([a, 0.0, 0.0])\n    newpoints.append([0.0, b, 0.0])\n    newpoints.append([-a, 0.0, 0.0])\n    newpoints.append([0.0, -b, 0.0])\n\n    return newpoints", "label": 1}
{"function": "def SimpleArc(sides=0, radius=1.0, startangle=0.0, endangle=45.0):\n    newpoints = []\n\n    startangle = radians(startangle)\n    endangle = radians(endangle)\n    sides += 1\n\n    angle = (endangle - startangle) / sides\n    x = cos(startangle) * radius\n    y = sin(startangle) * radius\n    newpoints.append([x, y, 0])\n    j = 1\n    while j < sides:\n        t = angle * j\n        x = cos(t + startangle) * radius\n        y = sin(t + startangle) * radius\n        newpoints.append([x, y, 0])\n        j += 1\n    x = cos(endangle) * radius\n    y = sin(endangle) * radius\n    newpoints.append([x, y, 0])\n\n    return newpoints", "label": 1}
{"function": "def SimpleSector(sides=0, radius=1.0, startangle=0.0, endangle=45.0):\n    newpoints = []\n\n    startangle = radians(startangle)\n    endangle = radians(endangle)\n    sides += 1\n\n    newpoints.append([0, 0, 0])\n    angle = (endangle - startangle) / sides\n    x = cos(startangle) * radius\n    y = sin(startangle) * radius\n    newpoints.append([x, y, 0])\n    j = 1\n    while j < sides:\n        t = angle * j\n        x = cos(t + startangle) * radius\n        y = sin(t + startangle) * radius\n        newpoints.append([x, y, 0])\n        j += 1\n    x = cos(endangle) * radius\n    y = sin(endangle) * radius\n    newpoints.append([x, y, 0])\n\n    return newpoints", "label": 1}
{"function": "def test__xla_dist_model_create_from_context():\n    # without spawn\n    model = _XlaDistModel.create_from_context()\n\n    assert model.backend() == \"xla-tpu\"\n\n    import torch_xla.core.xla_model as xm\n\n    _assert_model(\n        model,\n        {\n            \"device\": xm.xla_device(),\n            \"local_rank\": 0,\n            \"rank\": 0,\n            \"world_size\": 1,\n            \"node_index\": 0,\n            \"nnodes\": 1,\n            \"nproc_per_node\": 1,\n        },\n    )", "label": 1}
{"function": "def _test__xla_dist_model_create_from_context_in_child_proc(index):\n    model = _XlaDistModel.create_from_context()\n\n    assert model.backend() == \"xla-tpu\"\n\n    import torch_xla.core.xla_model as xm\n\n    _assert_model(\n        model,\n        {\n            \"device\": xm.xla_device(),\n            \"local_rank\": index,\n            \"rank\": xm.get_ordinal(),\n            \"world_size\": xm.xrt_world_size(),\n            \"node_index\": 0,\n            \"nnodes\": 1,\n            \"nproc_per_node\": xm.xrt_world_size(),\n        },\n    )", "label": 1}
{"function": "def test__xla_dist_model_create_from_context_in_child_proc(xmp_executor):\n    n = int(os.environ[\"NUM_TPU_WORKERS\"])\n    xmp_executor(_test__xla_dist_model_create_from_context_in_child_proc, args=(), nprocs=n)", "label": 1}
{"function": "def main_fold(fold):\n    import time\n    import torch.nn as nn\n    import torch.optim as optim\n    import torch_xla.core.xla_model as xm\n    from ignite.engine import Engine, Events\n\n    device = xm.xla_device(fold)\n\n    comp_model = _XlaDistModel.create_from_context()\n    assert comp_model.device() == device\n\n    model = nn.Linear(100, 10)\n\n    model.to(device)  # Move model before creating optimizer\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n    def training_step(engine, _):\n        data = torch.rand(4, 100, device=device)\n        model.train()\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = output.sum()\n        loss.backward()\n        xm.optimizer_step(optimizer, barrier=True)\n        return loss.item()\n\n    trainer = Engine(training_step)\n\n    # THIS CAN BE A CAUSE OF CRASH if DEVICE is OTHER THAN device\n    tensor = torch.tensor([fold + 1.0], dtype=torch.float).to(comp_model.device())\n    xm.all_reduce(\"max\", [tensor,])\n\n    time.sleep(0.01 * fold)\n\n    trainer.run([0] * 100, max_epochs=2)", "label": 1}
{"function": "def test__xla_dist_model_run_parallel_n_threads_without_sync():\n    # tests issue : https://github.com/pytorch/ignite/issues/1096\n    from joblib import Parallel, delayed\n\n    import torch_xla.core.xla_model as xm\n\n    devices = xm.get_xla_supported_devices()\n    folds = 1\n    d = 0\n    if len(devices) > 5:\n        folds = 5\n        d = 1\n    Parallel(n_jobs=folds, backend=\"threading\")(delayed(main_fold)(i + d) for i in range(folds))", "label": 1}
{"function": "    def training_step(engine, _):\n        data = torch.rand(4, 100, device=device)\n        model.train()\n        data = data.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = output.sum()\n        loss.backward()\n        xm.optimizer_step(optimizer, barrier=True)\n        return loss.item()", "label": 1}
{"function": "    def __init__(self, W, base_ring = QQ, prefix='u'):\n        r\"\"\"\n        Initiate the affine nil-Coxeter algebra corresponding to the Weyl\n        group `W` over the base ring.\n\n        EXAMPLES::\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['A',3,1])); U\n            The Nil-Coxeter Algebra of Type A3~ over Rational Field\n            sage: TestSuite(U).run()\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['C',3]), ZZ); U\n            The Nil-Coxeter Algebra of Type C3 over Integer Ring\n            sage: TestSuite(U).run()\n        \"\"\"\n\n        self._W = W\n        self._n = W.n\n        self._base_ring = base_ring\n        self._cartan_type = W.cartan_type()\n        H = IwahoriHeckeAlgebra(W, 0, 0, base_ring=base_ring)\n        super(IwahoriHeckeAlgebra.T,self).__init__(H, prefix=prefix)", "label": 1}
{"function": "    def _repr_(self):\n        r\"\"\"\n        EXAMPLES ::\n\n            sage: NilCoxeterAlgebra(WeylGroup(['A',3,1])) # indirect doctest\n            The Nil-Coxeter Algebra of Type A3~ over Rational Field\n\n        \"\"\"\n\n        return \"The Nil-Coxeter Algebra of Type %s over %s\"%(self._cartan_type._repr_(compact=True), self.base_ring())", "label": 1}
{"function": "    def homogeneous_generator_noncommutative_variables(self, r):\n        r\"\"\"\n        Give the `r^{th}` homogeneous function inside the Nil-Coxeter algebra.\n        In finite type `A` this is the sum of all decreasing elements of length `r`.\n        In affine type `A` this is the sum of all cyclically decreasing elements of length `r`.\n        This is only defined in finite type `A`, `B` and affine types `A^{(1)}`, `B^{(1)}`, `C^{(1)}`, `D^{(1)}`.\n\n        INPUT:\n\n        - ``r`` -- a positive integer at most the rank of the Weyl group\n\n        EXAMPLES::\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['A',3,1]))\n            sage: U.homogeneous_generator_noncommutative_variables(2)\n            u[1,0] + u[2,0] + u[0,3] + u[3,2] + u[3,1] + u[2,1]\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['B',4]))\n            sage: U.homogeneous_generator_noncommutative_variables(2)\n            u[1,2] + u[2,1] + u[3,1] + u[4,1] + u[2,3] + u[3,2] + u[4,2] + u[3,4] + u[4,3]\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['C',3]))\n            sage: U.homogeneous_generator_noncommutative_variables(2)\n            Traceback (most recent call last):\n            ...\n            AssertionError: Analogue of symmetric functions in noncommutative variables is not defined in type ['C', 3]\n\n        TESTS::\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['B',3,1]))\n            sage: U.homogeneous_generator_noncommutative_variables(-1)\n            0\n            sage: U.homogeneous_generator_noncommutative_variables(0)\n            1\n\n        \"\"\"\n        assert (len(self._cartan_type) == 2 and self._cartan_type[0] in ['A','B']) or (len(self._cartan_type) == 3 and self._cartan_type[2] == 1), \"Analogue of symmetric functions in noncommutative variables is not defined in type %s\"%(self._cartan_type)\n        if r >= self._n:\n            return self.zero()\n        return self.sum_of_monomials(w for w in self._W.pieri_factors() if w.length() == r)", "label": 1}
{"function": "    def homogeneous_noncommutative_variables(self,la):\n        r\"\"\"\n        Give the homogeneous function indexed by `la`, viewed inside the Nil-Coxeter algebra.\n        This is only defined in finite type `A`, `B` and affine types `A^{(1)}`, `B^{(1)}`, `C^{(1)}`, `D^{(1)}`.\n\n        INPUT:\n\n        - ``la`` -- a partition with first part bounded by the rank of the Weyl group\n\n        EXAMPLES::\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['B',2,1]))\n            sage: U.homogeneous_noncommutative_variables([2,1])\n            u[1,2,0] + 2*u[2,1,0] + u[0,2,0] + u[0,2,1] + u[1,2,1] + u[2,1,2] + u[2,0,2] + u[1,0,2]\n\n        TESTS::\n\n            sage: U = NilCoxeterAlgebra(WeylGroup(['B',2,1]))\n            sage: U.homogeneous_noncommutative_variables([])\n            1\n\n        \"\"\"\n        return prod(self.homogeneous_generator_noncommutative_variables(p) for p in la)", "label": 1}
{"function": "def get_arch_info(a):\n    return (a.cc, a.update_flags, a.writeback, copy_ctypes_list(a.operands[:a.op_count]))", "label": 1}
{"function": "    def imm(self):\n        return self.value.imm", "label": 1}
{"function": "    def reg(self):\n        return self.value.reg", "label": 1}
{"function": "    def fp(self):\n        return self.value.fp", "label": 1}
{"function": "    def mem(self):\n        return self.value.mem", "label": 1}
{"function": "    def pstate(self):\n        return self.value.pstate", "label": 1}
{"function": "    def sys(self):\n        return self.value.sys", "label": 1}
{"function": "    def prefetch(self):\n        return self.value.prefetch", "label": 1}
{"function": "    def barrier(self):\n        return self.value.barrier", "label": 1}
{"function": "def _get_content_from_fragment(block, user_id, course, request_factory, mock_get_current_request):\n    \"\"\"\n    Returns the content from the rendered fragment of a block\n    Arguments:\n        block: some sort of xblock descriptor, must implement .scope_ids.usage_id\n        user_id (int): id of user\n        course_id (CourseLocator): id of course\n    \"\"\"\n    fake_request = request_factory.get('')\n    mock_get_current_request.return_value = fake_request\n\n    # Load a block we know will pass access control checks\n    block = load_single_xblock(\n        request=fake_request,\n        user_id=user_id,\n        course_id=six.text_type(course.id),\n        usage_key_string=six.text_type(block.scope_ids.usage_id),\n        course=course,\n        will_recheck_access=True,\n    )\n    # Attempt to render the block, this should return different fragments if the content is gated or not.\n    frag = block.render('student_view')\n    return frag.content", "label": 1}
{"function": "    def _add_partial_read_service(self, ioi, elements):\n        \"\"\"\n        Add the partial read service to the tag IOI\n        \"\"\"\n        RequestService = 0x52\n        RequestPathSize = int(len(ioi)/2)\n        read_service = pack('<BB', RequestService, RequestPathSize)\n        read_service += ioi\n        read_service += pack('<H', int(elements))\n        read_service += pack('<I', self.Offset)\n        return read_service", "label": 1}
{"function": "    def _add_write_service(self, ioi, write_data, data_type):\n        \"\"\"\n        Add the write command stuff to the tagIOI\n        \"\"\"\n        RequestPathSize = int(len(ioi)/2)\n        RequestService = 0x4D\n        write_service = pack('<BB', RequestService, RequestPathSize)\n        write_service += ioi\n\n        if data_type == 160:\n            RequestNumberOfElements = self.StructIdentifier\n            TypeCodeLen = 0x02\n            write_service += pack('<BBHH', data_type, TypeCodeLen, self.StructIdentifier, len(write_data))\n        else:\n            TypeCodeLen = 0x00\n            write_service += pack('<BBH', data_type, TypeCodeLen, len(write_data))\n\n        for v in write_data:\n            try:\n                for i in range(len(v)):\n                    el = v[i]\n                    write_service += pack(self.CIPTypes[data_type][2], el)\n            except Exception:\n                write_service += pack(self.CIPTypes[data_type][2], v)\n\n        return write_service", "label": 1}
{"function": "    def _add_mod_write_service(self, tag_name, ioi, write_data, data_type):\n        \"\"\"\n        This will add the bit level request to the tagIOI\n        Writing to a bit is handled in a different way than\n        other writes\n        \"\"\"\n        element_size = self.CIPTypes[data_type][0]\n        data_len = len(write_data)\n        byte_count = element_size*data_len\n        RequestPathSize = int(len(ioi)/2)\n        RequestService = 0x4E\n        write_request = pack('<BB', RequestService, RequestPathSize)\n        write_request += ioi\n\n        fmt = self.CIPTypes[data_type][2]\n        fmt = fmt.upper()\n        s = tag_name.split('.')\n        if data_type == 211:\n            t = s[len(s)-1]\n            tag, base_tag, index = _parseTagName(t, 0)\n            index %= 32\n        else:\n            index = s[len(s)-1]\n            index = int(index)\n\n        write_request += pack('<h', byte_count)\n        byte = 2**(byte_count*8)-1\n        bits = 2**index\n        if write_data[0]:\n            write_request += pack(fmt, bits)\n            write_request += pack(fmt, byte)\n        else:\n            write_request += pack(fmt, 0x00)\n            write_request += pack(fmt, (byte-bits))\n\n        return write_request", "label": 1}
{"function": "    def _add_frag_write_service(self, count, ioi, write_data, data_type):\n        \"\"\"\n        Add the fragmented write command stuff to the tagIOI\n        \"\"\"\n        path_size = int(len(ioi)/2)\n        service = 0x53\n        request = pack('<BB', service, path_size)\n        request += ioi\n\n        if data_type == 160:\n            request += pack('<BB', data_type, 0x02)\n            request += pack('<H', self.StructIdentifier)\n        else:\n            request += pack('<H', data_type)\n        request += pack('<H', count)\n        request += pack('<I', self.Offset)\n\n        for v in write_data:\n            try:\n                for i in range(len(v)):\n                    el = v[i]\n                    request += pack(self.CIPTypes[data_type][2], el)\n            except Exception:\n                request += pack(self.CIPTypes[data_type][2], v)\n\n        return request", "label": 1}
{"function": "    def _buildMultiServiceHeader(self):\n        \"\"\"\n        Service header for making a multiple tag request\n        \"\"\"\n        MultiService = 0X0A\n        MultiPathSize = 0x02\n        MutliClassType = 0x20\n        MultiClassSegment = 0x02\n        MultiInstanceType = 0x24\n        MultiInstanceSegment = 0x01\n\n        return pack('<BBBBBB',\n                    MultiService,\n                    MultiPathSize,\n                    MutliClassType,\n                    MultiClassSegment,\n                    MultiInstanceType,\n                    MultiInstanceSegment)", "label": 1}
{"function": "    def _buildTagListRequest(self, programName):\n        \"\"\"\n        Build the request for the PLC tags\n        Program scoped tags will pass the program name for the request\n        \"\"\"\n        Service = 0x55\n        PathSegment = b\"\"\n\n        # If we're dealing with program scoped tags...\n        if programName:\n            PathSegment = pack('<BB', 0x91, len(programName)) + programName.encode('utf-8')\n            # if odd number of characters, need to add a byte to the end.\n            if len(programName) % 2:\n                PathSegment += pack('<B', 0x00)\n\n        PathSegment += pack('<H', 0x6B20)\n\n        if self.Offset < 256:\n            PathSegment += pack('<BB', 0x24, self.Offset)\n        else:\n            PathSegment += pack('<HH', 0x25, self.Offset)\n\n        PathSegmentLen = int(len(PathSegment)/2)\n        AttributeCount = 0x03\n        SymbolType = 0x02\n        ByteCount = 0x08\n        SymbolName = 0x01\n        Attributes = pack('<HHHH', AttributeCount, SymbolName, SymbolType, ByteCount)\n        TagListRequest = pack('<BB', Service, PathSegmentLen)\n        TagListRequest += PathSegment + Attributes\n\n        return TagListRequest", "label": 1}
{"function": "    def _parseReply(self, tag_name, elements, data):\n        \"\"\"\n        Gets the replies from the PLC\n        In the case of BOOL arrays and bits of\n            a word, we do some reformating\n        \"\"\"\n        tag, base_tag, index = _parseTagName(tag_name, 0)\n        data_type = self.KnownTags[base_tag][0]\n        bit_count = self.CIPTypes[data_type][0] * 8\n\n        # if bit of word was requested\n        if BitofWord(tag_name):\n            split_tag = tag_name.split('.')\n            bit_pos = split_tag[len(split_tag)-1]\n            bit_pos = int(bit_pos)\n\n            word_count = _getWordCount(bit_pos, elements, bit_count)\n            words = self._getReplyValues(tag_name, word_count, data)\n            vals = self._wordsToBits(tag_name, words, count=elements)\n        elif data_type == 211:\n            word_count = _getWordCount(index, elements, bit_count)\n            words = self._getReplyValues(tag_name, word_count, data)\n            vals = self._wordsToBits(tag_name, words, count=elements)\n        else:\n            vals = self._getReplyValues(tag_name, elements, data)\n\n        return vals", "label": 1}
{"function": "    def _getReplyValues(self, tag_name, elements, data):\n        \"\"\"\n        Gather up all the values in the reply/replies\n        \"\"\"\n        tag, base_tag, index = _parseTagName(tag_name, 0)\n        data_type = self.KnownTags[base_tag][0]\n        fmt = self.CIPTypes[data_type][2]\n        vals = []\n\n        data_size = self.CIPTypes[data_type][0]\n        numbytes = len(data)-data_size\n        counter = 0\n\n        # this is going to check if the data type was a struct\n        # if so, return the raw data\n        if data_type == 160:\n            tmp = unpack_from('<h', data, 2)[0]\n            if tmp != self.StructIdentifier:\n                d = data[4:4+len(data)]\n                vals.append(d)\n                self.Offset += len(data)\n                return vals\n\n        while True:\n            index = 2+(counter*data_size)\n            if index > numbytes:\n                break\n            if data_type == 160:\n                index = 4+(counter*data_size)\n                name_len = unpack_from('<L', data, index)[0]\n                s = data[index+4:index+4+name_len]\n                vals.append(str(s.decode(self.StringEncoding)))\n\n            elif data_type == 218:\n                # remove the data type\n                data = data[2:] \n                while len(data) > 0:\n                    # get the next string length\n                    length = unpack_from('<B', data, 0)[0]\n                    # remove the length from the packet\n                    data = data[1:]\n                    # grab the string\n                    s = data[:length]\n                    vals.append(str(s.decode(self.StringEncoding)))\n                    # remove the string from the packet\n                    data = data[length:]\n                break\n            else:\n                returnvalue = unpack_from(fmt, data, index)[0]\n                vals.append(returnvalue)\n\n            self.Offset += data_size\n            counter += 1\n\n        return vals", "label": 1}
{"function": "    def _initial_read(self, tag, base_tag, data_type):\n        \"\"\"\n        Store each unique tag read in a dict so that we can retreive the\n        data type or data length (for STRING) later\n        \"\"\"\n        # if a tag already exists, return True\n        if base_tag in self.KnownTags:\n            return tag, None, 0\n        if data_type:\n            self.KnownTags[base_tag] = (data_type, 0)\n            return tag, None, 0\n\n        ioi = self._buildTagIOI(base_tag, data_type)\n        request = self._add_partial_read_service(ioi, 1)\n\n        # send our tag read request\n        status, ret_data = self.conn.send(request)\n\n        # make sure it was successful\n        if status == 0 or status == 6:\n            data_type = unpack_from('<B', ret_data, 50)[0]\n            data_len = unpack_from('<H', ret_data, 2)[0]\n            self.KnownTags[base_tag] = (data_type, data_len)\n            return tag, None, 0\n        else:\n            return tag, None, status", "label": 1}
{"function": "    def _convert_write_data(self, tag, data_type, write_values):\n        \"\"\"\n        In order to handle write requests that are larger than a single\n        packet, we'll break up the values to write into multiple lists\n        of values.  The size of each list will be calculated based on the\n        connection size, length of the tag name and the data type.\n        \"\"\"\n        # packet header is always 110 bytes\n        packet_overhead = 110\n        # calculate number of bytes tag name will occupy\n        tag_length = len(tag) + len(tag) % 2\n        # calculate the available space (in bytes) for the write values\n        space_for_payload = self.ConnectionSize - packet_overhead - tag_length\n\n        # calculate how many bytes per value are required\n        bytes_per_value  = self.CIPTypes[data_type][0]\n        # calculate the limit for values in each request\n        limit = int(space_for_payload / bytes_per_value)\n        # split the list up into multiple smaller lists\n        chunks = [write_values[x:x+limit] for x in range(0, len(write_values), limit)]\n        return chunks", "label": 1}
{"function": "    def __init__(self, argv=sys.argv[1:]):\n        args = self.args = []\n        for aa in argv:\n            args.append(arg(len(args) + 1, aa))", "label": 1}
{"function": "    def items(self):\n        return [('spam%d' % a.num, a) for a in self.args]", "label": 1}
{"function": "    def values(self):\n        return self.args", "label": 1}
{"function": "    def getPhysicalRoot(self):\n        return self", "label": 1}
{"function": "    def on_dialog(self, dialog, title):\n        pass", "label": 1}
{"function": "    def test_mean(self):\n        self.check('CAN15/GMPEtInterface_high_combo.csv',\n                   max_discrep_percentage=100.)", "label": 1}
{"function": "    def test_mean(self):\n        self.check('CAN15/GMPEtInterface_Low_combo.csv',\n                   max_discrep_percentage=900.)", "label": 1}
{"function": "    def test_mean(self):\n        self.check('CAN15/GMPEtInterface_med_combo.csv',\n                   max_discrep_percentage=200.)", "label": 1}
{"function": "    def get(self):\n        status_active = FuzzingJobState.query.filter_by(name='Active').first()\n        status_completed = FuzzingJobState.query.filter_by(name='Completed').first()\n        status_queued = FuzzingJobState.query.filter_by(name='Queued').first()\n\n        total_job_count = FuzzingJob.query.count()\n        active_job_count = FuzzingJob.query.filter_by(state_id=status_active.id).count()\n        completed_job_count = FuzzingJob.query.filter_by(state_id=status_completed.id).count()\n        queued_job_count = FuzzingJob.query.filter_by(state_id=status_queued.id).count()\n        crash_count = FuzzingCrash.query.count()\n        node_count = FuzzingHost.query.count()\n        return {\n            'total_job_count': total_job_count,\n            'active_job_count': active_job_count,\n            'completed_job_count': completed_job_count,\n            'queued_job_count': queued_job_count,\n            'crash_count': crash_count,\n            'node_count': node_count,\n            'serverTime' : str(datetime.now())\n        }, 200", "label": 1}
{"function": "    def parse(self, response):\n        if response.url == self.start_urls[0]:\n            match = re.search(\"var csv_url = '(.*)'\",\n                              response.body_as_unicode())\n            assert match.group(1)\n\n            yield scrapy.Request(f\"https://www.arco.com{match.group(1)}\")\n        else:\n            for station in csv.DictReader(response.body_as_unicode().splitlines()):\n                yield GeojsonPointItem(\n                    lat=station['Lat'],\n                    lon=station['Lng'],\n                    name=station['StoreName'],\n                    addr_full=station['Address'],\n                    city=station['City'],\n                    state=station['State'],\n                    postcode=station['Zip'],\n                    country='US' if len(station['State']) == 2 else 'MX',\n                    phone=station['Phone'],\n                    ref=station['StoreNumber'],\n                    extras={\n                        'amenity:fuel': True,\n                        'payment:credit_cards': station['CreditCards'] == '1',\n                        'shop': 'convenience' if station['ampm'] == '1' else None\n                    }\n                )", "label": 1}
{"function": "    def __init__(self, name):\n        self.name = name", "label": 1}
{"function": "    def as_cmd(self, func):\n        return c.Function(c.NSName(self.name))", "label": 1}
{"function": "    def __init__(self, ass):\n        super().__init__(ass.mem_get)\n        self._ass = ass", "label": 1}
{"function": "    def declare(self):\n        self._ass.mem_addr.usage_read()\n        self._ass.mem_buf.usage_write()", "label": 1}
{"function": "    def copy(self):\n        return MemGetFn(self._ass)", "label": 1}
{"function": "    def __init__(self, ass):\n        super().__init__(ass.mem_set)\n        self._ass = ass", "label": 1}
{"function": "    def declare(self):\n        self._ass.mem_addr.usage_read()\n        self._ass.mem_buf.usage_read()", "label": 1}
{"function": "    def copy(self):\n        return MemSetFn(self._ass)", "label": 1}
{"function": "    def reduce(self, acc, val):\n        # Either USE_MEM or empty string\n        return val or acc", "label": 1}
{"function": "    def apply(self, top, value):\n        return value", "label": 1}
{"function": "def _add_outer_none(shape):\n    \"\"\"\n    Add None as an outer dimension for the shape.\n    \"\"\"\n    if isinstance(shape, tf.TensorShape):\n        return [None] + shape.dims\n    return [None, shape]", "label": 1}
{"function": "    def __init__(self, session, action_dist, obs_vectorizer):\n        \"\"\"\n        Construct a recurrent model.\n        \"\"\"\n        super().__init__(session, action_dist, obs_vectorizer)\n\n        self.seq_lens_ph = tf.placeholder(tf.int32, shape=(None,))\n        self.is_init_state_ph = tf.placeholder(tf.bool, shape=(None,))\n        self.mask_ph = tf.placeholder(tf.bool, (None, None))\n\n        # Set this to a variable or a tuple of variables\n        # for your model's initial state.\n        self.init_state_vars = None\n\n        # Set this to a placeholder for a batch of\n        # observation sequences.\n        self.obs_ph = None\n\n        # Set this to a placeholder or a tuple of\n        # placeholders for the first state in each\n        # sequence.\n        #\n        # If a value in _is_init_state is True, then the\n        # corresponding entry here is ignored.\n        self.first_state_ph = None\n\n        # Set these to the model outputs.\n        self.actor_out = None\n        self.critic_out = None\n        self.states_out = None", "label": 1}
{"function": "    def scale_outputs(self, scale):\n        \"\"\"\n        Scale the network outputs by the given amount.\n\n        This may be called right after initializing the\n        model to help deal with different reward scales.\n        \"\"\"\n        self.critic_out *= scale\n        self.actor_out *= scale", "label": 1}
{"function": "    def stateful(self):\n        return True", "label": 1}
{"function": "    def start_state(self, batch_size):\n        if isinstance(self.init_state_vars, tuple):\n            res = []\n            # pylint: disable=E1133\n            for var in self.init_state_vars:\n                var_val = self.session.run(var)\n                res.append(np.array([var_val] * batch_size))\n            return tuple(res)\n        var_val = self.session.run(self.init_state_vars)\n        return np.array([var_val] * batch_size)", "label": 1}
{"function": "    def step(self, observations, states):\n        vec_obs = self.obs_vectorizer.to_vecs(observations)\n        feed_dict = {\n            self.seq_lens_ph: [1] * len(observations),\n            self.is_init_state_ph: [False] * len(observations),\n            self.obs_ph: [[x] for x in vec_obs],\n            self.mask_ph: [[1]] * len(observations)\n        }\n\n        if isinstance(self.first_state_ph, tuple):\n            assert isinstance(states, tuple)\n            for key, value in zip(self.first_state_ph, states):\n                feed_dict[key] = value\n        else:\n            feed_dict[self.first_state_ph] = states\n\n        acts, vals, states = self.session.run((self.actor_out,\n                                               self.critic_out,\n                                               self.states_out),\n                                              feed_dict)\n        action_params = [a[0] for a in acts]\n        return {\n            'action_params': action_params,\n            'actions': self.action_dist.sample(action_params),\n            'states': states,\n            'values': np.array(vals).flatten()\n        }", "label": 1}
{"function": "    def batch_outputs(self):\n        seq_shape = tf.shape(self.actor_out)\n        out_count = seq_shape[0] * seq_shape[1]\n        actor_shape = (out_count,) + self.action_dist.param_shape\n        critic_shape = (out_count,)\n        masks = tf.reshape(self.mask_ph, critic_shape)\n        return (tf.boolean_mask(tf.reshape(self.actor_out, actor_shape), masks),\n                tf.boolean_mask(tf.reshape(self.critic_out, critic_shape), masks))", "label": 1}
{"function": "    def batches(self, rollouts, batch_size=None):\n        sizes = [r.num_steps for r in rollouts]\n        for rollout_indices in mini_batches(sizes, batch_size):\n            batch = [rollouts[i] for i in rollout_indices]\n            max_len = max([r.num_steps for r in batch])\n            obs_seqs = []\n            is_inits = []\n            masks = []\n            rollout_idxs = []\n            timestep_idxs = []\n            for rollout_idx, rollout in zip(rollout_indices, batch):\n                obs_seq = list(self.obs_vectorizer.to_vecs(rollout.step_observations))\n                empty_obs = np.zeros(np.array(obs_seq[0]).shape)\n                obs_seqs.append(_pad(obs_seq, max_len, value=empty_obs))\n                is_inits.append(not rollout.trunc_start)\n                masks.append(_pad([True] * rollout.num_steps, max_len))\n                rollout_idxs.extend([rollout_idx] * rollout.num_steps)\n                timestep_idxs.extend(range(rollout.num_steps))\n            feed_dict = {\n                self.obs_ph: obs_seqs,\n                self.is_init_state_ph: is_inits,\n                self.mask_ph: masks,\n                self.seq_lens_ph: [r.num_steps for r in batch]\n            }\n            self._add_first_states(feed_dict, batch)\n            yield {\n                'rollout_idxs': rollout_idxs,\n                'timestep_idxs': timestep_idxs,\n                'feed_dict': feed_dict\n            }", "label": 1}
{"function": "    def create_state_fields(self, dtype, state_size):\n        \"\"\"\n        Set self.first_state_ph and self.init_state_vars\n        with the given TF datatype and the state size.\n\n        The state size may be an integer, a TensorShape,\n        or a tuple thereof.\n        \"\"\"\n        if isinstance(state_size, tuple):\n            self.first_state_ph = ()\n            self.init_state_vars = ()\n            for sub_shape in state_size:\n                placeholder = tf.placeholder(dtype, _add_outer_none(sub_shape))\n                variable = tf.Variable(tf.zeros(sub_shape))\n                self.first_state_ph += (placeholder,)\n                self.init_state_vars += (variable,)\n        else:\n            placeholder = tf.placeholder(dtype, _add_outer_none(state_size))\n            variable = tf.Variable(tf.zeros(state_size))\n            self.first_state_ph = placeholder\n            self.init_state_vars = variable", "label": 1}
{"function": "    def _add_first_states(self, feed_dict, rollouts):\n        \"\"\"\n        Add first state placeholders for the rollouts.\n        \"\"\"\n        if isinstance(self.init_state_vars, tuple):\n            for i, placeholder in enumerate(self.first_state_ph):\n                first_states = [r.start_state[i][0] for r in rollouts]\n                feed_dict[placeholder] = first_states\n        else:\n            first_states = [r.start_state[0] for r in rollouts]\n            feed_dict[self.first_state_ph] = first_states", "label": 1}
{"function": "    def set_preferred(self, master, preferred):\n        self.preferred_name[master] = preferred", "label": 1}
{"function": "    def _num_uncovered(key):\n        (total, covered, percent) = covdata[key].coverage()\n        return total - covered", "label": 1}
{"function": "    def _percent_uncovered(key):\n        (total, covered, percent) = covdata[key].coverage()\n        if covered:\n            return -1.0*covered/total\n        else:\n            return total or 1e6", "label": 1}
{"function": "    def _alpha(key):\n        return key", "label": 1}
{"function": "    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)", "label": 1}
{"function": "    def __iter__(self):\n        for _ in range(self._retry_count):\n            try:\n                db.session.rollback()\n                return super().__iter__()\n            except OperationalError:\n                app.logger.warning(\"Retrying to reach database...\")\n                time.sleep(self._retry_sleep_interval_sec)\n\n        app.logger.critical(\"Database couldn't be reached.\")\n        raise DatabaseNotReachable", "label": 1}
{"function": "    def create(cls, **kwargs) -> Union[bool, Any]:\n        \"\"\"Create a new record and save it the database.\"\"\"\n        instance = cls(**kwargs)\n        return instance.save()", "label": 1}
{"function": "    def update(self, commit: bool = True, **kwargs) -> Union[bool, Any]:\n        \"\"\"Update specific fields of a record.\"\"\"\n        for attr, value in kwargs.items():\n            setattr(self, attr, value)\n        return self.save() if commit else self", "label": 1}
{"function": "    def save(self, commit: bool = True) -> Union[bool, Any]:\n        \"\"\"Save the record.\"\"\"\n        db.session.add(self)\n        if commit:\n            db.session.commit()\n        return self", "label": 1}
{"function": "    def delete(self, commit: bool = True) -> Union[bool, Any]:\n        \"\"\"Remove the record from the database.\"\"\"\n        db.session.delete(self)\n        return commit and db.session.commit()", "label": 1}
{"function": "    def runCodeBlocks(self):\n        \"\"\"\u4ee3\u7801\u5757\u4e2d\u53ef\u4ee5\u653eQPalette\u7684\u5b9a\u4e49\uff0c\u5982\u679c\u6709Qpalette\u5b9a\u4e49\uff0c\u5728\u9884\u89c8\u7684\u65f6\u5019\u9700\u8981\u5148\u8fd0\u884c\u4ee3\u7801\u5757\u4e2d\u7684\u5b9a\u4e49\"\"\"\n        self.codeBlocks = self.reCodeBlock.findall(self.srctext)\n        if self.codeBlocks:\n            try:\n                eval(\"from PyQt5.QtGui import QPalette\")\n                for code in self.codeBlocks:\n                    exec(code)\n            except:\n                print(\"warning: codeblock in qsst exec error.\")", "label": 1}
{"function": "    def convertQss(self):\n        \"\"\"\u6839\u636evarDict\u4e2d\u53d8\u91cf\u7684\u503c\uff0c\u628a\u6a21\u677f\u6587\u4ef6\u4e2d\u5f15\u7528\u7684\u53d8\u91cf\u7528\u503c\u66ff\u6362\uff0c\u8f6c\u6362\u4e3aqss\u6587\u4ef6\u3002\n        \"\"\"\n\n        qssStr = self.srctext\n        varDict = self.varDict\n        self.loadVars()\n        # \u5220\u9664\u53d8\u91cf\u5b9a\u4e49\n        varsDefined = re.compile(r'[$](\\w+)\\s*=[ \\t]*([#(),.\\w]*)[ \\t;]*[\\r\\n]{0,2}')\n        qssStr = varsDefined.sub(\"\", qssStr)\n\n        for v in self.varDict:\n            if v in varDict.keys():\n                # qssStr = qssStr.replace(\"$\" + v, varDict[v])\n                qssStr = re.sub(r'[$](\\w+)([\\s;]*)', lambda m:'{}{}'.format(varDict[m.group(1)], m.group(2)), qssStr)\n            else:\n                self.varUndefined.append(v)\n                # qssStr = qssStr.replace(\"$\" + v, ' ')\n                qssStr = re.sub(r'[$](\\w+)([\\s;]*)', lambda m:'{}{}'.format(\" \", m.group(2)), qssStr)\n        # \u5220\u9664\u4ee3\u7801\u5757\n        qssStr = self.reCodeBlock.sub(\"\", qssStr)\n        self.qss = qssStr", "label": 1}
{"function": "    def writeVars(self):\n        varDictNew = self.varDict\n        self.loadVars()\n        if self.varDict:  # \u5982\u679c\u6587\u4ef6\u4e2d\u53d8\u91cf\u4e0d\u4e3a\u7a7a\uff0c\u66f4\u65b0\u53d8\u91cf\u503c\n            self.srctext = re.sub(r'[$](\\w+)\\s*=[ \\t]*([#(),.\\w]*)[\\t ]*[;]?',\n                                  lambda m: '${} = {};'.format(m.group(1), varDictNew.get(m.group(1), \"\")),\n                                  self.srctext)\n            if self.varUndefined:  # \u5728\u7b2c\u4e00\u7684\u53d8\u91cf\u5904\u63d2\u5165\u591a\u51fa\u6765\u7684\u53d8\u91cf,\u5f15\u7528\u6bd4\u5b9a\u4e49\u7684\u53d8\u91cf\u591a\u7684\u65f6\u5019\u56de\u51fa\u73b0\u8fd9\u79cd\u60c5\u51b5\n                s = ''\n                for var, val in varDictNew.items():\n                    if var in self.varUndefined:\n                        s += \"$\" + var + \" = \" + val + \";\\n\"\n                self.srctext = re.sub(r'[$](\\w+)\\s*=[ \\t]*([#(),.\\w]*)[\\t ]*[;]?', r'{}$\\1 = \\2;\\n'.format(s),\n                                      self.srctext, 1)\n        else:\n            s = ''\n            for var, val in varDictNew.items():\n                s += \"$\" + var + \" = \" + val + \";\\n\"\n            s += '\\n'\n            self.srctext = s + self.srctext\n        self.loadVars()", "label": 1}
{"function": "  def testNestedMessageDescriptor(self):\n    field_name = 'optional_nested_message'\n    proto_type = unittest_pb2.TestAllTypes\n    self.assertEqual(\n        proto_type.NestedMessage.DESCRIPTOR,\n        proto_type.DESCRIPTOR.fields_by_name[field_name].message_type)", "label": 1}
{"function": "  def testEnums(self):\n    # We test only module-level enums here.\n    # TODO(robinson): Examine descriptors directly to check\n    # enum descriptor output.\n    self.assertEqual(4, unittest_pb2.FOREIGN_FOO)\n    self.assertEqual(5, unittest_pb2.FOREIGN_BAR)\n    self.assertEqual(6, unittest_pb2.FOREIGN_BAZ)\n\n    proto = unittest_pb2.TestAllTypes()\n    self.assertEqual(1, proto.FOO)\n    self.assertEqual(1, unittest_pb2.TestAllTypes.FOO)\n    self.assertEqual(2, proto.BAR)\n    self.assertEqual(2, unittest_pb2.TestAllTypes.BAR)\n    self.assertEqual(3, proto.BAZ)\n    self.assertEqual(3, unittest_pb2.TestAllTypes.BAZ)", "label": 1}
{"function": "  def testExtremeDefaultValues(self):\n    message = unittest_pb2.TestExtremeDefaultValues()\n\n    # Python pre-2.6 does not have isinf() or isnan() functions, so we have\n    # to provide our own.\n    def isnan(val):\n      # NaN is never equal to itself.\n      return val != val\n    def isinf(val):\n      # Infinity times zero equals NaN.\n      return not isnan(val) and isnan(val * 0)\n\n    self.assertTrue(isinf(message.inf_double))\n    self.assertTrue(message.inf_double > 0)\n    self.assertTrue(isinf(message.neg_inf_double))\n    self.assertTrue(message.neg_inf_double < 0)\n    self.assertTrue(isnan(message.nan_double))\n\n    self.assertTrue(isinf(message.inf_float))\n    self.assertTrue(message.inf_float > 0)\n    self.assertTrue(isinf(message.neg_inf_float))\n    self.assertTrue(message.neg_inf_float < 0)\n    self.assertTrue(isnan(message.nan_float))\n    self.assertEqual(\"? ? ?? ?? ??? ??/ ??-\", message.cpp_trigraph)", "label": 1}
{"function": "  def testHasDefaultValues(self):\n    desc = unittest_pb2.TestAllTypes.DESCRIPTOR\n\n    expected_has_default_by_name = {\n        'optional_int32': False,\n        'repeated_int32': False,\n        'optional_nested_message': False,\n        'default_int32': True,\n    }\n\n    has_default_by_name = dict(\n        [(f.name, f.has_default_value)\n         for f in desc.fields\n         if f.name in expected_has_default_by_name])\n    self.assertEqual(expected_has_default_by_name, has_default_by_name)", "label": 1}
{"function": "  def testContainingTypeBehaviorForExtensions(self):\n    self.assertEqual(unittest_pb2.optional_int32_extension.containing_type,\n                     unittest_pb2.TestAllExtensions.DESCRIPTOR)\n    self.assertEqual(unittest_pb2.TestRequired.single.containing_type,\n                     unittest_pb2.TestAllExtensions.DESCRIPTOR)", "label": 1}
{"function": "  def testExtensionScope(self):\n    self.assertEqual(unittest_pb2.optional_int32_extension.extension_scope,\n                     None)\n    self.assertEqual(unittest_pb2.TestRequired.single.extension_scope,\n                     unittest_pb2.TestRequired.DESCRIPTOR)", "label": 1}
{"function": "  def testIsExtension(self):\n    self.assertTrue(unittest_pb2.optional_int32_extension.is_extension)\n    self.assertTrue(unittest_pb2.TestRequired.single.is_extension)\n\n    message_descriptor = unittest_pb2.TestRequired.DESCRIPTOR\n    non_extension_descriptor = message_descriptor.fields_by_name['a']\n    self.assertTrue(not non_extension_descriptor.is_extension)", "label": 1}
{"function": "    def run_test(self):\n        self.log.info('prepare some coins for multiple *rawtransaction commands')\n        self.nodes[2].generate(1)\n        self.sync_all()\n        self.nodes[0].generate(101)\n        self.sync_all()\n        self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(),1.5)\n        self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(),1.0)\n        self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(),5.0)\n        self.sync_all()\n        self.nodes[0].generate(5)\n        self.sync_all()\n\n        self.log.info('Test getrawtransaction on genesis block coinbase returns an error')\n        block = self.nodes[0].getblock(self.nodes[0].getblockhash(0))\n        assert_raises_rpc_error(-5, \"The genesis block coinbase is not considered an ordinary transaction\", self.nodes[0].getrawtransaction, block['merkleroot'])\n\n        self.log.info('Check parameter types and required parameters of createrawtransaction')\n        # Test `createrawtransaction` required parameters\n        assert_raises_rpc_error(-1, \"createrawtransaction\", self.nodes[0].createrawtransaction)\n        assert_raises_rpc_error(-1, \"createrawtransaction\", self.nodes[0].createrawtransaction, [])\n\n        # Test `createrawtransaction` invalid extra parameters\n        assert_raises_rpc_error(-1, \"createrawtransaction\", self.nodes[0].createrawtransaction, [], {}, 0, False, 'foo')\n\n        # Test `createrawtransaction` invalid `inputs`\n        txid = '1d1d4e24ed99057e84c3f80fd8fbec79ed9e1acee37da269356ecea000000000'\n        assert_raises_rpc_error(-3, \"Expected type array\", self.nodes[0].createrawtransaction, 'foo', {})\n        assert_raises_rpc_error(-1, \"JSON value is not an object as expected\", self.nodes[0].createrawtransaction, ['foo'], {})\n        assert_raises_rpc_error(-1, \"JSON value is not a string as expected\", self.nodes[0].createrawtransaction, [{}], {})\n        assert_raises_rpc_error(-8, \"txid must be of length 64 (not 3, for 'foo')\", self.nodes[0].createrawtransaction, [{'txid': 'foo'}], {})\n        assert_raises_rpc_error(-8, \"txid must be hexadecimal string (not 'ZZZ7bb8b1697ea987f3b223ba7819250cae33efacb068d23dc24859824a77844')\", self.nodes[0].createrawtransaction, [{'txid': 'ZZZ7bb8b1697ea987f3b223ba7819250cae33efacb068d23dc24859824a77844'}], {})\n        assert_raises_rpc_error(-8, \"Invalid parameter, missing vout key\", self.nodes[0].createrawtransaction, [{'txid': txid}], {})\n        assert_raises_rpc_error(-8, \"Invalid parameter, missing vout key\", self.nodes[0].createrawtransaction, [{'txid': txid, 'vout': 'foo'}], {})\n        assert_raises_rpc_error(-8, \"Invalid parameter, vout must be positive\", self.nodes[0].createrawtransaction, [{'txid': txid, 'vout': -1}], {})\n        assert_raises_rpc_error(-8, \"Invalid parameter, sequence number is out of range\", self.nodes[0].createrawtransaction, [{'txid': txid, 'vout': 0, 'sequence': -1}], {})\n\n        # Test `createrawtransaction` invalid `outputs`\n        address = self.nodes[0].getnewaddress()\n        address2 = self.nodes[0].getnewaddress()\n        assert_raises_rpc_error(-1, \"JSON value is not an array as expected\", self.nodes[0].createrawtransaction, [], 'foo')\n        self.nodes[0].createrawtransaction(inputs=[], outputs={})  # Should not throw for backwards compatibility\n        self.nodes[0].createrawtransaction(inputs=[], outputs=[])\n        assert_raises_rpc_error(-8, \"Data must be hexadecimal string\", self.nodes[0].createrawtransaction, [], {'data': 'foo'})\n        assert_raises_rpc_error(-5, \"Invalid Feathercoin address\", self.nodes[0].createrawtransaction, [], {'foo': 0})\n        assert_raises_rpc_error(-3, \"Invalid amount\", self.nodes[0].createrawtransaction, [], {address: 'foo'})\n        assert_raises_rpc_error(-3, \"Amount out of range\", self.nodes[0].createrawtransaction, [], {address: -1})\n        assert_raises_rpc_error(-8, \"Invalid parameter, duplicated address: %s\" % address, self.nodes[0].createrawtransaction, [], multidict([(address, 1), (address, 1)]))\n        assert_raises_rpc_error(-8, \"Invalid parameter, duplicated address: %s\" % address, self.nodes[0].createrawtransaction, [], [{address: 1}, {address: 1}])\n        assert_raises_rpc_error(-8, \"Invalid parameter, duplicate key: data\", self.nodes[0].createrawtransaction, [], [{\"data\": 'aa'}, {\"data\": \"bb\"}])\n        assert_raises_rpc_error(-8, \"Invalid parameter, duplicate key: data\", self.nodes[0].createrawtransaction, [], multidict([(\"data\", 'aa'), (\"data\", \"bb\")]))\n        assert_raises_rpc_error(-8, \"Invalid parameter, key-value pair must contain exactly one key\", self.nodes[0].createrawtransaction, [], [{'a': 1, 'b': 2}])\n        assert_raises_rpc_error(-8, \"Invalid parameter, key-value pair not an object as expected\", self.nodes[0].createrawtransaction, [], [['key-value pair1'], ['2']])\n\n        # Test `createrawtransaction` invalid `locktime`\n        assert_raises_rpc_error(-3, \"Expected type number\", self.nodes[0].createrawtransaction, [], {}, 'foo')\n        assert_raises_rpc_error(-8, \"Invalid parameter, locktime out of range\", self.nodes[0].createrawtransaction, [], {}, -1)\n        assert_raises_rpc_error(-8, \"Invalid parameter, locktime out of range\", self.nodes[0].createrawtransaction, [], {}, 4294967296)\n\n        # Test `createrawtransaction` invalid `replaceable`\n        assert_raises_rpc_error(-3, \"Expected type bool\", self.nodes[0].createrawtransaction, [], {}, 0, 'foo')\n\n        self.log.info('Check that createrawtransaction accepts an array and object as outputs')\n        tx = CTransaction()\n        # One output\n        tx.deserialize(BytesIO(hex_str_to_bytes(self.nodes[2].createrawtransaction(inputs=[{'txid': txid, 'vout': 9}], outputs={address: 99}))))\n        assert_equal(len(tx.vout), 1)\n        assert_equal(\n            tx.serialize().hex(),\n            self.nodes[2].createrawtransaction(inputs=[{'txid': txid, 'vout': 9}], outputs=[{address: 99}]),\n        )\n        # Two outputs\n        tx.deserialize(BytesIO(hex_str_to_bytes(self.nodes[2].createrawtransaction(inputs=[{'txid': txid, 'vout': 9}], outputs=OrderedDict([(address, 99), (address2, 99)])))))\n        assert_equal(len(tx.vout), 2)\n        assert_equal(\n            tx.serialize().hex(),\n            self.nodes[2].createrawtransaction(inputs=[{'txid': txid, 'vout': 9}], outputs=[{address: 99}, {address2: 99}]),\n        )\n        # Multiple mixed outputs\n        tx.deserialize(BytesIO(hex_str_to_bytes(self.nodes[2].createrawtransaction(inputs=[{'txid': txid, 'vout': 9}], outputs=multidict([(address, 99), (address2, 99), ('data', '99')])))))\n        assert_equal(len(tx.vout), 3)\n        assert_equal(\n            tx.serialize().hex(),\n            self.nodes[2].createrawtransaction(inputs=[{'txid': txid, 'vout': 9}], outputs=[{address: 99}, {address2: 99}, {'data': '99'}]),\n        )\n\n        for type in [\"bech32\", \"p2sh-segwit\", \"legacy\"]:\n            addr = self.nodes[0].getnewaddress(\"\", type)\n            addrinfo = self.nodes[0].getaddressinfo(addr)\n            pubkey = addrinfo[\"scriptPubKey\"]\n\n            self.log.info('sendrawtransaction with missing prevtx info (%s)' %(type))\n\n            # Test `signrawtransactionwithwallet` invalid `prevtxs`\n            inputs  = [ {'txid' : txid, 'vout' : 3, 'sequence' : 1000}]\n            outputs = { self.nodes[0].getnewaddress() : 1 }\n            rawtx   = self.nodes[0].createrawtransaction(inputs, outputs)\n\n            prevtx = dict(txid=txid, scriptPubKey=pubkey, vout=3, amount=1)\n            succ = self.nodes[0].signrawtransactionwithwallet(rawtx, [prevtx])\n            assert succ[\"complete\"]\n            if type == \"legacy\":\n                del prevtx[\"amount\"]\n                succ = self.nodes[0].signrawtransactionwithwallet(rawtx, [prevtx])\n                assert succ[\"complete\"]\n\n            if type != \"legacy\":\n                assert_raises_rpc_error(-3, \"Missing amount\", self.nodes[0].signrawtransactionwithwallet, rawtx, [\n                    {\n                        \"txid\": txid,\n                        \"scriptPubKey\": pubkey,\n                        \"vout\": 3,\n                    }\n                ])\n\n            assert_raises_rpc_error(-3, \"Missing vout\", self.nodes[0].signrawtransactionwithwallet, rawtx, [\n                {\n                    \"txid\": txid,\n                    \"scriptPubKey\": pubkey,\n                    \"amount\": 1,\n                }\n            ])\n            assert_raises_rpc_error(-3, \"Missing txid\", self.nodes[0].signrawtransactionwithwallet, rawtx, [\n                {\n                    \"scriptPubKey\": pubkey,\n                    \"vout\": 3,\n                    \"amount\": 1,\n                }\n            ])\n            assert_raises_rpc_error(-3, \"Missing scriptPubKey\", self.nodes[0].signrawtransactionwithwallet, rawtx, [\n                {\n                    \"txid\": txid,\n                    \"vout\": 3,\n                    \"amount\": 1\n                }\n            ])\n\n        #########################################\n        # sendrawtransaction with missing input #\n        #########################################\n\n        self.log.info('sendrawtransaction with missing input')\n        inputs  = [ {'txid' : \"1d1d4e24ed99057e84c3f80fd8fbec79ed9e1acee37da269356ecea000000000\", 'vout' : 1}] #won't exists\n        outputs = { self.nodes[0].getnewaddress() : 4.998 }\n        rawtx   = self.nodes[2].createrawtransaction(inputs, outputs)\n        rawtx   = self.nodes[2].signrawtransactionwithwallet(rawtx)\n\n        # This will raise an exception since there are missing inputs\n        assert_raises_rpc_error(-25, \"Missing inputs\", self.nodes[2].sendrawtransaction, rawtx['hex'])\n\n        #####################################\n        # getrawtransaction with block hash #\n        #####################################\n\n        # make a tx by sending then generate 2 blocks; block1 has the tx in it\n        tx = self.nodes[2].sendtoaddress(self.nodes[1].getnewaddress(), 1)\n        block1, block2 = self.nodes[2].generate(2)\n        self.sync_all()\n        # We should be able to get the raw transaction by providing the correct block\n        gottx = self.nodes[0].getrawtransaction(tx, True, block1)\n        assert_equal(gottx['txid'], tx)\n        assert_equal(gottx['in_active_chain'], True)\n        # We should not have the 'in_active_chain' flag when we don't provide a block\n        gottx = self.nodes[0].getrawtransaction(tx, True)\n        assert_equal(gottx['txid'], tx)\n        assert 'in_active_chain' not in gottx\n        # We should not get the tx if we provide an unrelated block\n        assert_raises_rpc_error(-5, \"No such transaction found\", self.nodes[0].getrawtransaction, tx, True, block2)\n        # An invalid block hash should raise the correct errors\n        assert_raises_rpc_error(-1, \"JSON value is not a string as expected\", self.nodes[0].getrawtransaction, tx, True, True)\n        assert_raises_rpc_error(-8, \"parameter 3 must be of length 64 (not 6, for 'foobar')\", self.nodes[0].getrawtransaction, tx, True, \"foobar\")\n        assert_raises_rpc_error(-8, \"parameter 3 must be of length 64 (not 8, for 'abcd1234')\", self.nodes[0].getrawtransaction, tx, True, \"abcd1234\")\n        assert_raises_rpc_error(-8, \"parameter 3 must be hexadecimal string (not 'ZZZ0000000000000000000000000000000000000000000000000000000000000')\", self.nodes[0].getrawtransaction, tx, True, \"ZZZ0000000000000000000000000000000000000000000000000000000000000\")\n        assert_raises_rpc_error(-5, \"Block hash not found\", self.nodes[0].getrawtransaction, tx, True, \"0000000000000000000000000000000000000000000000000000000000000000\")\n        # Undo the blocks and check in_active_chain\n        self.nodes[0].invalidateblock(block1)\n        gottx = self.nodes[0].getrawtransaction(txid=tx, verbose=True, blockhash=block1)\n        assert_equal(gottx['in_active_chain'], False)\n        self.nodes[0].reconsiderblock(block1)\n        assert_equal(self.nodes[0].getbestblockhash(), block2)\n\n        #########################\n        # RAW TX MULTISIG TESTS #\n        #########################\n        # 2of2 test\n        addr1 = self.nodes[2].getnewaddress()\n        addr2 = self.nodes[2].getnewaddress()\n\n        addr1Obj = self.nodes[2].getaddressinfo(addr1)\n        addr2Obj = self.nodes[2].getaddressinfo(addr2)\n\n        # Tests for createmultisig and addmultisigaddress\n        assert_raises_rpc_error(-5, \"Invalid public key\", self.nodes[0].createmultisig, 1, [\"01020304\"])\n        self.nodes[0].createmultisig(2, [addr1Obj['pubkey'], addr2Obj['pubkey']]) # createmultisig can only take public keys\n        assert_raises_rpc_error(-5, \"Invalid public key\", self.nodes[0].createmultisig, 2, [addr1Obj['pubkey'], addr1]) # addmultisigaddress can take both pubkeys and addresses so long as they are in the wallet, which is tested here.\n\n        mSigObj = self.nodes[2].addmultisigaddress(2, [addr1Obj['pubkey'], addr1])['address']\n\n        #use balance deltas instead of absolute values\n        bal = self.nodes[2].getbalance()\n\n        # send 1.2 BTC to msig adr\n        txId = self.nodes[0].sendtoaddress(mSigObj, 1.2)\n        self.sync_all()\n        self.nodes[0].generate(1)\n        self.sync_all()\n        assert_equal(self.nodes[2].getbalance(), bal+Decimal('1.20000000')) #node2 has both keys of the 2of2 ms addr., tx should affect the balance\n\n\n        # 2of3 test from different nodes\n        bal = self.nodes[2].getbalance()\n        addr1 = self.nodes[1].getnewaddress()\n        addr2 = self.nodes[2].getnewaddress()\n        addr3 = self.nodes[2].getnewaddress()\n\n        addr1Obj = self.nodes[1].getaddressinfo(addr1)\n        addr2Obj = self.nodes[2].getaddressinfo(addr2)\n        addr3Obj = self.nodes[2].getaddressinfo(addr3)\n\n        mSigObj = self.nodes[2].addmultisigaddress(2, [addr1Obj['pubkey'], addr2Obj['pubkey'], addr3Obj['pubkey']])['address']\n\n        txId = self.nodes[0].sendtoaddress(mSigObj, 2.2)\n        decTx = self.nodes[0].gettransaction(txId)\n        rawTx = self.nodes[0].decoderawtransaction(decTx['hex'])\n        self.sync_all()\n        self.nodes[0].generate(1)\n        self.sync_all()\n\n        #THIS IS AN INCOMPLETE FEATURE\n        #NODE2 HAS TWO OF THREE KEY AND THE FUNDS SHOULD BE SPENDABLE AND COUNT AT BALANCE CALCULATION\n        assert_equal(self.nodes[2].getbalance(), bal) #for now, assume the funds of a 2of3 multisig tx are not marked as spendable\n\n        txDetails = self.nodes[0].gettransaction(txId, True)\n        rawTx = self.nodes[0].decoderawtransaction(txDetails['hex'])\n        vout = next(o for o in rawTx['vout'] if o['value'] == Decimal('2.20000000'))\n\n        bal = self.nodes[0].getbalance()\n        inputs = [{ \"txid\" : txId, \"vout\" : vout['n'], \"scriptPubKey\" : vout['scriptPubKey']['hex'], \"amount\" : vout['value']}]\n        outputs = { self.nodes[0].getnewaddress() : 2.19 }\n        rawTx = self.nodes[2].createrawtransaction(inputs, outputs)\n        rawTxPartialSigned = self.nodes[1].signrawtransactionwithwallet(rawTx, inputs)\n        assert_equal(rawTxPartialSigned['complete'], False) #node1 only has one key, can't comp. sign the tx\n\n        rawTxSigned = self.nodes[2].signrawtransactionwithwallet(rawTx, inputs)\n        assert_equal(rawTxSigned['complete'], True) #node2 can sign the tx compl., own two of three keys\n        self.nodes[2].sendrawtransaction(rawTxSigned['hex'])\n        rawTx = self.nodes[0].decoderawtransaction(rawTxSigned['hex'])\n        self.sync_all()\n        self.nodes[0].generate(1)\n        self.sync_all()\n        assert_equal(self.nodes[0].getbalance(), bal+Decimal('80.00000000')+Decimal('2.19000000')) #block reward + tx\n\n        # 2of2 test for combining transactions\n        bal = self.nodes[2].getbalance()\n        addr1 = self.nodes[1].getnewaddress()\n        addr2 = self.nodes[2].getnewaddress()\n\n        addr1Obj = self.nodes[1].getaddressinfo(addr1)\n        addr2Obj = self.nodes[2].getaddressinfo(addr2)\n\n        self.nodes[1].addmultisigaddress(2, [addr1Obj['pubkey'], addr2Obj['pubkey']])['address']\n        mSigObj = self.nodes[2].addmultisigaddress(2, [addr1Obj['pubkey'], addr2Obj['pubkey']])['address']\n        mSigObjValid = self.nodes[2].getaddressinfo(mSigObj)\n\n        txId = self.nodes[0].sendtoaddress(mSigObj, 2.2)\n        decTx = self.nodes[0].gettransaction(txId)\n        rawTx2 = self.nodes[0].decoderawtransaction(decTx['hex'])\n        self.sync_all()\n        self.nodes[0].generate(1)\n        self.sync_all()\n\n        assert_equal(self.nodes[2].getbalance(), bal) # the funds of a 2of2 multisig tx should not be marked as spendable\n\n        txDetails = self.nodes[0].gettransaction(txId, True)\n        rawTx2 = self.nodes[0].decoderawtransaction(txDetails['hex'])\n        vout = next(o for o in rawTx2['vout'] if o['value'] == Decimal('2.20000000'))\n\n        bal = self.nodes[0].getbalance()\n        inputs = [{ \"txid\" : txId, \"vout\" : vout['n'], \"scriptPubKey\" : vout['scriptPubKey']['hex'], \"redeemScript\" : mSigObjValid['hex'], \"amount\" : vout['value']}]\n        outputs = { self.nodes[0].getnewaddress() : 2.19 }\n        rawTx2 = self.nodes[2].createrawtransaction(inputs, outputs)\n        rawTxPartialSigned1 = self.nodes[1].signrawtransactionwithwallet(rawTx2, inputs)\n        self.log.debug(rawTxPartialSigned1)\n        assert_equal(rawTxPartialSigned1['complete'], False) #node1 only has one key, can't comp. sign the tx\n\n        rawTxPartialSigned2 = self.nodes[2].signrawtransactionwithwallet(rawTx2, inputs)\n        self.log.debug(rawTxPartialSigned2)\n        assert_equal(rawTxPartialSigned2['complete'], False) #node2 only has one key, can't comp. sign the tx\n        rawTxComb = self.nodes[2].combinerawtransaction([rawTxPartialSigned1['hex'], rawTxPartialSigned2['hex']])\n        self.log.debug(rawTxComb)\n        self.nodes[2].sendrawtransaction(rawTxComb)\n        rawTx2 = self.nodes[0].decoderawtransaction(rawTxComb)\n        self.sync_all()\n        self.nodes[0].generate(1)\n        self.sync_all()\n        assert_equal(self.nodes[0].getbalance(), bal+Decimal('80.00000000')+Decimal('2.19000000')) #block reward + tx\n\n        # decoderawtransaction tests\n        # witness transaction\n        encrawtx = \"010000000001010000000000000072c1a6a246ae63f74f931e8365e15a089c68d61900000000000000000000ffffffff0100e1f50500000000000102616100000000\"\n        decrawtx = self.nodes[0].decoderawtransaction(encrawtx, True) # decode as witness transaction\n        assert_equal(decrawtx['vout'][0]['value'], Decimal('1.00000000'))\n        assert_raises_rpc_error(-22, 'TX decode failed', self.nodes[0].decoderawtransaction, encrawtx, False) # force decode as non-witness transaction\n        # non-witness transaction\n        encrawtx = \"01000000010000000000000072c1a6a246ae63f74f931e8365e15a089c68d61900000000000000000000ffffffff0100e1f505000000000000000000\"\n        decrawtx = self.nodes[0].decoderawtransaction(encrawtx, False) # decode as non-witness transaction\n        assert_equal(decrawtx['vout'][0]['value'], Decimal('1.00000000'))\n\n        # getrawtransaction tests\n        # 1. valid parameters - only supply txid\n        txId = rawTx[\"txid\"]\n        assert_equal(self.nodes[0].getrawtransaction(txId), rawTxSigned['hex'])\n\n        # 2. valid parameters - supply txid and 0 for non-verbose\n        assert_equal(self.nodes[0].getrawtransaction(txId, 0), rawTxSigned['hex'])\n\n        # 3. valid parameters - supply txid and False for non-verbose\n        assert_equal(self.nodes[0].getrawtransaction(txId, False), rawTxSigned['hex'])\n\n        # 4. valid parameters - supply txid and 1 for verbose.\n        # We only check the \"hex\" field of the output so we don't need to update this test every time the output format changes.\n        assert_equal(self.nodes[0].getrawtransaction(txId, 1)[\"hex\"], rawTxSigned['hex'])\n\n        # 5. valid parameters - supply txid and True for non-verbose\n        assert_equal(self.nodes[0].getrawtransaction(txId, True)[\"hex\"], rawTxSigned['hex'])\n\n        # 6. invalid parameters - supply txid and string \"Flase\"\n        assert_raises_rpc_error(-1, \"not a boolean\", self.nodes[0].getrawtransaction, txId, \"Flase\")\n\n        # 7. invalid parameters - supply txid and empty array\n        assert_raises_rpc_error(-1, \"not a boolean\", self.nodes[0].getrawtransaction, txId, [])\n\n        # 8. invalid parameters - supply txid and empty dict\n        assert_raises_rpc_error(-1, \"not a boolean\", self.nodes[0].getrawtransaction, txId, {})\n\n        inputs  = [ {'txid' : \"1d1d4e24ed99057e84c3f80fd8fbec79ed9e1acee37da269356ecea000000000\", 'vout' : 1, 'sequence' : 1000}]\n        outputs = { self.nodes[0].getnewaddress() : 1 }\n        rawtx   = self.nodes[0].createrawtransaction(inputs, outputs)\n        decrawtx= self.nodes[0].decoderawtransaction(rawtx)\n        assert_equal(decrawtx['vin'][0]['sequence'], 1000)\n\n        # 9. invalid parameters - sequence number out of range\n        inputs  = [ {'txid' : \"1d1d4e24ed99057e84c3f80fd8fbec79ed9e1acee37da269356ecea000000000\", 'vout' : 1, 'sequence' : -1}]\n        outputs = { self.nodes[0].getnewaddress() : 1 }\n        assert_raises_rpc_error(-8, 'Invalid parameter, sequence number is out of range', self.nodes[0].createrawtransaction, inputs, outputs)\n\n        # 10. invalid parameters - sequence number out of range\n        inputs  = [ {'txid' : \"1d1d4e24ed99057e84c3f80fd8fbec79ed9e1acee37da269356ecea000000000\", 'vout' : 1, 'sequence' : 4294967296}]\n        outputs = { self.nodes[0].getnewaddress() : 1 }\n        assert_raises_rpc_error(-8, 'Invalid parameter, sequence number is out of range', self.nodes[0].createrawtransaction, inputs, outputs)\n\n        inputs  = [ {'txid' : \"1d1d4e24ed99057e84c3f80fd8fbec79ed9e1acee37da269356ecea000000000\", 'vout' : 1, 'sequence' : 4294967294}]\n        outputs = { self.nodes[0].getnewaddress() : 1 }\n        rawtx   = self.nodes[0].createrawtransaction(inputs, outputs)\n        decrawtx= self.nodes[0].decoderawtransaction(rawtx)\n        assert_equal(decrawtx['vin'][0]['sequence'], 4294967294)\n\n        ####################################\n        # TRANSACTION VERSION NUMBER TESTS #\n        ####################################\n\n        # Test the minimum transaction version number that fits in a signed 32-bit integer.\n        tx = CTransaction()\n        tx.nVersion = -0x80000000\n        rawtx = ToHex(tx)\n        decrawtx = self.nodes[0].decoderawtransaction(rawtx)\n        assert_equal(decrawtx['version'], -0x80000000)\n\n        # Test the maximum transaction version number that fits in a signed 32-bit integer.\n        tx = CTransaction()\n        tx.nVersion = 0x7fffffff\n        rawtx = ToHex(tx)\n        decrawtx = self.nodes[0].decoderawtransaction(rawtx)\n        assert_equal(decrawtx['version'], 0x7fffffff)\n\n        self.log.info('sendrawtransaction/testmempoolaccept with maxfeerate')\n\n        # Test a transaction with a small fee.\n        txId = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), 1.0)\n        rawTx = self.nodes[0].getrawtransaction(txId, True)\n        vout = next(o for o in rawTx['vout'] if o['value'] == Decimal('1.00000000'))\n\n        self.sync_all()\n        inputs = [{ \"txid\" : txId, \"vout\" : vout['n'] }]\n        # Fee 10,000 satoshis, (1 - (10000 sat * 0.00000001 BTC/sat)) = 0.9999\n        outputs = { self.nodes[0].getnewaddress() : Decimal(\"0.99990000\") }\n        rawTx = self.nodes[2].createrawtransaction(inputs, outputs)\n        rawTxSigned = self.nodes[2].signrawtransactionwithwallet(rawTx)\n        assert_equal(rawTxSigned['complete'], True)\n        # Fee 10,000 satoshis, ~100 b transaction, fee rate should land around 100 sat/byte = 0.00100000 BTC/kB\n        # Thus, testmempoolaccept should reject\n        testres = self.nodes[2].testmempoolaccept([rawTxSigned['hex']], 0.00001000)[0]\n        assert_equal(testres['allowed'], False)\n        assert_equal(testres['reject-reason'], '256: absurdly-high-fee')\n        # and sendrawtransaction should throw\n        assert_raises_rpc_error(-26, \"absurdly-high-fee\", self.nodes[2].sendrawtransaction, rawTxSigned['hex'], 0.00001000)\n        # and the following calls should both succeed\n        testres = self.nodes[2].testmempoolaccept(rawtxs=[rawTxSigned['hex']])[0]\n        assert_equal(testres['allowed'], True)\n        self.nodes[2].sendrawtransaction(hexstring=rawTxSigned['hex'])\n\n        # Test a transaction with a large fee.\n        txId = self.nodes[0].sendtoaddress(self.nodes[2].getnewaddress(), 1.0)\n        rawTx = self.nodes[0].getrawtransaction(txId, True)\n        vout = next(o for o in rawTx['vout'] if o['value'] == Decimal('1.00000000'))\n\n        self.sync_all()\n        inputs = [{ \"txid\" : txId, \"vout\" : vout['n'] }]\n        # Fee 2,000,000 satoshis, (1 - (2000000 sat * 0.00000001 BTC/sat)) = 0.98\n        outputs = { self.nodes[0].getnewaddress() : Decimal(\"0.98000000\") }\n        rawTx = self.nodes[2].createrawtransaction(inputs, outputs)\n        rawTxSigned = self.nodes[2].signrawtransactionwithwallet(rawTx)\n        assert_equal(rawTxSigned['complete'], True)\n        # Fee 2,000,000 satoshis, ~100 b transaction, fee rate should land around 20,000 sat/byte = 0.20000000 BTC/kB\n        # Thus, testmempoolaccept should reject\n        testres = self.nodes[2].testmempoolaccept([rawTxSigned['hex']])[0]\n        assert_equal(testres['allowed'], False)\n        assert_equal(testres['reject-reason'], '256: absurdly-high-fee')\n        # and sendrawtransaction should throw\n        assert_raises_rpc_error(-26, \"absurdly-high-fee\", self.nodes[2].sendrawtransaction, rawTxSigned['hex'])\n        # and the following calls should both succeed\n        testres = self.nodes[2].testmempoolaccept(rawtxs=[rawTxSigned['hex']], maxfeerate='0.20000000')[0]\n        assert_equal(testres['allowed'], True)\n        self.nodes[2].sendrawtransaction(hexstring=rawTxSigned['hex'], maxfeerate='0.20000000')", "label": 1}
{"function": "def update_profile(profile, users, groups):\n    profile.groups.all().delete()\n    profile.users.all().delete()\n    profile.groups.add(*groups)\n    profile.users.add(*users)", "label": 1}
{"function": "    def handle(self, *args, **options):\n        if len(args) < 1:\n            raise CommandError(\"Invalid number of arguments\")\n\n        name = args[0].strip()\n        try:\n            profile = Profile.objects.get(name=name)\n        except Profile.DoesNotExist:\n            raise CommandError(\"Invalid profile name\")\n\n        users = []\n        try:\n            users = [AstakosUser.objects.get(pk=int(pk)) for pk in\n                     options.get('users')]\n        except AstakosUser.DoesNotExist:\n            raise CommandError(\"Invalid user id\")\n\n        groups = []\n        try:\n            groups = [Group.objects.get(pk=int(pk)) for pk in\n                      options.get('groups')]\n        except Group.DoesNotExist:\n            raise CommandError(\"Invalid group id\")\n\n        update_profile(profile, users, groups)", "label": 1}
{"function": "def test():\n    filelist = [codedir + 'partial.v']\n    topmodule = 'TOP'\n    noreorder = False\n    nobind = False\n    include = None\n    define = None\n    \n    analyzer = VerilogDataflowAnalyzer(filelist, topmodule,\n                                       noreorder=noreorder,\n                                       nobind=nobind,\n                                       preprocess_include=include,\n                                       preprocess_define=define)\n    analyzer.generate()\n\n    directives = analyzer.get_directives()\n    instances = analyzer.getInstances()\n    terms = analyzer.getTerms()\n    binddict = analyzer.getBinddict()\n\n    output = []\n    \n    for bk, bv in sorted(binddict.items(), key=lambda x:str(x[0])):\n        for bvi in bv:\n            output.append(bvi.tostr())\n            output.append('\\n')\n            \n    rslt = ''.join(output)\n\n    print(rslt)\n    assert(expected == rslt)", "label": 1}
{"function": "    def __init__(self, plotly_name=\"hoverinfo\", parent_name=\"sankey.node\", **kwargs):\n        super(HoverinfoValidator, self).__init__(\n            plotly_name=plotly_name,\n            parent_name=parent_name,\n            edit_type=kwargs.pop(\"edit_type\", \"calc\"),\n            role=kwargs.pop(\"role\", \"info\"),\n            values=kwargs.pop(\"values\", [\"all\", \"none\", \"skip\"]),\n            **kwargs\n        )", "label": 1}
{"function": "    def testMethods(self):\n        self.assertArgIsBOOL(PDFSelection.drawForPage_withBox_active_, 2)", "label": 1}
{"function": "    def _format_vat_cl(self, values):\n        identification_types = [self.env.ref('l10n_latam_base.it_vat').id, self.env.ref('l10n_cl.it_RUT').id,\n                                self.env.ref('l10n_cl.it_RUN').id]\n        partner_country_is_chile = (values.get('country_id') == self.env.ref('base.cl').id) or (\n                    values.get('l10n_latam_identification_type_id') and\n                    self.env['l10n_latam.identification.type'].browse(\n                        values.get('l10n_latam_identification_type_id')).country_id == self.env.ref('base.cl'))\n        if partner_country_is_chile and \\\n                values.get('l10n_latam_identification_type_id') in identification_types and values.get('vat'):\n            return stdnum.util.get_cc_module('cl', 'vat').format(values['vat']).replace('.', '').replace(\n                'CL', '').upper()\n        else:\n            return values['vat']", "label": 1}
{"function": "    def create(self, values):\n        if values.get('vat'):\n            values['vat'] = self._format_vat_cl(values)\n        return super().create(values)", "label": 1}
{"function": "    def write(self, values):\n        for record in self:\n            vat_values = {\n                'vat': values.get('vat', record.vat),\n                'l10n_latam_identification_type_id': values.get(\n                    'l10n_latam_identification_type_id', record.l10n_latam_identification_type_id.id),\n                'country_id': values.get('country_id', record.country_id.id)\n            }\n            values['vat'] = self._format_vat_cl(vat_values)\n        return super().write(values)", "label": 1}
{"function": "def load_data():\n    dirname = \"cifar-10-batches-py\"\n    origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n    path = get_file(dirname, origin=origin, untar=True)\n\n    nb_test_samples = 10000\n    nb_train_samples = 50000\n\n    X_train = np.zeros((nb_train_samples, 3, 32, 32), dtype=\"uint8\")\n    y_train = np.zeros((nb_train_samples,), dtype=\"uint8\")\n\n    for i in range(1, 6):\n        fpath = os.path.join(path, 'data_batch_' + str(i))\n        data, labels = load_batch(fpath)\n        X_train[(i-1)*10000:i*10000, :, :, :] = data\n        y_train[(i-1)*10000:i*10000] = labels\n\n    fpath = os.path.join(path, 'test_batch')\n    X_test, y_test = load_batch(fpath)\n\n    y_train = np.reshape(y_train, (len(y_train), 1))\n    y_test = np.reshape(y_test, (len(y_test), 1))\n\n    return (X_train, y_train), (X_test, y_test)", "label": 1}
{"function": "    def build(self, attribute_list, class_list, col_sampler, bin_structure):\n        while len(self.alive_nodes) != 0:\n\n            # scan each attribute list\n            func = partial(self._process_one_attribute_list, attribute_list, class_list)\n            pool = Pool(self.num_thread)\n            rets = pool.map(func, col_sampler.col_selected)\n            pool.close()\n\n            # process the rets\n            for ret in rets:\n                for col, uint8_threshold, tree_node_G_H in ret:\n                    for tree_node_name in tree_node_G_H.keys():\n                        # get the original tree_node by tree_node_name using self.name_to_node\n                        tree_node = self.name_to_node[tree_node_name]\n                        G, H = tree_node_G_H[tree_node_name]\n                        G_left, H_left = tree_node.get_Gleft_Hleft(col, G, H)\n                        G_total, H_total = tree_node.Grad, tree_node.Hess\n                        gain = self.calculate_split_gain(G_left, H_left, G_total, H_total)\n                        tree_node.update_best_gain(col, uint8_threshold, bin_structure[col][uint8_threshold], gain)\n\n            # once had scan all column, we can get the best (feature,threshold,gain) for each alive tree node\n            cur_level_node_size = len(self.alive_nodes)\n            new_tree_nodes = []\n            treenode_leftinds = []\n            for _ in range(cur_level_node_size):\n                tree_node = self.alive_nodes.pop(0)\n                best_feature, best_uint8_threshold, best_threshold, best_gain = tree_node.get_best_feature_threshold_gain()\n                if best_gain > 0:\n                    nan_direction = 0  # TODO\n                    left_child = TreeNode(name=2*tree_node.name, depth=tree_node.depth+1, feature_dim=attribute_list.feature_dim)\n                    right_child = TreeNode(name=2*tree_node.name+1, depth=tree_node.depth+1, feature_dim=attribute_list.feature_dim)\n                    tree_node.internal_node_setter(best_feature, best_uint8_threshold, best_threshold, nan_direction, left_child, right_child)\n\n                    # to update class_list.corresponding_tree_node one pass, we should save (tree_node,left_inds)\n                    left_inds = attribute_list[best_feature][\"index\"][0:attribute_list.attribute_list_cutting_index[best_feature][best_uint8_threshold+1]]\n                    treenode_leftinds.append((tree_node, set(left_inds)))\n\n                    new_tree_nodes.append(left_child)\n                    new_tree_nodes.append(right_child)\n                    self.name_to_node[left_child.name] = left_child\n                    self.name_to_node[right_child.name] = right_child\n\n                else:\n                    leaf_score = self.calculate_leaf_score(tree_node.Grad, tree_node.Hess)\n                    tree_node.leaf_node_setter(leaf_score)\n\n            # update class_list.corresponding_tree_node one pass\n            class_list.update_corresponding_tree_node(treenode_leftinds)\n\n            # update histogram(Grad,Hess,num_sample) for each alive(new) tree node\n            class_list.update_histogram_for_tree_node()\n\n            # process the new tree nodes\n            # satisfy max_depth? min_child_weight? min_sample_split?\n            # if yes, it is leaf node, calculate its leaf score\n            # if no, put into self.alive_node\n            while len(new_tree_nodes) != 0:\n                tree_node = new_tree_nodes.pop()\n                if tree_node.depth >= self.max_depth \\\n                        or tree_node.Hess < self.min_child_weight \\\n                        or tree_node.num_sample <= self.min_sample_split:\n                    tree_node.leaf_node_setter(self.calculate_leaf_score(tree_node.Grad, tree_node.Hess))\n                else:\n                    self.alive_nodes.append(tree_node)", "label": 1}
{"function": "    def fit(self, attribute_list, class_list, row_sampler, col_sampler, bin_structure):\n        # when we start to fit a tree, we first conduct row and column sampling\n        col_sampler.shuffle()\n        row_sampler.shuffle()\n        class_list.sampling(row_sampler.row_mask)\n\n        # then we create the root node, initialize histogram(Gradient sum and Hessian sum)\n        root_node = TreeNode(name=1, depth=1, feature_dim=attribute_list.feature_dim)\n        root_node.Grad_setter(class_list.grad.sum())\n        root_node.Hess_setter(class_list.hess.sum())\n        self.root = root_node\n\n        # every time a new node is created, we put it into self.name_to_node\n        self.name_to_node[root_node.name] = root_node\n\n        # put it into the alive_node, and fill the class_list, all data are assigned to root node initially\n        self.alive_nodes.append(root_node)\n        for i in range(class_list.dataset_size):\n            class_list.corresponding_tree_node[i] = root_node\n\n        # then build the tree util there is no alive tree_node to split\n        self.build(attribute_list, class_list, col_sampler, bin_structure)\n        self.clean_up()", "label": 1}
{"function": "    def _predict(self, feature):\n        \"\"\"\n        :param feature: feature of a single sample\n        :return:\n        \"\"\"\n        cur_tree_node = self.root\n        while not cur_tree_node.is_leaf:\n            if feature[cur_tree_node.split_feature] <= cur_tree_node.split_threshold:\n                cur_tree_node = cur_tree_node.left_child\n            else:\n                cur_tree_node = cur_tree_node.right_child\n        return cur_tree_node.leaf_score", "label": 1}
{"function": "    def predict(self, features):\n        pool = Pool(self.num_thread)\n        preds = pool.map(self._predict, features)\n        pool.close()\n        return np.array(preds)", "label": 1}
{"function": "    def clean_up(self):\n        del self.alive_nodes, self.min_sample_split, self.min_child_weight, self.rowsample,\\\n            self.colsample, self.max_depth, self.reg_lambda, self.gamma", "label": 1}
{"function": "def tearDownModule():\n    global mol, Gvbase, Gv, gxyz\n    del mol, Gvbase, Gv, gxyz", "label": 1}
{"function": "def ft_ao_o0(mol, Gv):\n    nao = mol.nao_nr()\n    ngrids = Gv.shape[0]\n    aoG = numpy.zeros((nao,ngrids), dtype=numpy.complex)\n    gx = numpy.empty((12,ngrids), dtype=numpy.complex)\n    gy = numpy.empty((12,ngrids), dtype=numpy.complex)\n    gz = numpy.empty((12,ngrids), dtype=numpy.complex)\n    buf = numpy.empty((64,ngrids), dtype=numpy.complex)\n    kk = numpy.einsum('ki,ki->k', Gv, Gv)\n\n    i0 = 0\n    for ib in range(mol.nbas):\n        ci = mol._libcint_ctr_coeff(ib)\n        ei = mol.bas_exp(ib)\n        li = mol.bas_angular(ib)\n        ri = mol.bas_coord(ib)\n        ni = ci.shape[1]\n        di = (li*2+1) * ni\n        nfi = (li+1)*(li+2)//2\n        kr = numpy.dot(Gv,ri)\n        cs = numpy.exp(-1j*kr)\n\n        buf[:nfi*ni] = 0\n        for ip in range(ci.shape[0]):\n            ai = ei[ip]\n            fac = (numpy.pi/ai)**1.5 * numpy.exp(-.25/ai*kk)\n            gx[0] = 1\n            gy[0] = 1\n            gz[0] = cs * fac\n            if li > 0:\n                gx[1] = -1j*Gv[:,0]/(2*ai) * gx[0]\n                gy[1] = -1j*Gv[:,1]/(2*ai) * gy[0]\n                gz[1] = -1j*Gv[:,2]/(2*ai) * gz[0]\n                for m in range(1, li):\n                    gx[m+1] = m/(2*ai) * gx[m-1] - 1j*Gv[:,0]/(2*ai) * gx[m]\n                    gy[m+1] = m/(2*ai) * gy[m-1] - 1j*Gv[:,1]/(2*ai) * gy[m]\n                    gz[m+1] = m/(2*ai) * gz[m-1] - 1j*Gv[:,2]/(2*ai) * gz[m]\n\n            for m,(ix,iy,iz) in enumerate(loop_cart(li)):\n                val = gx[ix] * gy[iy] * gz[iz]\n                for i, cip in enumerate(ci[ip]):\n                    buf[i*nfi+m] += cip*val\n\n        ti = c2s_bra(li, numpy.eye(nfi)).T\n        tmp1 = numpy.empty((di,ngrids), dtype=numpy.complex)\n        for i in range(ni):\n            tmp1[i*(li*2+1):(i+1)*(li*2+1)] = \\\n                    numpy.einsum('pi,px->ix', ti, buf[i*nfi:(i+1)*nfi])\n        aoG[i0:i0+di] += tmp1\n        i0 += di\n    return aoG.T", "label": 1}
{"function": "def loop_cart(l):\n    for ix in reversed(range(l+1)):\n        for iy in reversed(range(l-ix+1)):\n            iz = l - ix - iy\n            yield ix, iy, iz", "label": 1}
{"function": "def c2s_bra(l, gcart):\n    if l == 0:\n        return gcart * 0.282094791773878143\n    elif l == 1:\n        return gcart * 0.488602511902919921\n    else:\n        m = gcart.shape[1]\n        gsph = numpy.empty((l*2+1,m))\n        fc2s = gto.moleintor.libcgto.CINTc2s_ket_sph\n        fc2s(gsph.ctypes.data_as(ctypes.c_void_p), ctypes.c_int(m),\n             gcart.ctypes.data_as(ctypes.c_void_p), ctypes.c_int(l))\n        return gsph", "label": 1}
{"function": "def finger(a):\n    return numpy.dot(a.ravel(), numpy.cos(numpy.arange(a.size)))", "label": 1}
{"function": "    def digest(self):\n        return sha3.sha3_384(self._data).digest()", "label": 1}
{"function": "    def digest(self):\n        return sha3.sha3_512(self._data).digest()", "label": 1}
{"function": "    def digest(self):\n        return lmhash.encrypt(self._data[:15]).decode('hex')", "label": 1}
{"function": "    def digest(self):\n        return nthash.encrypt(self._data[:127]).decode('hex')", "label": 1}
{"function": "    def digest(self):\n        return mysql323.encrypt(self._data[:64]).decode('hex')", "label": 1}
{"function": "    def digest(self):\n        return mysql41.encrypt(self._data[:64])[1:].decode('hex')", "label": 1}
{"function": "    def digest(self):\n        return oracle10.encrypt(self._data[:64], user=self._user).decode('hex')", "label": 1}
{"function": "    def digest(self):\n        ''' Removes the \"md5\" prefix '''\n        return postgres_md5.encrypt(self._data[:64],\n                                    user=self._user)[3:].decode('hex')", "label": 1}
{"function": "    def digest(self):\n        return msdcc.encrypt(self._data[:64], user=self._user).decode('hex')", "label": 1}
{"function": "    def digest(self):\n        return msdcc2.encrypt(self._data[:64], user=self._user).decode('hex')", "label": 1}
{"function": "    def __getitem__(self, index):\n\n        # if _DEBUG:\n        # index = 1\n\n        # img_id = self.ids[index]\n\n        im_path = self.annobase[index]['image']# os.path.join(self.root, img_id + '.jpg')\n        img = Image.open(im_path).convert(\"RGB\")\n        # im = cv2.imread(im_path)\n        anno = self.annobase[index]\n        target = RBoxList(torch.from_numpy(anno[\"boxes\"]), (anno['width'], anno['height']), mode=\"xywha\")\n        target.add_field(\"labels\", torch.from_numpy(anno[\"gt_classes\"]))\n        target.add_field(\"difficult\", torch.Tensor([0 for i in range(len(anno[\"gt_classes\"]))]))\n\n        masks = [np.array(mask).reshape(1, -1).tolist() for mask in anno[\"polys\"]]\n        # print('masks data:', masks)\n        masks = SegmentationMask(masks, img.size)\n        target.add_field(\"masks\", masks)\n\n        # target.add_field(\"masks\", torch.from_numpy(np.array(anno[\"polys\"]).reshape(-1)))\n\n        target = target.clip_to_image(remove_empty=True)\n        # print('target:', target, im_path)\n        if self.transforms is not None:\n            # off = int(self.num_samples * np.random.rand())\n            # mix_index = (off + index) % self.num_samples\n            # img_mix = Image.open(self.annobase[mix_index]['image']).convert(\"RGB\")\n            # img, target = self.mixup(img, img_mix, target)\n            img, target = self.transforms(img, target)\n        if _DEBUG:\n            if not target is None:\n                # print('target:', target, im_path)\n                self.show_boxes(img, target)\n\n        return img, target, index", "label": 1}
{"function": "    def __len__(self):\n        return len(self.ids)", "label": 1}
{"function": "    def get_img_info(self, index):\n        return {\"height\": self.annobase[index]['height'], \"width\": self.annobase[index]['width']}", "label": 1}
{"function": "    def map_class_id_to_class_name(self, class_id):\n        return RotationMaskDataset.CLASSES[class_id]", "label": 1}
{"function": "    def show_boxes(self, img, target):\n        bbox_np = target.bbox.data.cpu().numpy()\n        # print('image shape:', img.size())\n        np_img = np.transpose(np.uint8(img.data.cpu().numpy()), (1, 2, 0))\n        img_pil = Image.fromarray(np_img)\n        # print('bbox_np:', bbox_np)\n        draw_img = vis_image(img_pil, bbox_np)\n        draw_img.save('gt_show.jpg', 'jpeg')\n        polys = target.get_field('masks')\n        # print('polys:', polys.polygons)\n        mask_gt = vis_masks(polys.polygons, img_pil.size)\n        mask_gt.save('gt_show_masks.jpg', 'jpeg')\n\n        # print('Sleep for show...')\n        time.sleep(2)", "label": 1}
{"function": "def check_couchbase_buckets_items(_item, params, data):\n\n    total_items = data.get(\"curr_items_tot\")\n    if total_items is not None:\n        yield check_levels(\n            int(total_items),\n            \"items_count\",\n            params.get('curr_items_tot'),\n            infoname=\"Total items in vBuckets\",\n            human_readable_func=str,\n        )\n\n    write_queue = data.get(\"disk_write_queue\")\n    if write_queue is not None:\n        yield check_levels(\n            int(write_queue),\n            \"disk_write_ql\",\n            params.get(\"disk_write_ql\"),\n            infoname=\"Items in disk write queue\",\n            human_readable_func=str,\n        )\n\n    fetched = data.get(\"ep_bg_fetched\")\n    if fetched is not None:\n        yield check_levels(\n            int(fetched),\n            \"fetched_items\",\n            params.get(\"fetched_items\"),\n            infoname=\"Items fetched from disk\",\n            human_readable_func=str,\n        )\n\n    queue_fill = data.get(\"ep_diskqueue_fill\")\n    if queue_fill is not None:\n        yield check_levels(\n            queue_fill,\n            \"disk_fill_rate\",\n            params.get(\"disk_fill_rate\"),\n            unit=\"/s\",\n            infoname=\"Disk queue fill rate\",\n        )\n\n    queue_drain = data.get(\"ep_diskqueue_drain\")\n    if queue_drain is not None:\n        yield check_levels(\n            queue_drain,\n            \"disk_drain_rate\",\n            params.get(\"disk_drain_rate\"),\n            unit=\"/s\",\n            infoname=\"Disk queue drain rate\",\n        )", "label": 1}
{"function": "def pre_build():\n  update_pkgver_and_pkgrel(_G.newver.lstrip('v'))", "label": 1}
{"function": "def post_build():\n  git_add_files('PKGBUILD')\n  git_commit()", "label": 1}
{"function": "def find_nfort(conf):\n\tfc=conf.find_program(['nfort'],var='FC')\n\tconf.get_nfort_version(fc)\n\tconf.env.FC_NAME='NFORT'\n\tconf.env.FC_MOD_CAPITALIZATION='lower'", "label": 1}
{"function": "def nfort_flags(conf):\n\tv=conf.env\n\tv['_FCMODOUTFLAGS']=[]\n\tv['FCFLAGS_DEBUG']=[]\n\tv['FCFLAGS_fcshlib']=[]\n\tv['LINKFLAGS_fcshlib']=[]\n\tv['FCSTLIB_MARKER']=''\n\tv['FCSHLIB_MARKER']=''", "label": 1}
{"function": "    def handle_close(self):\n        raise Exception(\"handle_close not supposed to be called\")", "label": 1}
{"function": "    def handle_error(self):\n        raise", "label": 1}
{"function": "    def __init__(self, family, addr, handler=BaseTestHandler):\n        asyncore.dispatcher.__init__(self)\n        self.create_socket(family)\n        self.set_reuse_addr()\n        bind_af_aware(self.socket, addr)\n        self.listen(5)\n        self.handler = handler", "label": 1}
{"function": "    def address(self):\n        return self.socket.getsockname()", "label": 1}
{"function": "    def handle_accepted(self, sock, addr):\n        self.handler(sock)", "label": 1}
{"function": "    def handle_error(self):\n        raise", "label": 1}
{"function": "    def __init__(self, family, address):\n        BaseTestHandler.__init__(self)\n        self.create_socket(family)\n        self.connect(address)", "label": 1}
{"function": "    def handle_connect(self):\n        pass", "label": 1}
{"function": "    def tearDown(self):\n        asyncore.close_all(ignore_all=True)", "label": 1}
{"function": "    def loop_waiting_for_flag(self, instance, timeout=5):\n        timeout = float(timeout) / 100\n        count = 100\n        while asyncore.socket_map and count > 0:\n            asyncore.loop(timeout=0.01, count=1, use_poll=self.use_poll)\n            if instance.flag:\n                return\n            count -= 1\n            time.sleep(timeout)\n        self.fail(\"flag not set\")", "label": 1}
{"function": "    def test_vocabularies_are_initialized_even_with_empty_corpora(self):\n        parallel_corpora = []\n\n        ibm_model = IBMModel(parallel_corpora)\n        self.assertEqual(len(ibm_model.src_vocab), 1)  # addition of NULL token\n        self.assertEqual(len(ibm_model.trg_vocab), 0)", "label": 1}
{"function": "    def test_best_model2_alignment(self):\n        # arrange\n        sentence_pair = AlignedSent(\n            TestIBMModel.__TEST_TRG_SENTENCE, TestIBMModel.__TEST_SRC_SENTENCE\n        )\n        # None and 'bien' have zero fertility\n        translation_table = {\n            'i': {\"j'\": 0.9, 'aime': 0.05, 'bien': 0.02, 'jambon': 0.03, None: 0},\n            'love': {\"j'\": 0.05, 'aime': 0.9, 'bien': 0.01, 'jambon': 0.01, None: 0.03},\n            'ham': {\"j'\": 0, 'aime': 0.01, 'bien': 0, 'jambon': 0.99, None: 0},\n        }\n        alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.2)))\n        )\n\n        ibm_model = IBMModel([])\n        ibm_model.translation_table = translation_table\n        ibm_model.alignment_table = alignment_table\n\n        # act\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        # assert\n        self.assertEqual(a_info.alignment[1:], (1, 2, 4))  # 0th element unused\n        self.assertEqual(a_info.cepts, [[], [1], [2], [], [3]])", "label": 1}
{"function": "    def test_best_model2_alignment_does_not_change_pegged_alignment(self):\n        # arrange\n        sentence_pair = AlignedSent(\n            TestIBMModel.__TEST_TRG_SENTENCE, TestIBMModel.__TEST_SRC_SENTENCE\n        )\n        translation_table = {\n            'i': {\"j'\": 0.9, 'aime': 0.05, 'bien': 0.02, 'jambon': 0.03, None: 0},\n            'love': {\"j'\": 0.05, 'aime': 0.9, 'bien': 0.01, 'jambon': 0.01, None: 0.03},\n            'ham': {\"j'\": 0, 'aime': 0.01, 'bien': 0, 'jambon': 0.99, None: 0},\n        }\n        alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.2)))\n        )\n\n        ibm_model = IBMModel([])\n        ibm_model.translation_table = translation_table\n        ibm_model.alignment_table = alignment_table\n\n        # act: force 'love' to be pegged to 'jambon'\n        a_info = ibm_model.best_model2_alignment(sentence_pair, 2, 4)\n        # assert\n        self.assertEqual(a_info.alignment[1:], (1, 4, 4))\n        self.assertEqual(a_info.cepts, [[], [1], [], [], [2, 3]])", "label": 1}
{"function": "    def test_best_model2_alignment_handles_fertile_words(self):\n        # arrange\n        sentence_pair = AlignedSent(\n            ['i', 'really', ',', 'really', 'love', 'ham'],\n            TestIBMModel.__TEST_SRC_SENTENCE,\n        )\n        # 'bien' produces 2 target words: 'really' and another 'really'\n        translation_table = {\n            'i': {\"j'\": 0.9, 'aime': 0.05, 'bien': 0.02, 'jambon': 0.03, None: 0},\n            'really': {\"j'\": 0, 'aime': 0, 'bien': 0.9, 'jambon': 0.01, None: 0.09},\n            ',': {\"j'\": 0, 'aime': 0, 'bien': 0.3, 'jambon': 0, None: 0.7},\n            'love': {\"j'\": 0.05, 'aime': 0.9, 'bien': 0.01, 'jambon': 0.01, None: 0.03},\n            'ham': {\"j'\": 0, 'aime': 0.01, 'bien': 0, 'jambon': 0.99, None: 0},\n        }\n        alignment_table = defaultdict(\n            lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 0.2)))\n        )\n\n        ibm_model = IBMModel([])\n        ibm_model.translation_table = translation_table\n        ibm_model.alignment_table = alignment_table\n\n        # act\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        # assert\n        self.assertEqual(a_info.alignment[1:], (1, 3, 0, 3, 2, 4))\n        self.assertEqual(a_info.cepts, [[3], [1], [5], [2, 4], [6]])", "label": 1}
{"function": "    def test_best_model2_alignment_handles_empty_src_sentence(self):\n        # arrange\n        sentence_pair = AlignedSent(TestIBMModel.__TEST_TRG_SENTENCE, [])\n        ibm_model = IBMModel([])\n\n        # act\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        # assert\n        self.assertEqual(a_info.alignment[1:], (0, 0, 0))\n        self.assertEqual(a_info.cepts, [[1, 2, 3]])", "label": 1}
{"function": "    def test_best_model2_alignment_handles_empty_trg_sentence(self):\n        # arrange\n        sentence_pair = AlignedSent([], TestIBMModel.__TEST_SRC_SENTENCE)\n        ibm_model = IBMModel([])\n\n        # act\n        a_info = ibm_model.best_model2_alignment(sentence_pair)\n\n        # assert\n        self.assertEqual(a_info.alignment[1:], ())\n        self.assertEqual(a_info.cepts, [[], [], [], [], []])", "label": 1}
{"function": "    def test_neighboring_finds_neighbor_alignments(self):\n        # arrange\n        a_info = AlignmentInfo(\n            (0, 3, 2),\n            (None, 'des', '\u0153ufs', 'verts'),\n            ('UNUSED', 'green', 'eggs'),\n            [[], [], [2], [1]],\n        )\n        ibm_model = IBMModel([])\n\n        # act\n        neighbors = ibm_model.neighboring(a_info)\n\n        # assert\n        neighbor_alignments = set()\n        for neighbor in neighbors:\n            neighbor_alignments.add(neighbor.alignment)\n        expected_alignments = set(\n            [\n                # moves\n                (0, 0, 2),\n                (0, 1, 2),\n                (0, 2, 2),\n                (0, 3, 0),\n                (0, 3, 1),\n                (0, 3, 3),\n                # swaps\n                (0, 2, 3),\n                # original alignment\n                (0, 3, 2),\n            ]\n        )\n        self.assertEqual(neighbor_alignments, expected_alignments)", "label": 1}
{"function": "    def test_neighboring_sets_neighbor_alignment_info(self):\n        # arrange\n        a_info = AlignmentInfo(\n            (0, 3, 2),\n            (None, 'des', '\u0153ufs', 'verts'),\n            ('UNUSED', 'green', 'eggs'),\n            [[], [], [2], [1]],\n        )\n        ibm_model = IBMModel([])\n\n        # act\n        neighbors = ibm_model.neighboring(a_info)\n\n        # assert: select a few particular alignments\n        for neighbor in neighbors:\n            if neighbor.alignment == (0, 2, 2):\n                moved_alignment = neighbor\n            elif neighbor.alignment == (0, 3, 2):\n                swapped_alignment = neighbor\n\n        self.assertEqual(moved_alignment.cepts, [[], [], [1, 2], []])\n        self.assertEqual(swapped_alignment.cepts, [[], [], [2], [1]])", "label": 1}
{"function": "    def test_neighboring_returns_neighbors_with_pegged_alignment(self):\n        # arrange\n        a_info = AlignmentInfo(\n            (0, 3, 2),\n            (None, 'des', '\u0153ufs', 'verts'),\n            ('UNUSED', 'green', 'eggs'),\n            [[], [], [2], [1]],\n        )\n        ibm_model = IBMModel([])\n\n        # act: peg 'eggs' to align with '\u0153ufs'\n        neighbors = ibm_model.neighboring(a_info, 2)\n\n        # assert\n        neighbor_alignments = set()\n        for neighbor in neighbors:\n            neighbor_alignments.add(neighbor.alignment)\n        expected_alignments = set(\n            [\n                # moves\n                (0, 0, 2),\n                (0, 1, 2),\n                (0, 2, 2),\n                # no swaps\n                # original alignment\n                (0, 3, 2),\n            ]\n        )\n        self.assertEqual(neighbor_alignments, expected_alignments)", "label": 1}
{"function": "    def test_hillclimb(self):\n        # arrange\n        initial_alignment = AlignmentInfo((0, 3, 2), None, None, None)\n\n        def neighboring_mock(a, j):\n            if a.alignment == (0, 3, 2):\n                return set(\n                    [\n                        AlignmentInfo((0, 2, 2), None, None, None),\n                        AlignmentInfo((0, 1, 1), None, None, None),\n                    ]\n                )\n            elif a.alignment == (0, 2, 2):\n                return set(\n                    [\n                        AlignmentInfo((0, 3, 3), None, None, None),\n                        AlignmentInfo((0, 4, 4), None, None, None),\n                    ]\n                )\n            return set()\n\n        def prob_t_a_given_s_mock(a):\n            prob_values = {\n                (0, 3, 2): 0.5,\n                (0, 2, 2): 0.6,\n                (0, 1, 1): 0.4,\n                (0, 3, 3): 0.6,\n                (0, 4, 4): 0.7,\n            }\n            return prob_values.get(a.alignment, 0.01)\n\n        ibm_model = IBMModel([])\n        ibm_model.neighboring = neighboring_mock\n        ibm_model.prob_t_a_given_s = prob_t_a_given_s_mock\n\n        # act\n        best_alignment = ibm_model.hillclimb(initial_alignment)\n\n        # assert: hill climbing goes from (0, 3, 2) -> (0, 2, 2) -> (0, 4, 4)\n        self.assertEqual(best_alignment.alignment, (0, 4, 4))", "label": 1}
{"function": "    def test_create_dhparams(self, tmpdir):\n        filename = str(tmpdir.join(\"dhparam.pem\"))\n        certs.CertStore.load_dhparam(filename)\n        assert os.path.exists(filename)", "label": 1}
{"function": "    def test_umask_secret(self, tmpdir):\n        filename = str(tmpdir.join(\"secret\"))\n        with certs.CertStore.umask_secret(), open(filename, \"wb\"):\n            pass\n        # TODO: How do we actually attempt to read that file as another user?\n        assert os.stat(filename).st_mode & 0o77 == 0", "label": 1}
{"function": "    def test_with_ca(self, tmpdir):\n        ca = certs.CertStore.from_store(str(tmpdir), \"test\", 2048)\n        r = certs.dummy_cert(\n            ca.default_privatekey,\n            ca.default_ca,\n            b\"foo.com\",\n            [b\"one.com\", b\"two.com\", b\"*.three.com\", b\"127.0.0.1\"],\n            b\"Foo Ltd.\"\n        )\n        assert r.cn == b\"foo.com\"\n        assert r.altnames == [b'one.com', b'two.com', b'*.three.com']\n        assert r.organization == b\"Foo Ltd.\"\n\n        r = certs.dummy_cert(\n            ca.default_privatekey,\n            ca.default_ca,\n            None,\n            [],\n            None\n        )\n        assert r.cn is None\n        assert r.organization is None\n        assert r.altnames == []", "label": 1}
{"function": "    def test_simple(self, tdata):\n        with open(tdata.path(\"mitmproxy/net/data/text_cert\"), \"rb\") as f:\n            d = f.read()\n        c1 = certs.Cert.from_pem(d)\n        assert c1.cn == b\"google.com\"\n        assert len(c1.altnames) == 436\n        assert c1.organization == b\"Google Inc\"\n\n        with open(tdata.path(\"mitmproxy/net/data/text_cert_2\"), \"rb\") as f:\n            d = f.read()\n        c2 = certs.Cert.from_pem(d)\n        assert c2.cn == b\"www.inode.co.nz\"\n        assert len(c2.altnames) == 2\n        assert c2.digest(\"sha1\")\n        assert c2.notbefore\n        assert c2.notafter\n        assert c2.subject\n        assert c2.keyinfo == (\"RSA\", 2048)\n        assert c2.serial\n        assert c2.issuer\n        assert c2.to_pem()\n        assert c2.has_expired is not None\n\n        assert c1 != c2", "label": 1}
{"function": "    def test_err_broken_sans(self, tdata):\n        with open(tdata.path(\"mitmproxy/net/data/text_cert_weird1\"), \"rb\") as f:\n            d = f.read()\n        c = certs.Cert.from_pem(d)\n        # This breaks unless we ignore a decoding error.\n        assert c.altnames is not None", "label": 1}
{"function": "    def test_der(self, tdata):\n        with open(tdata.path(\"mitmproxy/net/data/dercert\"), \"rb\") as f:\n            d = f.read()\n        s = certs.Cert.from_der(d)\n        assert s.cn", "label": 1}
{"function": "    def test_state(self, tdata):\n        with open(tdata.path(\"mitmproxy/net/data/text_cert\"), \"rb\") as f:\n            d = f.read()\n        c = certs.Cert.from_pem(d)\n\n        c.get_state()\n        c2 = c.copy()\n        a = c.get_state()\n        b = c2.get_state()\n        assert a == b\n        assert c == c2\n        assert c is not c2\n\n        x = certs.Cert('')\n        x.set_state(a)\n        assert x == c", "label": 1}
{"function": "    def test_from_store_with_passphrase(self, tdata, tmpdir):\n        ca = certs.CertStore.from_store(str(tmpdir), \"mitmproxy\", 2048, \"password\")\n        ca.add_cert_file(\"*\", tdata.path(\"mitmproxy/data/mitmproxy.pem\"), \"password\")\n\n        assert ca.get_cert(b\"foo\", [])", "label": 1}
{"function": "    def __init__(self):\n        super(LiterateHaskellRenderer, self).__init__(\n            executable='ruby',\n            args=['-rubygems', os.path.join(__path__, 'bin/lhs2html.rb')])", "label": 1}
{"function": "    def is_enabled(cls, filename, syntax):\n        if syntax == 'text.tex.latex.haskell':  # Literate Haskell.tmLanguage\n            return True\n        return filename.endswith('.lhs')", "label": 1}
{"function": "def astar(matrix, targets, sources):\n    def neighbors(position):\n        (x, y) = position\n        candidates = [(x - 1, y), (x + 1, y), (x, y - 1), (x, y + 1)]\n        return [(x, y) for (x, y) in candidates if x >= 0 and x < len(matrix)\n                                               and y >= 0 and y < len(matrix[0])]\n\n    def evaluate(path):\n        f = sum(matrix[y][x] for (x, y) in path)\n        h = min(distance(path[-1], target) for target in targets)\n        return f + h\n\n    targets = set(targets)\n    frontier = set(sources)\n    explored = set()\n    frontier_queue = []\n    for source in sources:\n        path = [source]\n        heapq.heappush(frontier_queue, (evaluate(path), path))\n\n    while frontier:\n        (_, path) = heapq.heappop(frontier_queue)\n        frontier.remove(path[-1])\n        explored.add(path[-1])\n        if path[-1] in targets:\n            return path\n        for neighbor in neighbors(path[-1]):\n            if neighbor not in frontier | explored:\n                frontier.add(neighbor)\n                new_path = path + [neighbor]\n                heapq.heappush(frontier_queue, (evaluate(new_path), new_path))", "label": 1}
{"function": "def main():\n    with open(os.path.join(os.path.dirname(__file__), \"matrix.txt\")) as matfile:\n        matrix = parse(matfile)\n    targets = [(len(matrix) - 1, len(matrix[0]) - 1)]\n    sources = [(0, 0)]\n    print(sum(matrix[y][x] for (x, y) in astar(matrix, targets, sources)))", "label": 1}
{"function": "    def neighbors(position):\n        (x, y) = position\n        candidates = [(x - 1, y), (x + 1, y), (x, y - 1), (x, y + 1)]\n        return [(x, y) for (x, y) in candidates if x >= 0 and x < len(matrix)\n                                               and y >= 0 and y < len(matrix[0])]", "label": 1}
{"function": "    def evaluate(path):\n        f = sum(matrix[y][x] for (x, y) in path)\n        h = min(distance(path[-1], target) for target in targets)\n        return f + h", "label": 1}
{"function": "def main(uri):\n    # type: (Text) -> None\n    api = StrictIota(uri)\n\n    try:\n        node_info = api.get_node_info()\n    except NetworkError as e:\n        print(\n            \"Hm.  {uri} isn't responding; is the node running?\".format(uri=uri)\n        )\n        print(e)\n    except BadApiResponse as e:\n        print(\"Looks like {uri} isn't very talkative today ):\".format(uri=uri))\n        print(e)\n    else:\n        print('Hello {uri}!'.format(uri=uri))\n        pprint(node_info)", "label": 1}
{"function": "def reduce_tensor(tensor, n):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= n\n    return rt", "label": 1}
{"function": "def distribute_bn(model, world_size, reduce=False):\n    # ensure every node has the same running bn stats\n    for bn_name, bn_buf in unwrap_model(model).named_buffers(recurse=True):\n        if ('running_mean' in bn_name) or ('running_var' in bn_name):\n            if reduce:\n                # average bn stats across whole group\n                torch.distributed.all_reduce(bn_buf, op=dist.ReduceOp.SUM)\n                bn_buf /= float(world_size)\n            else:\n                # broadcast bn stats from rank 0 to whole group\n                torch.distributed.broadcast(bn_buf, 0)", "label": 1}
{"function": "    def handle(self, *args, **options):\n        Site.objects.update_or_create(\n            id=settings.SITE_ID,\n            defaults={\n                'domain': settings.HOST_NAME,\n                'name': settings.SITE_NAME\n            }\n        )", "label": 1}
{"function": "def urlopen(*args, **kwargs):\n    try:\n        return request_module.urlopen(*args, **kwargs)\n    except HTTPException as e:\n        # https://bugs.python.org/issue8823\n        raise EnvironmentError(e)", "label": 1}
{"function": "def install_urllib2_ca_file():\n    \"\"\"Makes urllib2.urlopen and urllib2.build_opener use the ca file\n    returned by get_ca_file()\n    \"\"\"\n\n    try:\n        import ssl\n    except ImportError:\n        return\n\n    base = request_module.HTTPSHandler\n\n    class MyHandler(base):\n\n        def __init__(self, debuglevel=0, context=None):\n            ca_file = get_ca_file()\n            if context is None and ca_file is not None:\n                context = ssl.create_default_context(\n                    purpose=ssl.Purpose.SERVER_AUTH,\n                    cafile=ca_file)\n            base.__init__(self, debuglevel, context)\n\n    request_module.HTTPSHandler = MyHandler", "label": 1}
{"function": "    def test_fields_under_root(self):\n        \"\"\"\n        wineventlog - Add tags and custom fields under root\n        \"\"\"\n        msg = \"Add tags and fields under root\"\n        self.write_event_log(msg)\n        evts = self.read_events(config={\n            \"tags\": [\"global\"],\n            \"fields\": {\"global\": \"field\", \"env\": \"prod\", \"log.level\": \"overwrite\"},\n            \"fields_under_root\": True,\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"tags\": [\"local\"],\n                    \"fields_under_root\": True,\n                    \"fields\": {\"local\": \"field\", \"env\": \"dev\"}\n                }\n            ]\n        })\n        self.assertTrue(len(evts), 1)\n        self.assert_common_fields(evts[0], msg=msg, level=\"overwrite\", extra={\n            \"winlog.keywords\": [\"Classic\"],\n            \"winlog.opcode\": \"Info\",\n            \"global\": \"field\",\n            \"env\": \"dev\",\n            \"local\": \"field\",\n            \"tags\": [\"global\", \"local\"],\n        })", "label": 1}
{"function": "    def test_fields_not_under_root(self):\n        \"\"\"\n        wineventlog - Add custom fields (not under root)\n        \"\"\"\n        msg = \"Add fields (not under root)\"\n        self.write_event_log(msg)\n        evts = self.read_events(config={\n            \"fields\": {\"global\": \"field\", \"env\": \"prod\", \"level\": \"overwrite\"},\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"fields\": {\"local\": \"field\", \"env\": \"dev\", \"num\": 1}\n                }\n            ]\n        })\n        self.assertTrue(len(evts), 1)\n        self.assert_common_fields(evts[0], msg=msg, extra={\n            \"log.level\": \"information\",\n            \"winlog.keywords\": [\"Classic\"],\n            \"winlog.opcode\": \"Info\",\n            \"fields.global\": \"field\",\n            \"fields.env\": \"dev\",\n            \"fields.level\": \"overwrite\",\n            \"fields.local\": \"field\",\n            \"fields.num\": 1,\n        })\n        self.assertTrue(\"tags\" not in evts[0])", "label": 1}
{"function": "    def test_include_xml(self):\n        \"\"\"\n        wineventlog - Include raw XML event\n        \"\"\"\n        msg = \"Include raw XML event\"\n        self.write_event_log(msg)\n        evts = self.read_events(config={\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"include_xml\": True,\n                }\n            ]\n        })\n        self.assertTrue(len(evts), 1)\n        self.assert_common_fields(evts[0], msg=msg)\n        self.assertTrue(\"event.original\" in evts[0])\n        original = evts[0][\"event.original\"]\n        self.assertTrue(original.endswith('</Event>'),\n                        'xml value should end with </Event>: \"{}\"'.format(original))", "label": 1}
{"function": "    def test_query_event_id(self):\n        \"\"\"\n        wineventlog - Query by event IDs\n        \"\"\"\n        msg = \"event_id test case\"\n        self.write_event_log(msg, eventID=10)  # Excluded\n        self.write_event_log(msg, eventID=50)\n        self.write_event_log(msg, eventID=100)\n        self.write_event_log(msg, eventID=150)  # Excluded\n        self.write_event_log(msg, eventID=175)\n        self.write_event_log(msg, eventID=200)\n        evts = self.read_events(config={\n            \"tags\": [\"event_id\"],\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"event_id\": \"50, 100-200, -150\"\n                }\n            ]\n        }, expected_events=4)\n        self.assertTrue(len(evts), 4)\n        self.assertEqual(evts[0][\"winlog.event_id\"], 50)\n        self.assertEqual(evts[1][\"winlog.event_id\"], 100)\n        self.assertEqual(evts[2][\"winlog.event_id\"], 175)\n        self.assertEqual(evts[3][\"winlog.event_id\"], 200)", "label": 1}
{"function": "    def test_query_level_single(self):\n        \"\"\"\n        wineventlog - Query by level (warning)\n        \"\"\"\n        self.write_event_log(\"success\", level=win32evtlog.EVENTLOG_SUCCESS)\n        self.write_event_log(\"error\", level=win32evtlog.EVENTLOG_ERROR_TYPE)\n        self.write_event_log(\n            \"warning\", level=win32evtlog.EVENTLOG_WARNING_TYPE)\n        self.write_event_log(\n            \"information\", level=win32evtlog.EVENTLOG_INFORMATION_TYPE)\n        evts = self.read_events(config={\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"level\": \"warning\"\n                }\n            ]\n        })\n        self.assertTrue(len(evts), 1)\n        self.assertEqual(evts[0][\"log.level\"], \"warning\")", "label": 1}
{"function": "    def test_query_level_multiple(self):\n        \"\"\"\n        wineventlog - Query by level (error, warning)\n        \"\"\"\n        self.write_event_log(\n            \"success\", level=win32evtlog.EVENTLOG_SUCCESS)  # Level 0, Info\n        self.write_event_log(\n            \"error\", level=win32evtlog.EVENTLOG_ERROR_TYPE)  # Level 2\n        self.write_event_log(\n            \"warning\", level=win32evtlog.EVENTLOG_WARNING_TYPE)  # Level 3\n        self.write_event_log(\n            \"information\", level=win32evtlog.EVENTLOG_INFORMATION_TYPE)  # Level 4\n        evts = self.read_events(config={\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"level\": \"error, warning\"\n                }\n            ]\n        }, expected_events=2)\n        self.assertTrue(len(evts), 2)\n        self.assertEqual(evts[0][\"log.level\"], \"error\")\n        self.assertEqual(evts[1][\"log.level\"], \"warning\")", "label": 1}
{"function": "    def test_query_ignore_older(self):\n        \"\"\"\n        wineventlog - Query by time (ignore_older than 2s)\n        \"\"\"\n        self.write_event_log(\">=2 seconds old\", eventID=20)\n        time.sleep(2)\n        self.write_event_log(\"~0 seconds old\", eventID=10)\n        evts = self.read_events(config={\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"ignore_older\": \"2s\"\n                }\n            ]\n        })\n        self.assertTrue(len(evts), 1)\n        self.assertEqual(evts[0][\"winlog.event_id\"], 10)\n        self.assertEqual(evts[0][\"event.code\"], 10)", "label": 1}
{"function": "    def test_query_provider(self):\n        \"\"\"\n        wineventlog - Query by provider name (event source)\n        \"\"\"\n        self.write_event_log(\"selected\", source=self.otherAppName)\n        self.write_event_log(\"filtered\")\n        evts = self.read_events(config={\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"provider\": [self.otherAppName]\n                }\n            ]\n        })\n        self.assertTrue(len(evts), 1)\n        self.assertEqual(evts[0][\"winlog.provider_name\"], self.otherAppName)", "label": 1}
{"function": "    def test_query_multi_param(self):\n        \"\"\"\n        wineventlog - Query by multiple params\n        \"\"\"\n        self.write_event_log(\"selected\", source=self.otherAppName,\n                             eventID=556, level=win32evtlog.EVENTLOG_ERROR_TYPE)\n        self.write_event_log(\"filtered\", source=self.otherAppName, eventID=556)\n        self.write_event_log(\n            \"filtered\", level=win32evtlog.EVENTLOG_WARNING_TYPE)\n        evts = self.read_events(config={\n            \"event_logs\": [\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"event_id\": \"10-20, 30-40, -35, -18, 400-1000, -432\",\n                    \"level\": \"warn, error\",\n                    \"provider\": [self.otherAppName]\n                }\n            ]\n        })\n        self.assertTrue(len(evts), 1)\n        self.assertEqual(evts[0][\"message\"], \"selected\")", "label": 1}
{"function": "    def test_unknown_eventlog_config(self):\n        \"\"\"\n        wineventlog - Unknown config parameter\n        \"\"\"\n        self.render_config_template(\n            event_logs=[\n                {\n                    \"name\": self.providerName,\n                    \"api\": self.api,\n                    \"forwarded\": False,\n                    \"invalid\": \"garbage\"}\n            ]\n        )\n        self.start_beat().check_wait(exit_code=1)\n        assert self.log_contains(\n            \"1 error: invalid event log key 'invalid' found.\")", "label": 1}
{"function": "    def setUp(self):\n\n        model = gtsam.noiseModel.Isotropic.Sigma(6, 0.1)\n\n        # We consider a small graph:\n        #                            symbolic FG\n        #               x2               0  1\n        #             / | \\              1  2\n        #            /  |  \\             2  3\n        #          x3   |   x1           2  0\n        #           \\   |   /            0  3\n        #            \\  |  /\n        #               x0\n        #\n        p0 = Point3(0, 0, 0)\n        self.R0 = Rot3.Expmap(np.array([0.0, 0.0, 0.0]))\n        p1 = Point3(1, 2, 0)\n        self.R1 = Rot3.Expmap(np.array([0.0, 0.0, 1.570796]))\n        p2 = Point3(0, 2, 0)\n        self.R2 = Rot3.Expmap(np.array([0.0, 0.0, 3.141593]))\n        p3 = Point3(-1, 1, 0)\n        self.R3 = Rot3.Expmap(np.array([0.0, 0.0, 4.712389]))\n\n        pose0 = Pose3(self.R0, p0)\n        pose1 = Pose3(self.R1, p1)\n        pose2 = Pose3(self.R2, p2)\n        pose3 = Pose3(self.R3, p3)\n\n        g = NonlinearFactorGraph()\n        g.add(gtsam.BetweenFactorPose3(x0, x1, pose0.between(pose1), model))\n        g.add(gtsam.BetweenFactorPose3(x1, x2, pose1.between(pose2), model))\n        g.add(gtsam.BetweenFactorPose3(x2, x3, pose2.between(pose3), model))\n        g.add(gtsam.BetweenFactorPose3(x2, x0, pose2.between(pose0), model))\n        g.add(gtsam.BetweenFactorPose3(x0, x3, pose0.between(pose3), model))\n        g.add(gtsam.PriorFactorPose3(x0, pose0, model))\n        self.graph = g", "label": 1}
{"function": "    def test_buildPose3graph(self):\n        pose3graph = gtsam.InitializePose3.buildPose3graph(self.graph)", "label": 1}
{"function": "    def test_orientations(self):\n        pose3Graph = gtsam.InitializePose3.buildPose3graph(self.graph)\n        initial = gtsam.InitializePose3.computeOrientationsChordal(pose3Graph)\n    \n        # comparison is up to M_PI, that's why we add some multiples of 2*M_PI\n        self.gtsamAssertEquals(initial.atRot3(x0), self.R0, 1e-6)\n        self.gtsamAssertEquals(initial.atRot3(x1), self.R1, 1e-6)\n        self.gtsamAssertEquals(initial.atRot3(x2), self.R2, 1e-6)\n        self.gtsamAssertEquals(initial.atRot3(x3), self.R3, 1e-6)", "label": 1}
{"function": "    def test_initializePoses(self):\n        g2oFile = gtsam.findExampleDataFile(\"pose3example-grid\")\n        is3D = True\n        inputGraph, expectedValues = gtsam.readG2o(g2oFile, is3D)\n        priorModel = gtsam.noiseModel.Unit.Create(6)\n        inputGraph.add(gtsam.PriorFactorPose3(0, Pose3(), priorModel))\n\n        initial = gtsam.InitializePose3.initialize(inputGraph)\n        # TODO(frank): very loose !!\n        self.gtsamAssertEquals(initial, expectedValues, 0.1)", "label": 1}
{"function": "    def __init__(self, ai_game):\n        \"\"\"Initialize the alien and set its starting position.\"\"\"\n        super().__init__()\n        self.screen = ai_game.screen\n        self.settings = ai_game.settings\n\n        # Load the alien image and set its rect attribute.\n        self.image = pygame.image.load('images/alien.bmp')\n        self.rect = self.image.get_rect()\n\n        # Start each new alien near the top left of the screen.\n        self.rect.x = self.rect.width\n        self.rect.y = self.rect.height\n\n        # Store the alien's exact horizontal position.\n        self.x = float(self.rect.x)", "label": 1}
{"function": "    def check_edges(self):\n        \"\"\"Return True if alien is at edge of screen.\"\"\"\n        screen_rect = self.screen.get_rect()\n        if self.rect.right >= screen_rect.right or self.rect.left <= 0:\n            return True", "label": 1}
{"function": "    def update(self):\n        \"\"\"Move the alien right or left.\"\"\"\n        self.x += (self.settings.alien_speed *\n                        self.settings.fleet_direction)\n        self.rect.x = self.x", "label": 1}
{"function": "def _whats_your_job():\n    return 'ping'", "label": 1}
{"function": "def _whats_your_name():\n    return 'ping'", "label": 1}
{"function": "def _support_ipv6():\n    return True", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, open, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, real0, real1):\n        pass", "label": 1}
{"function": "    def __init__(self, real, timeperiod=30):\n        pass", "label": 1}
{"function": "    def __init__(self, real, timeperiod=5, vfactor=0.7):\n        pass", "label": 1}
{"function": "    def __init__(self, real):\n        pass", "label": 1}
{"function": "    def __init__(self, real):\n        pass", "label": 1}
{"function": "    def __init__(self, real, timeperiod=30):\n        pass", "label": 1}
{"function": "    def __init__(self, high, low, close):\n        pass", "label": 1}
{"function": "    def __init__(self, real, timeperiod=30):\n        pass", "label": 1}
{"function": "    def __init__(self, real, timeperiod=30):\n        pass", "label": 1}
{"function": "    def __init__(self, real, timeperiod=14):\n        pass", "label": 1}
{"function": "  def testCalcReductionLayers(self):\n    num_cells = 18\n    num_reduction_layers = 2\n    reduction_layers = nasnet_utils.calc_reduction_layers(\n        num_cells, num_reduction_layers)\n    self.assertEqual(len(reduction_layers), 2)\n    self.assertEqual(reduction_layers[0], 6)\n    self.assertEqual(reduction_layers[1], 12)", "label": 1}
{"function": "  def testGetChannelIndex(self):\n    data_formats = ['NHWC', 'NCHW']\n    for data_format in data_formats:\n      index = nasnet_utils.get_channel_index(data_format)\n      correct_index = 3 if data_format == 'NHWC' else 1\n      self.assertEqual(index, correct_index)", "label": 1}
{"function": "  def testGetChannelDim(self):\n    data_formats = ['NHWC', 'NCHW']\n    shape = [10, 20, 30, 40]\n    for data_format in data_formats:\n      dim = nasnet_utils.get_channel_dim(shape, data_format)\n      correct_dim = shape[3] if data_format == 'NHWC' else shape[1]\n      self.assertEqual(dim, correct_dim)", "label": 1}
{"function": "  def testGlobalAvgPool(self):\n    data_formats = ['NHWC', 'NCHW']\n    inputs = tf.placeholder(tf.float32, (5, 10, 20, 10))\n    for data_format in data_formats:\n      output = nasnet_utils.global_avg_pool(\n          inputs, data_format)\n      self.assertEqual(output.shape, [5, 10])", "label": 1}
{"function": "    def action_redirect_to_forum(self):\n        self.ensure_one()\n        action = self.env.ref('website_forum.action_forum_post').read()[0]\n        action['view_mode'] = 'tree'\n        action['context'] = {\n            'create': False\n        }\n        action['domain'] = [('forum_id', '=', self.forum_id.id)]\n\n        return action", "label": 1}
{"function": "def demo():\n    import time\n\n    ble = bluetooth.BLE()\n    uart = BLEUART(ble)\n\n    def on_rx():\n        print('rx: ', uart.read().decode().strip())\n\n    uart.irq(handler=on_rx)\n    nums = [4, 8, 15, 16, 23, 42]\n    i = 0\n\n    try:\n        while True:\n            uart.write(str(nums[i]) + '\\n')\n            i = (i + 1) % len(nums)\n            time.sleep_ms(1000)\n    except KeyboardInterrupt:\n        pass\n\n    uart.close()", "label": 1}
{"function": "    def __init__(self, ble, name='mpy-uart', rxbuf=100):\n        self._ble = ble\n        self._ble.active(True)\n        self._ble.irq(handler=self._irq)\n        ((self._tx_handle, self._rx_handle,),) = self._ble.gatts_register_services((_UART_SERVICE,))\n        # Increase the size of the rx buffer and enable append mode.\n        self._ble.gatts_set_buffer(self._rx_handle, rxbuf, True)\n        self._connections = set()\n        self._rx_buffer = bytearray()\n        self._handler = None\n        # Optionally add services=[_UART_UUID], but this is likely to make the payload too large.\n        self._payload = advertising_payload(name=name, appearance=_ADV_APPEARANCE_GENERIC_COMPUTER)\n        self._advertise()", "label": 1}
{"function": "    def irq(self, handler):\n        self._handler = handler", "label": 1}
{"function": "    def _irq(self, event, data):\n        # Track connections so we can send notifications.\n        if event == _IRQ_CENTRAL_CONNECT:\n            conn_handle, _, _, = data\n            self._connections.add(conn_handle)\n        elif event == _IRQ_CENTRAL_DISCONNECT:\n            conn_handle, _, _, = data\n            if conn_handle in self._connections:\n                self._connections.remove(conn_handle)\n            # Start advertising again to allow a new connection.\n            self._advertise()\n        elif event == _IRQ_GATTS_WRITE:\n            conn_handle, value_handle, = data\n            if conn_handle in self._connections and value_handle == self._rx_handle:\n                self._rx_buffer += self._ble.gatts_read(self._rx_handle)\n                if self._handler:\n                    self._handler()", "label": 1}
{"function": "    def any(self):\n        return len(self._rx_buffer)", "label": 1}
{"function": "    def show_status(self, status, force=False):\n        'Displays the specified status message.'\n\n        if not force and status is getattr(self, '_shown_status', None):\n            return\n\n        # make the text area not editable for statuses like \"Invisble\" and\n        # \"Offline\", which have the \"editable\" attribute set to False\n        self.Editable = status.editable\n\n        self.display.empty_text = getattr(status, 'hint', '')\n        self.ChangeValue(status.message)                    # change text\n        self.SetButtonIcon(StatusMessage.icon_for(status))  # change icon\n        self.status_state = status.status                   # store the state\n        self._shown_status = status", "label": 1}
{"function": "    def on_typing(self, e):\n        'Invoked when the user is typing in the textfield.'\n\n        if self.searching:\n            search.link_prefs(profile.prefs)\n            e.Skip()\n            self.buddylist.search(e.EventObject.Value)\n        else:\n            self.cancel_timer()", "label": 1}
{"function": "    def on_status_button(self, button):\n        '''\n        Invoked when the user clicks the state button to the left of the\n        dropdown.\n        '''\n        # toggle the control's status state\n        isavail = StatusMessage.is_available_state(self.status_state)\n\n        # do we need to change the shown text?\n        needs_change = self._shown_status in StatusMessage.SpecialStatuses or not self._shown_status.editable\n\n        self.oldValue = None\n\n        self.change_state(state = 'Away' if isavail else 'Available',)", "label": 1}
{"function": "    def change_state(self, state, change_text = False):\n        if not isinstance(state, basestring):\n            raise TypeError('change_state takes a string got a %s' % type(state))\n        self.status_state = state\n\n        if change_text:\n            self.ChangeValue(self.status_state, state.title())\n        else:\n            self.Default = state.title()\n\n        edit_toggle = getattr(profile.status, 'edit_toggle', True)\n        if getattr(profile.status, 'edit_toggle', True):\n            # update the icon\n            self.SetButtonIcon(StatusMessage.icon_for(self.status_state))\n\n            self.cancel_timer()\n            self.timer.StartOneShot(self.set_delay)\n\n            # select all text in the textfield\n            disp = self.display\n            disp.TypeField()\n            wx.CallAfter(disp.txtfld.SetSelection, -1, -1)\n        else:\n            self.setandshow(profile.status.copy(status = self.status_state, editable = None, edit_toggle = None))", "label": 1}
{"function": "    def on_status_button_left_click(self, e = None):\n        if self.searching:\n            return self.TextField.SetFocus()\n        self.skipenter = True\n        self.button_timer.Start(BUTTON_HOLD_TIME, True)\n        if e: e.Skip(True)", "label": 1}
{"function": "    def on_status_button_left_up(self, e = None):\n        if not self.searching:\n            self.button_timer.Stop()\n        if e: e.Skip(True)", "label": 1}
{"function": "    def on_status_button_right_click(self, e = None):\n        if not self.searching:\n            self.show_extended_status_menu()", "label": 1}
{"function": "    def show_extended_status_menu(self):\n        from gui.status import get_state_choices\n\n        m = SimpleMenu(self, skinkey = skin.get('%s.MenuSkin'%self.skinkey))\n\n        for status in get_state_choices():\n            statusname, statuslabel = status\n\n            def onclick(item, state=statusname):\n                self.change_state(state)#, change_text = self.status_state == self.GetValue())\n\n            m.AppendItem(SimpleMenuItem([StatusMessage.icon_for(statusname), statuslabel],\n                                        method = onclick))\n\n        if m.GetCount() > 0:\n            m.Display(self.cbutton)", "label": 1}
{"function": "    def on_text_lose_focus(self, new_msg):\n        if self.searching:\n            return self.on_search_timer()\n\n        # Cancel the status button timer if it's running.\n        self.cancel_timer()\n\n        if getattr(self, 'skipenter', False):\n            wx.CallAfter(lambda: setattr(self, 'skipenter', False))\n        else:\n            # don't set status if we lost focus because the user is clicking\n            # on the state button\n            if wx.GetMouseState().LeftDown() and wx.FindWindowAtPoint(wx.GetMousePosition()) is self.cbutton:\n                return\n\n            profile_status = self.get_profile_status()\n            if new_msg == '':\n                self.display.empty_text = profile_status.hint\n            if new_msg != profile_status.message or self.status_state != profile_status.status:\n                # entering a new text value clears all exceptions\n                newmsg = StatusMessage(new_msg, self.status_state, new_msg)\n                self.set_profile_status(newmsg)", "label": 1}
{"function": "    def on_profile_status_changed(self, *a):\n        \"Invoked when the profile's status changes.\"\n\n        self.show_status(profile.status)", "label": 1}
{"function": "  def get_prior_mean(self):\n    \"\"\"If un-chunked data is available, set initial level to the first value.\"\"\"\n    with variable_scope.variable_scope(self._variable_scope):\n      if self._input_statistics is not None:\n        # TODO(allenl): Better support for multivariate series here.\n        initial_value = array_ops.stack([\n            math_ops.reduce_mean(\n                self._scale_data(\n                    self._input_statistics.series_start_moments.mean)),\n            0.\n        ])\n        return initial_value + variable_scope.get_variable(\n            name=\"prior_state_mean\",\n            shape=initial_value.get_shape(),\n            initializer=init_ops.zeros_initializer(),\n            dtype=self.dtype,\n            trainable=self._configuration.trainable_start_state)\n      else:\n        return super(AdderStateSpaceModel, self).get_prior_mean()", "label": 1}
{"function": "  def transition_to_powers(self, powers):\n    \"\"\"Computes powers of the adder transition matrix efficiently.\n\n    Args:\n      powers: An integer Tensor, shape [...], with powers to raise the\n        transition matrix to.\n    Returns:\n      A floating point Tensor with shape [..., 2, 2] containing:\n        transition^power = [[1., power],\n                            [0., 1.]]\n    \"\"\"\n    paddings = array_ops.concat(\n        [\n            array_ops.zeros([array_ops.rank(powers), 2], dtype=dtypes.int32),\n            [(0, 1), (1, 0)]\n        ],\n        axis=0)\n    powers_padded = array_ops.pad(powers[..., None, None], paddings=paddings)\n    identity_matrices = linalg_ops.eye(\n        num_rows=2, batch_shape=array_ops.shape(powers), dtype=self.dtype)\n    return identity_matrices + math_ops.cast(powers_padded, self.dtype)", "label": 1}
{"function": "  def transition_power_noise_accumulator(self, num_steps):\n    \"\"\"Computes power sums in closed form.\"\"\"\n    def _pack_and_reshape(*values):\n      return array_ops.reshape(\n          array_ops.stack(axis=1, values=values),\n          array_ops.concat(values=[array_ops.shape(num_steps), [2, 2]], axis=0))\n\n    num_steps = math_ops.cast(num_steps, self.dtype)\n    noise_transitions = num_steps - 1\n    noise_transform = ops.convert_to_tensor(self.get_noise_transform(),\n                                            self.dtype)\n    noise_covariance_transformed = math_ops.matmul(\n        math_ops.matmul(noise_transform,\n                        self.state_transition_noise_covariance),\n        noise_transform,\n        adjoint_b=True)\n    # Un-packing the transformed noise as:\n    # [[a b]\n    #  [c d]]\n    a, b, c, d = array_ops.unstack(\n        array_ops.reshape(noise_covariance_transformed, [-1, 4]), axis=1)\n    sum_of_first_n = noise_transitions * (noise_transitions + 1) / 2\n    sum_of_first_n_squares = sum_of_first_n * (2 * noise_transitions + 1) / 3\n    return _pack_and_reshape(\n        num_steps * a + sum_of_first_n * (b + c) + sum_of_first_n_squares * d,\n        num_steps * b + sum_of_first_n * d,\n        num_steps * c + sum_of_first_n * d,\n        num_steps * d)", "label": 1}
{"function": "  def get_state_transition(self):\n    return [[1., 1.],  # Add slope to level\n            [0., 1.]]  # Maintain slope", "label": 1}
{"function": "  def get_noise_transform(self):\n    if self.use_level_noise:\n      return [[1., 0.],\n              [0., 1.]]\n    else:\n      return [[0.],\n              [1.]]", "label": 1}
{"function": "  def get_observation_model(self, times):\n    \"\"\"Observe level but not slope.\n\n    See StateSpaceModel.get_observation_model.\n\n    Args:\n      times: Unused. See the parent class for details.\n    Returns:\n      A static, univariate observation model for later broadcasting.\n    \"\"\"\n    del times  # Does not rely on times. Uses broadcasting from the parent.\n    return constant_op.constant([1., 0.], dtype=self.dtype)", "label": 1}
{"function": "    def _pack_and_reshape(*values):\n      return array_ops.reshape(\n          array_ops.stack(axis=1, values=values),\n          array_ops.concat(values=[array_ops.shape(num_steps), [2, 2]], axis=0))", "label": 1}
{"function": "    def initialize(self) -> None:\n        \"\"\"Initialize.\"\"\"\n        super().initialize()\n        self.door_sensor = self.args[\"mail_door_sensor\"]\n        self.slot_sensor = self.args[\"mail_slot_sensor\"]\n        self.mail_status = self.args[\"mail_status\"]\n        self.state = self.get_state(self.mail_status)\n\n        self.newState = \"\"\n        self.just_opened_door = False\n        self.just_notified = False\n        \n        self.listen_state(self.just_opened, self.args[\"door\"])\n\n        if \"mail_slot_sensor\" in self.args:\n            self.listen_state(self.mailbox_opened, self.slot_sensor)\n        if \"mail_door_sensor\" in self.args:\n            self.listen_state(self.mailbox_opened, self.door_sensor)", "label": 1}
{"function": "    def just_opened(self, entity, attribute, old, new, kwargs):\n        if (new == \"Open\"):\n            self.log(\"Just opened front door\")\n            self.just_opened_door = True\n            self.run_in(self.reset_just_opened, 90)", "label": 1}
{"function": "    def reset_just_opened(self, kwargs):\n        self.log(\"Front door no longer just opened\")\n        self.just_opened_door = False", "label": 1}
{"function": "    def read(self):\n        self.get_style_context().add_class('read')\n        self._g('unread').props.visible = False", "label": 1}
{"function": "    def do_event(self, event):\n        shortcuts = {\n            'u': (self._sbb.vote, [+1]),\n            'd': (self._sbb.vote, [-1]),\n            'n': (self._sbb.vote, [0]),\n            'c': (self.goto_comments.emit, []),\n            'a': (self.get_toplevel().goto_sublist,\n                  ['/u/{}'.format(self.data['author'])]),\n            's': (self.get_toplevel().goto_sublist,\n                  ['/r/{}'.format(self.data['subreddit'])]),\n        }\n        return process_shortcuts(shortcuts, event)", "label": 1}
{"function": "    def __comments_clicked_cb(self, button):\n        self.goto_comments.emit()", "label": 1}
{"function": "    def _fetch_thumbnail(self):\n        url = get_thumbnail_url(self.data)\n        if url is not None:\n            self._msg = self._api.download_thumb(\n                url, self.__message_done_cb)", "label": 1}
{"function": "    def do_unrealize(self):\n        if self._msg is not None:\n            self._api.cancel(self._msg)", "label": 1}
{"function": "    def __message_done_cb(self, pixbuf):\n        self._msg = None\n        self._g('preview').props.pixbuf = pixbuf\n        self._g('preview-button').show()\n        self._g('preview-button').connect('clicked', self.__image_clicked_cb)", "label": 1}
{"function": "    def __image_clicked_cb(self, button):\n        if self._preview_palette is None:\n            self._preview_palette = get_preview_palette(\n                self._api, self.data, relative_to=button)\n        self._preview_palette.show()", "label": 1}
{"function": "    def __init__(self, api: RedditAPI, data):\n        Gtk.ListBoxRow.__init__(self)\n        self.get_style_context().add_class('link-row')\n        self.add_events(Gdk.EventMask.KEY_PRESS_MASK)\n        self._api = api\n        self.data = data['data']\n\n        is_comment_reply = self.data.get('subreddit') is not None\n\n        self._builder = Gtk.Builder.new_from_resource(\n            '/today/sam/reddit-is-gtk/row-comment.ui')\n        self._g = self._builder.get_object\n        self.add(self._g('box'))\n\n        read = not self.data.get('new', True)\n        if read:\n            self.read()\n\n        # Keep a reference so the GC doesn't collect them\n        self._abb = AuthorButtonBehaviour(self._g('author'), self.data)\n        self._tbb = TimeButtonBehaviour(self._g('time'), self.data)\n        if is_comment_reply:\n            self._srbb = SubButtonBehaviour(self._g('subreddit'), self.data)\n        else:\n            self._g('subreddit').props.sensitive = False\n            self._g('subreddit').props.label = 'PM'\n\n        self._g('nsfw').props.visible = self.data.get('over_18')\n        self._g('saved').props.visible = self.data.get('saved')\n\n        self._g('title').props.label = (self.data.get('link_title') or\n                                        self.data['subject'])\n        content = newmarkdown.make_markdown_widget(self.data['body'])\n        self._g('grid').attach(content, 0, 2, 3, 1)\n\n        if is_comment_reply:\n            self._g('type-private-message').props.visible = False\n        else:\n            self._g('type-comment-reply').props.visible = False", "label": 1}
{"function": "    def do_event(self, event):\n        shortcuts = {\n            'a': (self.get_toplevel().goto_sublist,\n                  ['/u/{}'.format(self.data['author'])]),\n        }\n        if self.data.get('subreddit'):\n            shortcuts['s'] = (\n                self.get_toplevel().goto_sublist,\n                ['/r/{}'.format(self.data['subreddit'])],\n            )\n        return process_shortcuts(shortcuts, event)", "label": 1}
{"function": "    def read(self):\n        if 'new' in self.data and self.data['new']:\n            self._api.read_message(self.data['name'])\n            self.data['new'] = False\n        self.get_style_context().add_class('read')\n        self._g('unread').props.visible = False", "label": 1}
{"function": "    def test_keypath_separator_getter_setter(self):\n        d = KeypathDict({}, keypath_separator=None)\n        self.assertEqual(d.keypath_separator, None)\n        d['a.b.c'] = 1\n        with self.assertRaises(ValueError):\n            d.keypath_separator = '.'\n        d.keypath_separator = '/'\n        self.assertEqual(d.keypath_separator, '/')\n        d['x/y/z'] = 2\n        r = {\n            'a.b.c': 1,\n            'x': {\n                'y': {\n                    'z': 2,\n                },\n            },\n        }\n        self.assertEqual(d, r)", "label": 1}
{"function": "    def test_set_override_existing_item(self):\n        d = {}\n        b = KeypathDict(d)\n        b.set('a.b.c', 1)\n        r = {\n            'a': {\n                'b': {\n                    'c': 1\n                }\n            }\n        }\n        b.set('a.b.c', 2)\n        r = {\n            'a': {\n                'b': {\n                    'c': 2\n                }\n            }\n        }\n        self.assertEqual(b, r)\n        b.set('a.b.c.d', 3)\n        r = {\n            'a': {\n                'b': {\n                    'c': {\n                        'd': 3\n                    }\n                }\n            }\n        }\n        self.assertEqual(b, r)", "label": 1}
{"function": "    def test_setitem_override_existing_item(self):\n        d = {}\n        b = KeypathDict(d)\n        b['a.b.c'] = 1\n        r = {\n            'a': {\n                'b': {\n                    'c': 1\n                }\n            }\n        }\n        b['a.b.c'] = 2\n        r = {\n            'a': {\n                'b': {\n                    'c': 2\n                }\n            }\n        }\n        self.assertEqual(b, r)\n        b['a.b.c.d'] = 3\n        r = {\n            'a': {\n                'b': {\n                    'c': {\n                        'd': 3\n                    }\n                }\n            }\n        }\n        self.assertEqual(b, r)", "label": 1}
{"function": "    def test_setitem_with_keys_list(self):\n        d = {\n            'a': {\n                'b': {\n                    'c': 1,\n                    'd': 2,\n                },\n            },\n        }\n        b = KeypathDict(d)\n        b['a', 'b.c'] = 2\n        self.assertEqual(b['a.b.c'], 2)\n        b['a', 'b', 'c'] = 3\n        self.assertEqual(b['a.b.c'], 3)\n        b['a', 'b', 'd'] = 4\n        self.assertEqual(b['a.b.d'], 4)\n        b['a', 'b', 'e'] = 5\n        self.assertEqual(b['a.b.e'], 5)", "label": 1}
{"function": "    def test_setitem_with_keys_list_and_no_keypath_separator(self):\n        d = {\n            'a': {\n                'b': {\n                    'c': 1,\n                    'd': 2,\n                },\n            },\n        }\n        b = KeypathDict(d, keypath_separator=None)\n        b['a', 'b', 'c'] = 3\n        with self.assertRaises(KeyError):\n            val = b['a.b.c']\n            print(val)\n        self.assertEqual(b['a', 'b', 'c'], 3)\n\n        b['a', 'b', 'd'] = 4\n        with self.assertRaises(KeyError):\n            val = b['a.b.d']\n            print(val)\n        self.assertEqual(b['a', 'b', 'd'], 4)\n\n        b['a', 'b', 'e'] = 5\n        with self.assertRaises(KeyError):\n            val = b['a.b.e']\n            print(val)\n        self.assertEqual(b['a', 'b', 'e'], 5)", "label": 1}
{"function": "    def test_setitem_with_dict_value_with_separator_in_keys(self):\n        d = {\n            'a': {\n                'b': {\n                    'c': 1,\n                    'd': 2,\n                },\n            },\n        }\n        b = KeypathDict(d)\n        v = {\n            'i.j.k': 3,\n            'x.y.z': 4,\n        }\n        # print(b['a.b.e.x.y.z'])\n        # print(b.keypaths())\n        with self.assertRaises(ValueError):\n            b['a.b.e'] = v", "label": 1}
{"function": "    def test_delitem_with_1_valid_key(self):\n        d = {\n            'a': 1,\n        }\n        b = KeypathDict(d)\n        del b['a']\n        with self.assertRaises(KeyError):\n            del b['a']\n        self.assertEqual(b.get('a'), None)", "label": 1}
{"function": "    def test_delitem_with_1_invalid_key(self):\n        d = {\n            'a': 1,\n        }\n        b = KeypathDict(d)\n        with self.assertRaises(KeyError):\n            del b['b']\n        self.assertEqual(b.get('b'), None)", "label": 1}
{"function": "    def test_delitem_with_2_valid_keys(self):\n        d = {\n            'a': {\n                'b': 1,\n            }\n        }\n        b = KeypathDict(d)\n\n        del b['a.b']\n        with self.assertRaises(KeyError):\n            del b['a.b']\n        self.assertEqual(b.get('a'), {})\n\n        del b['a']\n        with self.assertRaises(KeyError):\n            del b['a']\n        self.assertEqual(b.get('a'), None)", "label": 1}
{"function": "    def test_delitem_with_2_invalid_keys(self):\n        d = {\n            'a': {\n                'b': 1,\n            }\n        }\n        b = KeypathDict(d)\n        with self.assertRaises(KeyError):\n            del b['a.c']\n        self.assertEqual(b.get('a'), { 'b': 1 })", "label": 1}
{"function": "  def testExecuteUpdateJobsFailureOnSecondCopyNoCSCollisionNoForce(\n      self, uploader_cs_mock):\n    uploader_cs_mock.Exists.side_effect = [False, True, False]\n    uploader_cs_mock.Copy.side_effect = cloud_storage.CloudStorageError\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    config._config_data = self.new_dependencies.copy()\n    config._is_dirty = True\n    config._pending_uploads = [self.new_pending_upload,\n                               self.final_pending_upload]\n    self.assertEqual(self.new_dependencies, config._config_data)\n    self.assertTrue(config._is_dirty)\n    self.assertEqual(2, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    self.assertEqual(self.final_pending_upload, config._pending_uploads[1])\n    expected_exists_calls = [mock.call(self.new_bucket, self.new_remote_path),\n                             mock.call(self.final_bucket,\n                                       self.final_remote_path)]\n    expected_insert_calls = [mock.call(self.new_bucket, self.new_remote_path,\n                                       self.new_dep_path)]\n    expected_copy_calls = []\n    expected_delete_calls = [mock.call(self.new_bucket, self.new_remote_path)]\n\n    self.assertRaises(cloud_storage.CloudStorageError,\n                      config.ExecuteUpdateJobs)\n    self.assertTrue(config._is_dirty)\n    self.assertEqual(2, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    self.assertEqual(self.final_pending_upload, config._pending_uploads[1])\n    self.assertEqual(self.new_dependencies, config._config_data)\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    expected_file_lines = list(self.expected_file_lines)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))\n    self.assertEqual(expected_insert_calls,\n                     uploader_cs_mock.Insert.call_args_list)\n    self.assertEqual(expected_exists_calls,\n                     uploader_cs_mock.Exists.call_args_list)\n    self.assertEqual(expected_copy_calls,\n                     uploader_cs_mock.Copy.call_args_list)\n    self.assertEqual(expected_delete_calls,\n                     uploader_cs_mock.Delete.call_args_list)", "label": 1}
{"function": "  def testExecuteUpdateJobsSuccessOnePendingDepNoCloudStorageCollision(\n      self, uploader_cs_mock):\n    uploader_cs_mock.Exists.return_value = False\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    config._config_data = self.new_dependencies.copy()\n    config._pending_uploads = [self.new_pending_upload]\n    self.assertEqual(self.new_dependencies, config._config_data)\n    self.assertTrue(config._IsDirty())\n    self.assertEqual(1, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    expected_exists_calls = [mock.call(self.new_bucket, self.new_remote_path)]\n    expected_insert_calls = [mock.call(self.new_bucket, self.new_remote_path,\n                                       self.new_dep_path)]\n    expected_copy_calls = []\n    expected_delete_calls = []\n\n    self.assertTrue(config.ExecuteUpdateJobs())\n    self.assertFalse(config._IsDirty())\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(self.new_dependencies, config._config_data)\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    expected_file_lines = list(self.new_expected_file_lines)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(expected_insert_calls,\n                     uploader_cs_mock.Insert.call_args_list)\n    self.assertEqual(expected_exists_calls,\n                     uploader_cs_mock.Exists.call_args_list)\n    self.assertEqual(expected_copy_calls,\n                     uploader_cs_mock.Copy.call_args_list)\n    self.assertEqual(expected_delete_calls,\n                     uploader_cs_mock.Delete.call_args_list)", "label": 1}
{"function": "  def testExecuteUpdateJobsSuccessOnePendingDepCloudStorageCollision(\n      self, uploader_cs_mock):\n    uploader_cs_mock.Exists.return_value = True\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    config._config_data = self.new_dependencies.copy()\n    config._pending_uploads = [self.new_pending_upload]\n    self.assertEqual(self.new_dependencies, config._config_data)\n    self.assertTrue(config._IsDirty())\n    self.assertEqual(1, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    expected_exists_calls = [mock.call(self.new_bucket, self.new_remote_path)]\n    expected_insert_calls = [mock.call(self.new_bucket, self.new_remote_path,\n                                       self.new_dep_path)]\n    expected_copy_calls = [mock.call(self.new_bucket, self.new_bucket,\n                                     self.new_remote_path,\n                                     self.expected_new_backup_path)]\n\n    self.assertTrue(config.ExecuteUpdateJobs(force=True))\n    self.assertFalse(config._IsDirty())\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(self.new_dependencies, config._config_data)\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    expected_file_lines = list(self.new_expected_file_lines)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(expected_insert_calls,\n                     uploader_cs_mock.Insert.call_args_list)\n    self.assertEqual(expected_exists_calls,\n                     uploader_cs_mock.Exists.call_args_list)\n    self.assertEqual(expected_copy_calls,\n                     uploader_cs_mock.Copy.call_args_list)", "label": 1}
{"function": "  def testExecuteUpdateJobsErrorOnePendingDepCloudStorageCollisionNoForce(\n      self, uploader_cs_mock):\n    uploader_cs_mock.Exists.return_value = True\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    config._config_data = self.new_dependencies.copy()\n    config._is_dirty = True\n    config._pending_uploads = [self.new_pending_upload]\n    self.assertEqual(self.new_dependencies, config._config_data)\n    self.assertTrue(config._is_dirty)\n    self.assertEqual(1, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    expected_exists_calls = [mock.call(self.new_bucket, self.new_remote_path)]\n    expected_insert_calls = []\n    expected_copy_calls = []\n\n    self.assertRaises(dependency_manager.CloudStorageUploadConflictError,\n                      config.ExecuteUpdateJobs)\n    self.assertTrue(config._is_dirty)\n    self.assertTrue(config._pending_uploads)\n    self.assertEqual(self.new_dependencies, config._config_data)\n    self.assertEqual(1, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    expected_file_lines = list(self.expected_file_lines)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))\n    self.assertEqual(expected_insert_calls,\n                     uploader_cs_mock.Insert.call_args_list)\n    self.assertEqual(expected_exists_calls,\n                     uploader_cs_mock.Exists.call_args_list)\n    self.assertEqual(expected_copy_calls,\n                     uploader_cs_mock.Copy.call_args_list)", "label": 1}
{"function": "  def testExecuteUpdateJobsSuccessMultiplePendingDepsOneCloudStorageCollision(\n      self, uploader_cs_mock):\n    uploader_cs_mock.Exists.side_effect = [False, True]\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    config._config_data = self.final_dependencies.copy()\n    config._pending_uploads = [self.new_pending_upload,\n                               self.final_pending_upload]\n    self.assertEqual(self.final_dependencies, config._config_data)\n    self.assertTrue(config._IsDirty())\n    self.assertEqual(2, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    self.assertEqual(self.final_pending_upload, config._pending_uploads[1])\n\n    expected_exists_calls = [mock.call(self.new_bucket, self.new_remote_path),\n                             mock.call(self.final_bucket,\n                                       self.final_remote_path)]\n    expected_insert_calls = [mock.call(self.new_bucket, self.new_remote_path,\n                                       self.new_dep_path),\n                             mock.call(self.final_bucket,\n                                       self.final_remote_path,\n                                       self.final_dep_path)]\n    expected_copy_calls = [mock.call(self.final_bucket, self.final_bucket,\n                                     self.final_remote_path,\n                                     self.expected_final_backup_path)]\n\n    self.assertTrue(config.ExecuteUpdateJobs(force=True))\n    self.assertFalse(config._IsDirty())\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(self.final_dependencies, config._config_data)\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    expected_file_lines = list(self.final_expected_file_lines)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(expected_insert_calls,\n                     uploader_cs_mock.Insert.call_args_list)\n    self.assertEqual(expected_exists_calls,\n                     uploader_cs_mock.Exists.call_args_list)\n    self.assertEqual(expected_copy_calls,\n                     uploader_cs_mock.Copy.call_args_list)", "label": 1}
{"function": "  def testUpdateCloudStorageDependenciesReadOnlyConfig(\n      self, uploader_cs_mock):\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path)\n    with self.assertRaises(dependency_manager.ReadWriteError):\n      config.AddCloudStorageDependencyUpdateJob(\n          'dep', 'plat', 'path')\n    with self.assertRaises(dependency_manager.ReadWriteError):\n      config.AddCloudStorageDependencyUpdateJob(\n          'dep', 'plat', 'path', version='1.2.3')\n    with self.assertRaises(dependency_manager.ReadWriteError):\n      config.AddCloudStorageDependencyUpdateJob(\n          'dep', 'plat', 'path', execute_job=False)\n    with self.assertRaises(dependency_manager.ReadWriteError):\n      config.AddCloudStorageDependencyUpdateJob(\n          'dep', 'plat', 'path', version='1.2.3', execute_job=False)", "label": 1}
{"function": "  def testUpdateCloudStorageDependenciesMissingDependency(\n      self, uploader_cs_mock):\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    self.assertRaises(ValueError, config.AddCloudStorageDependencyUpdateJob,\n                      'dep', 'plat', 'path')\n    self.assertRaises(ValueError, config.AddCloudStorageDependencyUpdateJob,\n                      'dep', 'plat', 'path', version='1.2.3')\n    self.assertRaises(ValueError, config.AddCloudStorageDependencyUpdateJob,\n                      'dep', 'plat', 'path', execute_job=False)\n    self.assertRaises(ValueError, config.AddCloudStorageDependencyUpdateJob,\n                      'dep', 'plat', 'path', version='1.2.3', execute_job=False)", "label": 1}
{"function": "  def testUpdateCloudStorageDependenciesWrite(\n      self, base_config_cs_mock, uploader_cs_mock):\n    expected_dependencies = self.dependencies\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    self.assertFalse(config._IsDirty())\n    self.assertEqual(expected_dependencies, config._config_data)\n\n    base_config_cs_mock.CalculateHash.return_value = self.new_dep_hash\n    uploader_cs_mock.Exists.return_value = False\n    expected_dependencies = self.new_dependencies\n    config.AddCloudStorageDependencyUpdateJob(\n        'dep1', 'plat2', self.new_dep_path, execute_job=True)\n    self.assertFalse(config._IsDirty())\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(expected_dependencies, config._config_data)\n    # check that file contents has been updated\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    expected_file_lines = list(self.new_expected_file_lines)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))\n\n    expected_dependencies = self.final_dependencies\n    base_config_cs_mock.CalculateHash.return_value = self.final_dep_hash\n    config.AddCloudStorageDependencyUpdateJob(\n        'dep2', 'plat1', self.final_dep_path, execute_job=True)\n    self.assertFalse(config._IsDirty())\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(expected_dependencies, config._config_data)\n    # check that file contents has been updated\n    expected_file_lines = list(self.final_expected_file_lines)\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))", "label": 1}
{"function": "  def testUpdateCloudStorageDependenciesNoWrite(\n      self, base_config_cs_mock, uploader_cs_mock):\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n\n    self.assertRaises(ValueError, config.AddCloudStorageDependencyUpdateJob,\n                      'dep', 'plat', 'path')\n    self.assertRaises(ValueError, config.AddCloudStorageDependencyUpdateJob,\n                      'dep', 'plat', 'path', version='1.2.3')\n\n    expected_dependencies = self.dependencies\n    config = dependency_manager.BaseConfig(self.file_path, writable=True)\n    self.assertFalse(config._IsDirty())\n    self.assertFalse(config._pending_uploads)\n    self.assertEqual(expected_dependencies, config._config_data)\n\n    base_config_cs_mock.CalculateHash.return_value = self.new_dep_hash\n    uploader_cs_mock.Exists.return_value = False\n    expected_dependencies = self.new_dependencies\n    config.AddCloudStorageDependencyUpdateJob(\n        'dep1', 'plat2', self.new_dep_path, execute_job=False)\n    self.assertTrue(config._IsDirty())\n    self.assertEqual(1, len(config._pending_uploads))\n    self.assertEqual(self.new_pending_upload, config._pending_uploads[0])\n    self.assertEqual(expected_dependencies, config._config_data)\n    # check that file contents have not been updated.\n    expected_file_lines = list(self.expected_file_lines)\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))\n\n    expected_dependencies = self.final_dependencies\n    base_config_cs_mock.CalculateHash.return_value = self.final_dep_hash\n    config.AddCloudStorageDependencyUpdateJob(\n        'dep2', 'plat1', self.final_dep_path, execute_job=False)\n    self.assertTrue(config._IsDirty())\n    self.assertEqual(expected_dependencies, config._config_data)\n    # check that file contents have not been updated.\n    expected_file_lines = list(self.expected_file_lines)\n    file_module = fake_filesystem.FakeFileOpen(self.fs)\n    for line in file_module(self.file_path):\n      self.assertEqual(expected_file_lines.pop(0), line.strip())\n    self.fs.CloseOpenFile(file_module(self.file_path))", "label": 1}
{"function": "  def setUp(self):\n    self.addTypeEqualityFunc(uploader.CloudStorageUploader,\n                             uploader.CloudStorageUploader.__eq__)\n    self.setUpPyfakefs()\n\n    self.cs_bucket = 'bucket1'\n    self.cs_base_folder = 'dependencies_folder'\n    self.cs_hash = 'hash12'\n    self.download_path = '../../relative/dep1/path2'\n    self.local_paths = ['../../../relative/local/path21',\n                        '../../../relative/local/path22']\n    self.platform_dict = {'cloud_storage_hash': self.cs_hash,\n                          'download_path': self.download_path,\n                          'local_paths': self.local_paths}\n    self.dependencies = {\n        'dep1': {\n            'cloud_storage_bucket': self.cs_bucket,\n            'cloud_storage_base_folder': self.cs_base_folder,\n            'file_info': {\n                'plat1': {\n                    'cloud_storage_hash': 'hash11',\n                    'download_path': '../../relative/dep1/path1',\n                    'local_paths': ['../../../relative/local/path11',\n                                    '../../../relative/local/path12']},\n                'plat2': self.platform_dict\n            }\n        },\n        'dep2': {\n            'cloud_storage_bucket': 'bucket2',\n            'file_info': {\n                'plat1': {\n                    'cloud_storage_hash': 'hash21',\n                    'download_path': '../../relative/dep2/path1',\n                    'local_paths': ['../../../relative/local/path31',\n                                    '../../../relative/local/path32']},\n                'plat2': {\n                    'cloud_storage_hash': 'hash22',\n                    'download_path': '../../relative/dep2/path2'}}}}\n\n    self.file_path = os.path.abspath(os.path.join(\n        'path', 'to', 'config', 'file'))\n\n\n    self.expected_file_lines = [\n      # pylint: disable=bad-continuation\n      '{', '\"config_type\": \"BaseConfig\",', '\"dependencies\": {',\n        '\"dep1\": {', '\"cloud_storage_base_folder\": \"dependencies_folder\",',\n          '\"cloud_storage_bucket\": \"bucket1\",', '\"file_info\": {',\n            '\"plat1\": {', '\"cloud_storage_hash\": \"hash11\",',\n              '\"download_path\": \"../../relative/dep1/path1\",',\n              '\"local_paths\": [', '\"../../../relative/local/path11\",',\n                              '\"../../../relative/local/path12\"', ']', '},',\n            '\"plat2\": {', '\"cloud_storage_hash\": \"hash12\",',\n              '\"download_path\": \"../../relative/dep1/path2\",',\n              '\"local_paths\": [', '\"../../../relative/local/path21\",',\n                              '\"../../../relative/local/path22\"', ']',\n              '}', '}', '},',\n        '\"dep2\": {', '\"cloud_storage_bucket\": \"bucket2\",', '\"file_info\": {',\n            '\"plat1\": {', '\"cloud_storage_hash\": \"hash21\",',\n              '\"download_path\": \"../../relative/dep2/path1\",',\n              '\"local_paths\": [', '\"../../../relative/local/path31\",',\n                              '\"../../../relative/local/path32\"', ']', '},',\n            '\"plat2\": {', '\"cloud_storage_hash\": \"hash22\",',\n              '\"download_path\": \"../../relative/dep2/path2\"', '}', '}', '}',\n      '}', '}']\n    self.fs.CreateFile(self.file_path,\n                       contents='\\n'.join(self.expected_file_lines))", "label": 1}
{"function": "def _GatherGrad(op, grad):\n  \"\"\"Gradient for Gather op.\"\"\"\n  if op.inputs[0].get_shape().is_fully_defined():\n    dense_shape = constant_op.constant(op.inputs[0].get_shape().as_list())\n    values_shape = [-1] + op.inputs[0].get_shape()[1:].as_list()\n  else:\n    # op.inputs[0] can be large, so colocate the shape calculation with it.\n    with ops.colocate_with(op.inputs[0]):\n      dense_shape = array_ops.shape(op.inputs[0])\n      values_shape = array_ops.concat(0, [[-1], dense_shape[1:]])\n\n  values = array_ops.reshape(grad, values_shape)\n  indices = array_ops.reshape(op.inputs[1], [-1])\n  return [ops.IndexedSlices(values, indices, dense_shape), None]", "label": 1}
{"function": "def _GatherNdGrad(unused_op, unused_grad):\n  raise NotImplementedError(\"Gradient for gather_nd is not implemented.\")", "label": 1}
{"function": "def _CheckNumericsGrad(_, grad):\n  \"\"\"Gradient for check_numerics op.\"\"\"\n  return grad", "label": 1}
{"function": "def _IdGrad(_, grad):\n  return grad", "label": 1}
{"function": "def _RefIdGrad(_, grad):\n  return grad", "label": 1}
{"function": "def _ReshapeGrad(op, grad):\n  return [array_ops.reshape(grad, array_ops.shape(op.inputs[0])), None]", "label": 1}
{"function": "def _ReshapeToInput(op, grad):\n  \"\"\"Reshapes the gradient to the shape of the original input.\"\"\"\n  return array_ops.reshape(grad, array_ops.shape(op.inputs[0]))", "label": 1}
{"function": "def _ExpandDimsGrad(op, grad):\n  return [_ReshapeToInput(op, grad), None]", "label": 1}
{"function": "def _SqueezeGrad(op, grad):\n  return _ReshapeToInput(op, grad)", "label": 1}
{"function": "def _TransposeGrad(op, grad):\n  \"\"\"Returns unshuffle(grad).\"\"\"\n  p = op.inputs[1]\n  return [array_ops.transpose(grad, array_ops.invert_permutation(p)), None]", "label": 1}
{"function": "def resize_img():\n\tdirs = os.listdir(\"split_pic//6\")\n\tfor filename in dirs:\n\t\tim = tf.gfile.FastGFile(\"split_pic//6//{}\".format(filename), 'rb').read()\n\t\t# print(\"\u6b63\u5728\u5904\u7406\u7b2c%d\u5f20\u7167\u7247\"%counter)\n\t\twith tf.Session() as sess:\n\t\t\timg_data = tf.image.decode_jpeg(im)\n\t\t\timage_float = tf.image.convert_image_dtype(img_data, tf.float32)\n\t\t\tresized = tf.image.resize_images(image_float, [64, 64], method=3)\n\t\t\tresized_im = resized.eval()\n\t\t\t# new_mat = np.asarray(resized_im).reshape(1, 64, 64, 3)\n\t\t\tscipy.misc.imsave(\"resized_img6//{}\".format(filename),resized_im)", "label": 1}
{"function": "def image_to_h5():\n\tdirs = os.listdir(\"resized_img\")\n\tY = [] #label\n\tX = [] #data\n\tprint(len(dirs))\n\tfor filename in dirs:\n\t\tlabel = int(filename.split('_')[0])\n\t\tY.append(label)\n\t\tim = Image.open(\"resized_img//{}\".format(filename)).convert('RGB')\n\t\tmat = np.asarray(im) #image \u8f6c\u77e9\u9635\n\t\tX.append(mat)\n\n\tfile = h5py.File(\"dataset//data.h5\",\"w\")\n\tfile.create_dataset('X', data=np.array(X))\n\tfile.create_dataset('Y', data=np.array(Y))\n\tfile.close()", "label": 1}
{"function": "    def __init__(\n            self,\n            env,\n            inner_policy,\n            qf,\n            vf,\n            *,  # Mostly numbers after here.\n            num_train_tasks,\n            num_test_tasks=None,\n            latent_dim,\n            encoder_hidden_sizes,\n            test_env_sampler,\n            policy_class=ContextConditionedPolicy,\n            encoder_class=MLPEncoder,\n            policy_lr=3E-4,\n            qf_lr=3E-4,\n            vf_lr=3E-4,\n            context_lr=3E-4,\n            policy_mean_reg_coeff=1E-3,\n            policy_std_reg_coeff=1E-3,\n            policy_pre_activation_coeff=0.,\n            soft_target_tau=0.005,\n            kl_lambda=.1,\n            optimizer_class=torch.optim.Adam,\n            use_information_bottleneck=True,\n            use_next_obs_in_context=False,\n            meta_batch_size=64,\n            num_steps_per_epoch=1000,\n            num_initial_steps=100,\n            num_tasks_sample=100,\n            num_steps_prior=100,\n            num_steps_posterior=0,\n            num_extra_rl_steps_posterior=100,\n            batch_size=1024,\n            embedding_batch_size=1024,\n            embedding_mini_batch_size=1024,\n            discount=0.99,\n            replay_buffer_size=1000000,\n            reward_scale=1,\n            update_post_train=1):\n\n        self._env = env\n        self._qf1 = qf\n        self._qf2 = copy.deepcopy(qf)\n        self._vf = vf\n        self._num_train_tasks = num_train_tasks\n        self._latent_dim = latent_dim\n\n        self._policy_mean_reg_coeff = policy_mean_reg_coeff\n        self._policy_std_reg_coeff = policy_std_reg_coeff\n        self._policy_pre_activation_coeff = policy_pre_activation_coeff\n        self._soft_target_tau = soft_target_tau\n        self._kl_lambda = kl_lambda\n        self._use_information_bottleneck = use_information_bottleneck\n        self._use_next_obs_in_context = use_next_obs_in_context\n\n        self._meta_batch_size = meta_batch_size\n        self._num_steps_per_epoch = num_steps_per_epoch\n        self._num_initial_steps = num_initial_steps\n        self._num_tasks_sample = num_tasks_sample\n        self._num_steps_prior = num_steps_prior\n        self._num_steps_posterior = num_steps_posterior\n        self._num_extra_rl_steps_posterior = num_extra_rl_steps_posterior\n        self._batch_size = batch_size\n        self._embedding_batch_size = embedding_batch_size\n        self._embedding_mini_batch_size = embedding_mini_batch_size\n        self._discount = discount\n        self._replay_buffer_size = replay_buffer_size\n        self._reward_scale = reward_scale\n        self._update_post_train = update_post_train\n        self._task_idx = None\n        self._single_env = env[0]()\n        self.max_episode_length = self._single_env.spec.max_episode_length\n\n        self._is_resuming = False\n\n        if num_test_tasks is None:\n            num_test_tasks = test_env_sampler.n_tasks\n        if num_test_tasks is None:\n            raise ValueError('num_test_tasks must be provided if '\n                             'test_env_sampler.n_tasks is None')\n\n        worker_args = dict(deterministic=True, accum_context=True)\n        self._evaluator = MetaEvaluator(test_task_sampler=test_env_sampler,\n                                        worker_class=PEARLWorker,\n                                        worker_args=worker_args,\n                                        n_test_tasks=num_test_tasks)\n\n        encoder_spec = self.get_env_spec(self._single_env, latent_dim,\n                                         'encoder')\n        encoder_in_dim = int(np.prod(encoder_spec.input_space.shape))\n        encoder_out_dim = int(np.prod(encoder_spec.output_space.shape))\n        context_encoder = encoder_class(input_dim=encoder_in_dim,\n                                        output_dim=encoder_out_dim,\n                                        hidden_sizes=encoder_hidden_sizes)\n\n        self._policy = policy_class(\n            latent_dim=latent_dim,\n            context_encoder=context_encoder,\n            policy=inner_policy,\n            use_information_bottleneck=use_information_bottleneck,\n            use_next_obs=use_next_obs_in_context)\n\n        # buffer for training RL update\n        self._replay_buffers = {\n            i: PathBuffer(replay_buffer_size)\n            for i in range(num_train_tasks)\n        }\n\n        self._context_replay_buffers = {\n            i: PathBuffer(replay_buffer_size)\n            for i in range(num_train_tasks)\n        }\n\n        self.target_vf = copy.deepcopy(self._vf)\n        self.vf_criterion = torch.nn.MSELoss()\n\n        self._policy_optimizer = optimizer_class(\n            self._policy.networks[1].parameters(),\n            lr=policy_lr,\n        )\n        self.qf1_optimizer = optimizer_class(\n            self._qf1.parameters(),\n            lr=qf_lr,\n        )\n        self.qf2_optimizer = optimizer_class(\n            self._qf2.parameters(),\n            lr=qf_lr,\n        )\n        self.vf_optimizer = optimizer_class(\n            self._vf.parameters(),\n            lr=vf_lr,\n        )\n        self.context_optimizer = optimizer_class(\n            self._policy.networks[0].parameters(),\n            lr=context_lr,\n        )", "label": 1}
{"function": "    def __getstate__(self):\n        \"\"\"Object.__getstate__.\n\n        Returns:\n            dict: the state to be pickled for the instance.\n\n        \"\"\"\n        data = self.__dict__.copy()\n        del data['_replay_buffers']\n        del data['_context_replay_buffers']\n        return data", "label": 1}
{"function": "    def __setstate__(self, state):\n        \"\"\"Object.__setstate__.\n\n        Args:\n            state (dict): unpickled state.\n\n        \"\"\"\n        self.__dict__.update(state)\n        self._replay_buffers = {\n            i: PathBuffer(self._replay_buffer_size)\n            for i in range(self._num_train_tasks)\n        }\n\n        self._context_replay_buffers = {\n            i: PathBuffer(self._replay_buffer_size)\n            for i in range(self._num_train_tasks)\n        }\n        self._is_resuming = True", "label": 1}
{"function": "    def train(self, trainer):\n        \"\"\"Obtain samples, train, and evaluate for each epoch.\n\n        Args:\n            trainer (Trainer): Gives the algorithm the access to\n                :method:`Trainer..step_epochs()`, which provides services\n                such as snapshotting and sampler control.\n\n        \"\"\"\n        for _ in trainer.step_epochs():\n            epoch = trainer.step_itr / self._num_steps_per_epoch\n\n            # obtain initial set of samples from all train tasks\n            if epoch == 0 or self._is_resuming:\n                for idx in range(self._num_train_tasks):\n                    self._task_idx = idx\n                    self._obtain_samples(trainer, epoch,\n                                         self._num_initial_steps, np.inf)\n                    self._is_resuming = False\n\n            # obtain samples from random tasks\n            for _ in range(self._num_tasks_sample):\n                idx = np.random.randint(self._num_train_tasks)\n                self._task_idx = idx\n                self._context_replay_buffers[idx].clear()\n                # obtain samples with z ~ prior\n                if self._num_steps_prior > 0:\n                    self._obtain_samples(trainer, epoch, self._num_steps_prior,\n                                         np.inf)\n                # obtain samples with z ~ posterior\n                if self._num_steps_posterior > 0:\n                    self._obtain_samples(trainer, epoch,\n                                         self._num_steps_posterior,\n                                         self._update_post_train)\n                # obtain extras samples for RL training but not encoder\n                if self._num_extra_rl_steps_posterior > 0:\n                    self._obtain_samples(trainer,\n                                         epoch,\n                                         self._num_extra_rl_steps_posterior,\n                                         self._update_post_train,\n                                         add_to_enc_buffer=False)\n\n            logger.log('Training...')\n            # sample train tasks and optimize networks\n            self._train_once()\n            trainer.step_itr += 1\n\n            logger.log('Evaluating...')\n            # evaluate\n            self._policy.reset_belief()\n            self._evaluator.evaluate(self)", "label": 1}
{"function": "    def _train_once(self):\n        \"\"\"Perform one iteration of training.\"\"\"\n        for _ in range(self._num_steps_per_epoch):\n            indices = np.random.choice(range(self._num_train_tasks),\n                                       self._meta_batch_size)\n            self._optimize_policy(indices)", "label": 1}
{"function": "    def _optimize_policy(self, indices):\n        \"\"\"Perform algorithm optimizing.\n\n        Args:\n            indices (list): Tasks used for training.\n\n        \"\"\"\n        num_tasks = len(indices)\n        context = self._sample_context(indices)\n        # clear context and reset belief of policy\n        self._policy.reset_belief(num_tasks=num_tasks)\n\n        # data shape is (task, batch, feat)\n        obs, actions, rewards, next_obs, terms = self._sample_data(indices)\n        policy_outputs, task_z = self._policy(obs, context)\n        new_actions, policy_mean, policy_log_std, log_pi = policy_outputs[:4]\n\n        # flatten out the task dimension\n        t, b, _ = obs.size()\n        obs = obs.view(t * b, -1)\n        actions = actions.view(t * b, -1)\n        next_obs = next_obs.view(t * b, -1)\n\n        # optimize qf and encoder networks\n        q1_pred = self._qf1(torch.cat([obs, actions], dim=1), task_z)\n        q2_pred = self._qf2(torch.cat([obs, actions], dim=1), task_z)\n        v_pred = self._vf(obs, task_z.detach())\n\n        with torch.no_grad():\n            target_v_values = self.target_vf(next_obs, task_z)\n\n        # KL constraint on z if probabilistic\n        self.context_optimizer.zero_grad()\n        if self._use_information_bottleneck:\n            kl_div = self._policy.compute_kl_div()\n            kl_loss = self._kl_lambda * kl_div\n            kl_loss.backward(retain_graph=True)\n\n        self.qf1_optimizer.zero_grad()\n        self.qf2_optimizer.zero_grad()\n\n        rewards_flat = rewards.view(self._batch_size * num_tasks, -1)\n        rewards_flat = rewards_flat * self._reward_scale\n        terms_flat = terms.view(self._batch_size * num_tasks, -1)\n        q_target = rewards_flat + (\n            1. - terms_flat) * self._discount * target_v_values\n        qf_loss = torch.mean((q1_pred - q_target)**2) + torch.mean(\n            (q2_pred - q_target)**2)\n        qf_loss.backward()\n\n        self.qf1_optimizer.step()\n        self.qf2_optimizer.step()\n        self.context_optimizer.step()\n\n        # compute min Q on the new actions\n        q1 = self._qf1(torch.cat([obs, new_actions], dim=1), task_z.detach())\n        q2 = self._qf2(torch.cat([obs, new_actions], dim=1), task_z.detach())\n        min_q = torch.min(q1, q2)\n\n        # optimize vf\n        v_target = min_q - log_pi\n        vf_loss = self.vf_criterion(v_pred, v_target.detach())\n        self.vf_optimizer.zero_grad()\n        vf_loss.backward()\n        self.vf_optimizer.step()\n        self._update_target_network()\n\n        # optimize policy\n        log_policy_target = min_q\n        policy_loss = (log_pi - log_policy_target).mean()\n\n        mean_reg_loss = self._policy_mean_reg_coeff * (policy_mean**2).mean()\n        std_reg_loss = self._policy_std_reg_coeff * (policy_log_std**2).mean()\n        pre_tanh_value = policy_outputs[-1]\n        pre_activation_reg_loss = self._policy_pre_activation_coeff * (\n            (pre_tanh_value**2).sum(dim=1).mean())\n        policy_reg_loss = (mean_reg_loss + std_reg_loss +\n                           pre_activation_reg_loss)\n        policy_loss = policy_loss + policy_reg_loss\n\n        self._policy_optimizer.zero_grad()\n        policy_loss.backward()\n        self._policy_optimizer.step()", "label": 1}
{"function": "    def _obtain_samples(self,\n                        trainer,\n                        itr,\n                        num_samples,\n                        update_posterior_rate,\n                        add_to_enc_buffer=True):\n        \"\"\"Obtain samples.\n\n        Args:\n            trainer (Trainer): Trainer.\n            itr (int): Index of iteration (epoch).\n            num_samples (int): Number of samples to obtain.\n            update_posterior_rate (int): How often (in episodes) to infer\n                posterior of policy.\n            add_to_enc_buffer (bool): Whether or not to add samples to encoder\n                buffer.\n\n        \"\"\"\n        self._policy.reset_belief()\n        total_samples = 0\n\n        if update_posterior_rate != np.inf:\n            num_samples_per_batch = (update_posterior_rate *\n                                     self.max_episode_length)\n        else:\n            num_samples_per_batch = num_samples\n\n        while total_samples < num_samples:\n            paths = trainer.obtain_samples(itr, num_samples_per_batch,\n                                           self._policy,\n                                           self._env[self._task_idx])\n            total_samples += sum([len(path['rewards']) for path in paths])\n\n            for path in paths:\n                p = {\n                    'observations':\n                    path['observations'],\n                    'actions':\n                    path['actions'],\n                    'rewards':\n                    path['rewards'].reshape(-1, 1),\n                    'next_observations':\n                    path['next_observations'],\n                    'dones':\n                    np.array([\n                        step_type == StepType.TERMINAL\n                        for step_type in path['step_types']\n                    ]).reshape(-1, 1)\n                }\n                self._replay_buffers[self._task_idx].add_path(p)\n\n                if add_to_enc_buffer:\n                    self._context_replay_buffers[self._task_idx].add_path(p)\n\n            if update_posterior_rate != np.inf:\n                context = self._sample_context(self._task_idx)\n                self._policy.infer_posterior(context)", "label": 1}
{"function": "    def _sample_data(self, indices):\n        \"\"\"Sample batch of training data from a list of tasks.\n\n        Args:\n            indices (list): List of task indices to sample from.\n\n        Returns:\n            torch.Tensor: Obervations, with shape :math:`(X, N, O^*)` where X\n                is the number of tasks. N is batch size.\n            torch.Tensor: Actions, with shape :math:`(X, N, A^*)`.\n            torch.Tensor: Rewards, with shape :math:`(X, N, 1)`.\n            torch.Tensor: Next obervations, with shape :math:`(X, N, O^*)`.\n            torch.Tensor: Dones, with shape :math:`(X, N, 1)`.\n\n        \"\"\"\n        # transitions sampled randomly from replay buffer\n        initialized = False\n        for idx in indices:\n            batch = self._replay_buffers[idx].sample_transitions(\n                self._batch_size)\n            if not initialized:\n                o = batch['observations'][np.newaxis]\n                a = batch['actions'][np.newaxis]\n                r = batch['rewards'][np.newaxis]\n                no = batch['next_observations'][np.newaxis]\n                d = batch['dones'][np.newaxis]\n                initialized = True\n            else:\n                o = np.vstack((o, batch['observations'][np.newaxis]))\n                a = np.vstack((a, batch['actions'][np.newaxis]))\n                r = np.vstack((r, batch['rewards'][np.newaxis]))\n                no = np.vstack((no, batch['next_observations'][np.newaxis]))\n                d = np.vstack((d, batch['dones'][np.newaxis]))\n\n        o = torch.as_tensor(o, device=global_device()).float()\n        a = torch.as_tensor(a, device=global_device()).float()\n        r = torch.as_tensor(r, device=global_device()).float()\n        no = torch.as_tensor(no, device=global_device()).float()\n        d = torch.as_tensor(d, device=global_device()).float()\n\n        return o, a, r, no, d", "label": 1}
{"function": "    def execute(self, context):\n        MUV_OT_MoveUV.__running = True\n        self.__operating = False\n        self.__first_time = True\n\n        context.window_manager.modal_handler_add(self)\n        self.__topology_dict, self.__ini_uvs = _find_uv(context)\n\n        if context.area:\n            context.area.tag_redraw()\n\n        return {'RUNNING_MODAL'}", "label": 1}
{"function": "    def __init__(self, sm):\n        self._mModel = sm\n        self._mCurrentBytePos = 0\n        self._mCurrentCharLen = 0\n        self.reset()", "label": 1}
{"function": "    def reset(self):\n        self._mCurrentState = eStart", "label": 1}
{"function": "    def next_state(self, c):\n        # for each byte we get its class\n        # if it is first byte, we also get byte length\n        # PY3K: aBuf is a byte stream, so c is an int, not a byte\n        byteCls = self._mModel['classTable'][wrap_ord(c)]\n        if self._mCurrentState == eStart:\n            self._mCurrentBytePos = 0\n            self._mCurrentCharLen = self._mModel['charLenTable'][byteCls]\n        # from byte's class and stateTable, we get its next state\n        curr_state = (self._mCurrentState * self._mModel['classFactor']\n                      + byteCls)\n        self._mCurrentState = self._mModel['stateTable'][curr_state]\n        self._mCurrentBytePos += 1\n        return self._mCurrentState", "label": 1}
{"function": "    def get_current_charlen(self):\n        return self._mCurrentCharLen", "label": 1}
{"function": "    def get_coding_state_machine(self):\n        return self._mModel['name']", "label": 1}
{"function": "    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        init.xavier_uniform_(self.weight)\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n            init.constant_(self.bias, 0)\n        else:\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()", "label": 1}
{"function": "    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)", "label": 1}
{"function": "    def forward(self, input, adj):\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output", "label": 1}
{"function": "    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'", "label": 1}
{"function": "    def test_home_installation_scheme(self):\n        # This ensure two things:\n        # - that --home generates the desired set of directory names\n        # - test --home is supported on all platforms\n        builddir = self.mkdtemp()\n        destination = os.path.join(builddir, \"installation\")\n\n        dist = Distribution({\"name\": \"foopkg\"})\n        # script_name need not exist, it just need to be initialized\n        dist.script_name = os.path.join(builddir, \"setup.py\")\n        dist.command_obj[\"build\"] = support.DummyCommand(\n            build_base=builddir,\n            build_lib=os.path.join(builddir, \"lib\"),\n            )\n\n        cmd = install(dist)\n        cmd.home = destination\n        cmd.ensure_finalized()\n\n        self.assertEqual(cmd.install_base, destination)\n        self.assertEqual(cmd.install_platbase, destination)\n\n        def check_path(got, expected):\n            got = os.path.normpath(got)\n            expected = os.path.normpath(expected)\n            self.assertEqual(got, expected)\n\n        libdir = os.path.join(destination, \"lib\", \"python\")\n        check_path(cmd.install_lib, libdir)\n        check_path(cmd.install_platlib, libdir)\n        check_path(cmd.install_purelib, libdir)\n        check_path(cmd.install_headers,\n                   os.path.join(destination, \"include\", \"python\", \"foopkg\"))\n        check_path(cmd.install_scripts, os.path.join(destination, \"bin\"))\n        check_path(cmd.install_data, destination)", "label": 1}
{"function": "    def test_user_site(self):\n        # site.USER_SITE was introduced in 2.6\n        if sys.version < '2.6':\n            return\n\n        # preparing the environment for the test\n        self.old_user_base = site.USER_BASE\n        self.old_user_site = site.USER_SITE\n        self.tmpdir = self.mkdtemp()\n        self.user_base = os.path.join(self.tmpdir, 'B')\n        self.user_site = os.path.join(self.tmpdir, 'S')\n        site.USER_BASE = self.user_base\n        site.USER_SITE = self.user_site\n        install_module.USER_BASE = self.user_base\n        install_module.USER_SITE = self.user_site\n\n        def _expanduser(path):\n            return self.tmpdir\n        self.old_expand = os.path.expanduser\n        os.path.expanduser = _expanduser\n\n        def cleanup():\n            site.USER_BASE = self.old_user_base\n            site.USER_SITE = self.old_user_site\n            install_module.USER_BASE = self.old_user_base\n            install_module.USER_SITE = self.old_user_site\n            os.path.expanduser = self.old_expand\n\n        self.addCleanup(cleanup)\n\n        for key in ('nt_user', 'unix_user', 'os2_home'):\n            self.assertIn(key, INSTALL_SCHEMES)\n\n        dist = Distribution({'name': 'xx'})\n        cmd = install(dist)\n\n        # making sure the user option is there\n        options = [name for name, short, lable in\n                   cmd.user_options]\n        self.assertIn('user', options)\n\n        # setting a value\n        cmd.user = 1\n\n        # user base and site shouldn't be created yet\n        self.assertFalse(os.path.exists(self.user_base))\n        self.assertFalse(os.path.exists(self.user_site))\n\n        # let's run finalize\n        cmd.ensure_finalized()\n\n        # now they should\n        self.assertTrue(os.path.exists(self.user_base))\n        self.assertTrue(os.path.exists(self.user_site))\n\n        self.assertIn('userbase', cmd.config_vars)\n        self.assertIn('usersite', cmd.config_vars)", "label": 1}
{"function": "    def test_handle_extra_path(self):\n        dist = Distribution({'name': 'xx', 'extra_path': 'path,dirs'})\n        cmd = install(dist)\n\n        # two elements\n        cmd.handle_extra_path()\n        self.assertEqual(cmd.extra_path, ['path', 'dirs'])\n        self.assertEqual(cmd.extra_dirs, 'dirs')\n        self.assertEqual(cmd.path_file, 'path')\n\n        # one element\n        cmd.extra_path = ['path']\n        cmd.handle_extra_path()\n        self.assertEqual(cmd.extra_path, ['path'])\n        self.assertEqual(cmd.extra_dirs, 'path')\n        self.assertEqual(cmd.path_file, 'path')\n\n        # none\n        dist.extra_path = cmd.extra_path = None\n        cmd.handle_extra_path()\n        self.assertEqual(cmd.extra_path, None)\n        self.assertEqual(cmd.extra_dirs, '')\n        self.assertEqual(cmd.path_file, None)\n\n        # three elements (no way !)\n        cmd.extra_path = 'path,dirs,again'\n        self.assertRaises(DistutilsOptionError, cmd.handle_extra_path)", "label": 1}
{"function": "    def test_finalize_options(self):\n        dist = Distribution({'name': 'xx'})\n        cmd = install(dist)\n\n        # must supply either prefix/exec-prefix/home or\n        # install-base/install-platbase -- not both\n        cmd.prefix = 'prefix'\n        cmd.install_base = 'base'\n        self.assertRaises(DistutilsOptionError, cmd.finalize_options)\n\n        # must supply either home or prefix/exec-prefix -- not both\n        cmd.install_base = None\n        cmd.home = 'home'\n        self.assertRaises(DistutilsOptionError, cmd.finalize_options)\n\n        # can't combine user with prefix/exec_prefix/home or\n        # install_(plat)base\n        cmd.prefix = None\n        cmd.user = 'user'\n        self.assertRaises(DistutilsOptionError, cmd.finalize_options)", "label": 1}
{"function": "    def test_record(self):\n        install_dir = self.mkdtemp()\n        project_dir, dist = self.create_dist(py_modules=['hello'],\n                                             scripts=['sayhi'])\n        os.chdir(project_dir)\n        self.write_file('hello.py', \"def main(): print 'o hai'\")\n        self.write_file('sayhi', 'from hello import main; main()')\n\n        cmd = install(dist)\n        dist.command_obj['install'] = cmd\n        cmd.root = install_dir\n        cmd.record = os.path.join(project_dir, 'filelist')\n        cmd.ensure_finalized()\n        cmd.run()\n\n        f = open(cmd.record)\n        try:\n            content = f.read()\n        finally:\n            f.close()\n\n        found = [os.path.basename(line) for line in content.splitlines()]\n        expected = ['hello.py', 'hello.pyc', 'sayhi',\n                    'UNKNOWN-0.0.0-py%s.%s.egg-info' % sys.version_info[:2]]\n        self.assertEqual(found, expected)", "label": 1}
{"function": "    def test_record_extensions(self):\n        install_dir = self.mkdtemp()\n        project_dir, dist = self.create_dist(ext_modules=[\n            Extension('xx', ['xxmodule.c'])])\n        os.chdir(project_dir)\n        support.copy_xxmodule_c(project_dir)\n\n        buildextcmd = build_ext(dist)\n        support.fixup_build_ext(buildextcmd)\n        buildextcmd.ensure_finalized()\n\n        cmd = install(dist)\n        dist.command_obj['install'] = cmd\n        dist.command_obj['build_ext'] = buildextcmd\n        cmd.root = install_dir\n        cmd.record = os.path.join(project_dir, 'filelist')\n        cmd.ensure_finalized()\n        cmd.run()\n\n        f = open(cmd.record)\n        try:\n            content = f.read()\n        finally:\n            f.close()\n\n        found = [os.path.basename(line) for line in content.splitlines()]\n        expected = [_make_ext_name('xx'),\n                    'UNKNOWN-0.0.0-py%s.%s.egg-info' % sys.version_info[:2]]\n        self.assertEqual(found, expected)", "label": 1}
{"function": "    def test_debug_mode(self):\n        # this covers the code called when DEBUG is set\n        old_logs_len = len(self.logs)\n        install_module.DEBUG = True\n        try:\n            with captured_stdout():\n                self.test_record()\n        finally:\n            install_module.DEBUG = False\n        self.assertTrue(len(self.logs) > old_logs_len)", "label": 1}
{"function": "        def check_path(got, expected):\n            got = os.path.normpath(got)\n            expected = os.path.normpath(expected)\n            self.assertEqual(got, expected)", "label": 1}
{"function": "        def _expanduser(path):\n            return self.tmpdir", "label": 1}
{"function": "        def cleanup():\n            site.USER_BASE = self.old_user_base\n            site.USER_SITE = self.old_user_site\n            install_module.USER_BASE = self.old_user_base\n            install_module.USER_SITE = self.old_user_site\n            os.path.expanduser = self.old_expand", "label": 1}
{"function": "def load_video_dir(root, dirs, save_dir, save_name):\n  videos, sparse_videos = [], []\n  first_videos = []\n  for idx, cdir in enumerate(dirs):\n    annot_path = osp.join(root, cdir, 'annot')\n    frame_path = osp.join(root, cdir, 'extraction')\n    all_frames = glob.glob( osp.join(frame_path, '*.png') )\n    all_annots = glob.glob( osp.join(annot_path, '*.pts') )\n    assert len(all_frames) == len(all_annots), 'The length is not right for {} : {} vs {}'.format(cdir, len(all_frames), len(all_annots))\n    all_frames = sorted(all_frames)\n    all_annots = sorted(all_annots)\n    current_video = []\n    txtfile = open(osp.join(save_dir, save_name + cdir), 'w')\n    nonefile = open(osp.join(save_dir, save_name + cdir + '.none'), 'w')\n\n    all_sizes = []\n    for frame, annot in zip(all_frames, all_annots):\n      basename_f = osp.basename(frame)\n      basename_a = osp.basename(annot)\n      assert basename_a[:6] == basename_f[:6], 'The name of {} is not right with {}'.format(frame, annot)\n      current_video.append( (frame, annot) )\n      box_str = datasets.dataset_utils.for_generate_box_str(annot, 68, EXPAND_RATIO)\n      txtfile.write('{} {} {}\\n'.format(frame, annot, box_str))\n      nonefile.write('{} None {}\\n'.format(frame, box_str))\n      all_sizes.append( str2size(box_str) )\n      if len(current_video) == 1:\n        first_videos.append( (frame, annot) )\n    txtfile.close()\n    nonefile.close()\n    videos.append( current_video )\n    all_sizes = np.array( all_sizes )\n    print ('--->>> {:} : [{:02d}/{:02d}] : {:} has {:} frames | face size : mean={:.2f}, std={:.2f}'.format(save_name, idx, len(dirs), cdir, len(all_frames), all_sizes.mean(), all_sizes.std()))\n\n    for jxj, video in enumerate(current_video):\n      if jxj <= 3 or jxj + 3 >= len(current_video): continue\n      if jxj % 10 == 3:\n        sparse_videos.append( video )\n\n  txtfile = open(osp.join(save_dir, save_name), 'w')\n  nonefile = open(osp.join(save_dir, save_name + '.none'), 'w')\n  for video in videos:\n    for cpair in video:\n      box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n      txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n      nonefile.write('{} {} {}\\n'.format(cpair[0], 'None', box_str))\n      txtfile.flush()\n      nonefile.flush()\n  txtfile.close()\n  nonefile.close()\n\n  txtfile = open(osp.join(save_dir, save_name + '.sparse' + afterfix), 'w')\n  nonefile = open(osp.join(save_dir, save_name + '.sparse.none' + afterfix), 'w')\n  for cpair in sparse_videos:\n    box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n    txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n    nonefile.write('{} {} {}\\n'.format(cpair[0], 'None', box_str))\n  txtfile.close()\n  nonefile.close()\n\n  txtfile = open(osp.join(save_dir, save_name + '.first'), 'w')\n  for cpair in first_videos:\n    box_str = datasets.dataset_utils.for_generate_box_str(cpair[1], 68, EXPAND_RATIO)\n    txtfile.write('{} {} {}\\n'.format(cpair[0], cpair[1], box_str))\n  txtfile.close()\n\n  print ('{} finish save into {}'.format(save_name, save_dir))\n  return videos", "label": 1}
{"function": "def generate_300vw_list(root, save_dir):\n  assert osp.isdir(root), '{} is not dir'.format(root)\n  if not osp.isdir(save_dir): os.makedirs(save_dir)\n  test_1_dirs = [114, 124, 125, 126, 150, 158, 401, 402, 505, 506, 507, 508, 509, 510, 511, 514, 515, 518, 519, 520, 521, 522, 524, 525, 537, 538, 540, 541, 546, 547, 548]\n  test_2_dirs = [203, 208, 211, 212, 213, 214, 218, 224, 403, 404, 405, 406, 407, 408, 409, 412, 550, 551, 553]\n  test_3_dirs = [410, 411, 516, 517, 526, 528, 529, 530, 531, 533, 557, 558, 559, 562]\n  train_dirs  = ['009', '059', '002', '033', '020', '035', '018', '119', '120', '025', '205', '047', '007', '013', '004', '143',\n                 '034', '028', '053', '225', '041', '010', '031', '046', '049', '011', '027', '003', '016', '160', '113', '001', '029', '043',\n                 '112', '138', '144', '204', '057', '015', '044', '048', '017', '115', '223', '037', '123', '019', '039', '022']\n\n  test_1_dirs, test_2_dirs, test_3_dirs = [ '{}'.format(x) for x in test_1_dirs], [ '{}'.format(x) for x in test_2_dirs], [ '{}'.format(x) for x in test_3_dirs]\n  #all_dirs = os.listdir(root)\n  #train_dirs = set(all_dirs) - set(test_1_dirs) - set(test_2_dirs) - set(test_3_dirs) - set(['ReadMe.txt', 'extra.zip'])\n  #train_dirs = list( train_dirs )\n  assert len(train_dirs) == 50, 'The length of train_dirs is not right : {}'.format( len(train_dirs) )\n  assert len(test_3_dirs) == 14, 'The length of test_3_dirs is not right : {}'.format( len(test_3_dirs) )\n\n  load_video_dir(root,  train_dirs, save_dir, '300VW.train.lst')\n  load_video_dir(root, test_1_dirs, save_dir, '300VW.test-1.lst')\n  load_video_dir(root, test_2_dirs, save_dir, '300VW.test-2.lst')\n  load_video_dir(root, test_3_dirs, save_dir, '300VW.test-3.lst')", "label": 1}
{"function": "\tdef __init__(self):\n\t\tRpcRequest.__init__(self, 'Domain', '2018-01-29', 'ListEmailVerification','domain')\n\t\tself.set_method('POST')\n\t\tif hasattr(self, \"endpoint_map\"):\n\t\t\tsetattr(self, \"endpoint_map\", endpoint_data.getEndpointMap())\n\t\tif hasattr(self, \"endpoint_regional\"):\n\t\t\tsetattr(self, \"endpoint_regional\", endpoint_data.getEndpointRegional())", "label": 1}
{"function": "\tdef get_EndCreateTime(self):\n\t\treturn self.get_query_params().get('EndCreateTime')", "label": 1}
{"function": "\tdef set_EndCreateTime(self,EndCreateTime):\n\t\tself.add_query_param('EndCreateTime',EndCreateTime)", "label": 1}
{"function": "\tdef get_PageNum(self):\n\t\treturn self.get_query_params().get('PageNum')", "label": 1}
{"function": "\tdef set_PageNum(self,PageNum):\n\t\tself.add_query_param('PageNum',PageNum)", "label": 1}
{"function": "\tdef get_VerificationStatus(self):\n\t\treturn self.get_query_params().get('VerificationStatus')", "label": 1}
{"function": "\tdef set_VerificationStatus(self,VerificationStatus):\n\t\tself.add_query_param('VerificationStatus',VerificationStatus)", "label": 1}
{"function": "\tdef get_BeginCreateTime(self):\n\t\treturn self.get_query_params().get('BeginCreateTime')", "label": 1}
{"function": "        def set_value(self, value):\n            self.__value = value\n            self.changed()  # Emit signal", "label": 1}
{"function": "        def get_value(self):\n            return self.__value", "label": 1}
{"function": "        def __init__(self, model):\n            self.model = model\n            model.changed.connect(self.model_changed)", "label": 1}
{"function": "        def model_changed(self):\n            print(\"   New value:\", self.model.get_value())", "label": 1}
{"function": "  def _define_vars(self, params):\n    pass", "label": 1}
{"function": "  def inference_graph(self, data):\n    with ops.device(self.device_assigner):\n      # Compute activations for the neural network.\n      nn_activations = layers.fully_connected(data, self.params.layer_size)\n\n      for _ in range(1, self.params.num_layers):\n        # pylint: disable=W0106\n        nn_activations = layers.fully_connected(nn_activations,\n                                                self.params.layer_size)\n      return nn_activations", "label": 1}
{"function": "  def _define_vars(self, params):\n    pass", "label": 1}
{"function": "  def inference_graph(self, data):\n    with ops.device(self.device_assigner):\n      # Compute activations for the neural network.\n      nn_activations = layers.fully_connected(data, 1)\n\n      # There is always one activation per instance by definition, so squeeze\n      # away the extra dimension.\n      return array_ops.squeeze(nn_activations, squeeze_dims=[1])", "label": 1}
{"function": "  def _define_vars(self, params):\n    pass", "label": 1}
{"function": "  def inference_graph(self, data):\n    with ops.device(self.device_assigner):\n      # Compute activations for the neural network.\n      nn_activations = [layers.fully_connected(data, self.params.layer_size)]\n\n      for _ in range(1, self.params.num_layers):\n        # pylint: disable=W0106\n        nn_activations.append(\n            layers.fully_connected(\n                nn_activations[-1],\n                self.params.layer_size))\n\n      nn_activations_tensor = array_ops.concat(\n          nn_activations, 1, name=\"flattened_nn_activations\")\n\n      return nn_activations_tensor", "label": 1}
{"function": "  def _IpPrefix2AddressMask(self, addr):\n    def _Length2Mask(length):\n      return 0xFFFFFFFF & ~((1 << (32 - length)) - 1)\n\n    addr, masklen = addr.split('/')\n    return self._Ip2Long(addr), _Length2Mask(int(masklen))", "label": 1}
{"function": "  def _GetHostAddresses(self, iface):\n    \"\"\"Returns the IP addresses on host's interfaces, breaking out |iface|.\"\"\"\n    interface_list = self._EnumerateHostInterfaces()\n    addresses = []\n    iface_address = None\n    found_iface = False\n    for line in interface_list:\n      if not line.startswith((' ', '\\t')):\n        found_iface = iface in line\n      match = re.search(r'(?<=inet )\\S+', line)\n      if match:\n        address = match.group(0)\n        if '/' in address:\n          address = self._IpPrefix2AddressMask(address)\n        else:\n          match = re.search(r'(?<=netmask )\\S+', line)\n          address = self._Ip2Long(address), int(match.group(0), 16)\n        if found_iface:\n          assert not iface_address, (\n            'Found %s twice when parsing host interfaces.' % iface)\n          iface_address = address\n        else:\n          addresses.append(address)\n    return addresses, iface_address", "label": 1}
{"function": "  def _GetDeviceAddresses(self, excluded_iface):\n    \"\"\"Returns the IP addresses on all connected devices.\n    Excludes interface |excluded_iface| on the selected device.\n    \"\"\"\n    my_device = str(self._device)\n    addresses = []\n    for device_serial in android_device.GetDeviceSerials(None):\n      try:\n        device = device_utils.DeviceUtils(device_serial)\n        if device_serial == my_device:\n          excluded = excluded_iface\n        else:\n          excluded = 'no interfaces excluded on other devices'\n        output = device.RunShellCommand(\n            ['ip', '-o', '-4', 'addr'], check_return=True)\n        addresses += [\n            line.split()[3] for line in output if excluded not in line]\n      except device_errors.CommandFailedError:\n        logging.warning('Unable to determine IP addresses for %s',\n                        device_serial)\n    return addresses", "label": 1}
{"function": "  def _ConfigureNetwork(self, device_iface, host_iface):\n    \"\"\"Configures the |device_iface| to be on the same network as |host_iface|.\n    \"\"\"\n    def _Long2Ip(value):\n      return socket.inet_ntoa(struct.pack('!L', value))\n\n    def _IsNetworkUnique(network, addresses):\n      return all((addr & mask != network & mask) for addr, mask in addresses)\n\n    def _NextUnusedAddress(network, netmask, used_addresses):\n      # Excludes '0' and broadcast.\n      for suffix in range(1, 0xFFFFFFFF & ~netmask):\n        candidate = network | suffix\n        if candidate not in used_addresses:\n          return candidate\n\n    def HasHostAddress():\n      _, host_address = self._GetHostAddresses(host_iface)\n      return bool(host_address)\n\n    if not HasHostAddress():\n      if platform.GetHostPlatform().GetOSName() == 'mac':\n        if 'Telemetry' not in subprocess.check_output(\n            ['networksetup', '-listallnetworkservices']):\n          subprocess.check_call(\n              ['/usr/bin/sudo', 'networksetup',\n               '-createnetworkservice', 'Telemetry', host_iface])\n          subprocess.check_call(\n              ['/usr/bin/sudo', 'networksetup',\n               '-setmanual', 'Telemetry', '192.168.123.1', '255.255.255.0'])\n      elif platform.GetHostPlatform().GetOSName() == 'linux':\n        with open(self._NETWORK_INTERFACES) as f:\n          orig_interfaces = f.read()\n        if self._INTERFACES_INCLUDE not in orig_interfaces:\n          interfaces = '\\n'.join([\n              orig_interfaces,\n              '',\n              '# Added by Telemetry.',\n              self._INTERFACES_INCLUDE])\n          self._WriteProtectedFile(self._NETWORK_INTERFACES, interfaces)\n        interface_conf_file = self._TELEMETRY_INTERFACE_FILE.format(host_iface)\n        if not os.path.exists(interface_conf_file):\n          interface_conf_dir = os.path.dirname(interface_conf_file)\n          if not os.path.exists(interface_conf_dir):\n            subprocess.call(['/usr/bin/sudo', '/bin/mkdir', interface_conf_dir])\n            subprocess.call(\n                ['/usr/bin/sudo', '/bin/chmod', '755', interface_conf_dir])\n          interface_conf = '\\n'.join([\n              '# Added by Telemetry for RNDIS forwarding.',\n              'allow-hotplug %s' % host_iface,\n              'iface %s inet static' % host_iface,\n              '  address 192.168.123.1',\n              '  netmask 255.255.255.0',\n              ])\n          self._WriteProtectedFile(interface_conf_file, interface_conf)\n          subprocess.check_call(['/usr/bin/sudo', 'ifup', host_iface])\n      logging.info('Waiting for RNDIS connectivity...')\n      py_utils.WaitFor(HasHostAddress, 30)\n\n    addresses, host_address = self._GetHostAddresses(host_iface)\n    assert host_address, 'Interface %s could not be configured.' % host_iface\n\n    host_ip, netmask = host_address  # pylint: disable=unpacking-non-sequence\n    network = host_ip & netmask\n\n    if not _IsNetworkUnique(network, addresses):\n      logging.warning(\n        'The IP address configuration %s of %s is not unique!\\n'\n        'Check your /etc/network/interfaces. If this overlap is intended,\\n'\n        'you might need to use: ip rule add from <device_ip> lookup <table>\\n'\n        'or add the interface to a bridge in order to route to this network.',\n        host_address, host_iface\n      )\n\n    # Find unused IP address.\n    used_addresses = [addr for addr, _ in addresses]\n    used_addresses += [self._IpPrefix2AddressMask(addr)[0]\n                       for addr in self._GetDeviceAddresses(device_iface)]\n    used_addresses += [host_ip]\n\n    device_ip = _NextUnusedAddress(network, netmask, used_addresses)\n    assert device_ip, ('The network %s on %s is full.' %\n                       (host_address, host_iface))\n\n    host_ip = _Long2Ip(host_ip)\n    device_ip = _Long2Ip(device_ip)\n    netmask = _Long2Ip(netmask)\n\n    # TODO(szym) run via su -c if necessary.\n    self._device.RunShellCommand(\n        ['ifconfig', device_iface, device_ip, 'netmask', netmask, 'up'],\n        check_return=True)\n    # Enabling the interface sometimes breaks adb.\n    self._device.WaitUntilFullyBooted()\n    self._host_iface = host_iface\n    self._host_ip = host_ip\n    self.device_iface = device_iface\n    self._device_ip = device_ip", "label": 1}
{"function": "  def _TestConnectivity(self):\n    with open(os.devnull, 'wb') as devnull:\n      return subprocess.call(['ping', '-q', '-c1', '-W1', self._device_ip],\n                             stdout=devnull) == 0", "label": 1}
{"function": "  def OverrideRoutingPolicy(self):\n    \"\"\"Override any routing policy that could prevent\n    packets from reaching the rndis interface\n    \"\"\"\n    policies = self._device.RunShellCommand(['ip', 'rule'], check_return=True)\n    if len(policies) > 1 and not 'lookup main' in policies[1]:\n      self._device.RunShellCommand(\n          ['ip', 'rule', 'add', 'prio', '1', 'from', 'all', 'table', 'main'],\n          check_return=True)\n      self._device.RunShellCommand(\n          ['ip', 'route', 'flush', 'cache'], check_return=True)", "label": 1}
{"function": "  def RestoreRoutingPolicy(self):\n    policies = self._device.RunShellCommand(['ip', 'rule'], check_return=True)\n    if len(policies) > 1 and re.match(\"^1:.*lookup main\", policies[1]):\n      self._device.RunShellCommand(\n          ['ip', 'rule', 'del', 'prio', '1'], check_return=True)\n      self._device.RunShellCommand(\n          ['ip', 'route', 'flush', 'cache'], check_return=True)", "label": 1}
{"function": "  def _CheckConfigureNetwork(self):\n    \"\"\"Enables RNDIS and configures it, retrying until we have connectivity.\"\"\"\n    force = False\n    for _ in range(3):\n      device_iface, host_iface = self._CheckEnableRndis(force)\n      self._ConfigureNetwork(device_iface, host_iface)\n      self.OverrideRoutingPolicy()\n      # Sometimes the first packet will wake up the connection.\n      for _ in range(3):\n        if self._TestConnectivity():\n          return\n      force = True\n    self.RestoreRoutingPolicy()\n    raise Exception('No connectivity, giving up.')", "label": 1}
{"function": "    def HoRNDISLoaded():\n      return 'HoRNDIS' in subprocess.check_output(['kextstat'])", "label": 1}
{"function": "    def _Length2Mask(length):\n      return 0xFFFFFFFF & ~((1 << (32 - length)) - 1)", "label": 1}
{"function": "    def test_atomic_min_int64(self):\n        self.check_atomic_min(dtype=np.int64, lo=-65535, hi=65535)", "label": 1}
{"function": "    def test_atomic_min_uint64(self):\n        self.check_atomic_min(dtype=np.uint64, lo=0, hi=65535)", "label": 1}
{"function": "    def test_atomic_min_float(self):\n        self.check_atomic_min(dtype=np.float32, lo=-65535, hi=65535)", "label": 1}
{"function": "    def test_atomic_min_double(self):\n        self.check_atomic_min(dtype=np.float64, lo=-65535, hi=65535)", "label": 1}
{"function": "    def test_atomic_max_double_normalizedindex(self):\n        vals = np.random.randint(0, 65535, size=(32, 32)).astype(np.float64)\n        res = np.zeros(1, np.float64)\n        cuda_func = cuda.jit('void(float64[:], float64[:,:])')(\n            atomic_max_double_normalizedindex)\n        cuda_func[32, 32](res, vals)\n\n        gold = np.max(vals)\n        np.testing.assert_equal(res, gold)", "label": 1}
{"function": "    def test_atomic_max_double_oneindex(self):\n        vals = np.random.randint(0, 128, size=32).astype(np.float64)\n        res = np.zeros(1, np.float64)\n        cuda_func = cuda.jit('void(float64[:], float64[:])')(\n            atomic_max_double_oneindex)\n        cuda_func[1, 32](res, vals)\n\n        gold = np.max(vals)\n        np.testing.assert_equal(res, gold)", "label": 1}
{"function": "    def test_atomic_max_nan_location(self):\n        vals = np.random.randint(0, 128, size=(1,1)).astype(np.float64)\n        gold = vals.copy().reshape(1)\n        res = np.zeros(1, np.float64) + np.nan\n        cuda_func = cuda.jit('void(float64[:], float64[:,:])')(atomic_max)\n        cuda_func[1, 1](res, vals)\n\n        np.testing.assert_equal(res, gold)", "label": 1}
{"function": "    def test_atomic_max_nan_val(self):\n        res = np.random.randint(0, 128, size=1).astype(np.float64)\n        gold = res.copy()\n        vals = np.zeros((1, 1), np.float64) + np.nan\n        cuda_func = cuda.jit('void(float64[:], float64[:,:])')(atomic_max)\n        cuda_func[1, 1](res, vals)\n\n        np.testing.assert_equal(res, gold)", "label": 1}
{"function": "    def test_atomic_max_double_shared(self):\n        vals = np.random.randint(0, 32, size=32).astype(np.float64)\n        res = np.zeros(1, np.float64)\n        cuda_func = cuda.jit('void(float64[:], float64[:])')(atomic_max_double_shared)\n        cuda_func[1, 32](res, vals)\n\n        gold = np.max(vals)\n        np.testing.assert_equal(res, gold)", "label": 1}
{"function": "    def test_atomic_compare_and_swap(self):\n        n = 100\n        res = [-99] * (n // 2) + [-1] * (n // 2)\n        random.shuffle(res)\n        res = np.asarray(res, dtype=np.int32)\n        out = np.zeros_like(res)\n        ary = np.random.randint(1, 10, size=res.size).astype(res.dtype)\n\n        fill_mask = res == -99\n        unfill_mask = res == -1\n\n        expect_res = np.zeros_like(res)\n        expect_res[fill_mask] = ary[fill_mask]\n        expect_res[unfill_mask] = -1\n\n        expect_out = np.zeros_like(out)\n        expect_out[fill_mask] = res[fill_mask]\n        expect_out[unfill_mask] = -1\n\n        cuda_func = cuda.jit(atomic_compare_and_swap)\n        cuda_func[10, 10](res, out, ary)\n\n        np.testing.assert_array_equal(expect_res, res)\n        np.testing.assert_array_equal(expect_out, out)", "label": 1}
{"function": "    def test_string_rjust(self):\n        self.check_func(lambda x: x.str.rjust(0))\n        self.check_func(lambda x: x.str.rjust(10))\n        self.check_func(lambda x: x.str.rjust(30, \"x\"))", "label": 1}
{"function": "    def test_string_rpartition(self):\n        with self.assertRaises(NotImplementedError):\n            self.check_func(lambda x: x.str.rpartition())", "label": 1}
{"function": "    def test_string_slice(self):\n        self.check_func(lambda x: x.str.slice(start=1))\n        self.check_func(lambda x: x.str.slice(stop=3))\n        self.check_func(lambda x: x.str.slice(step=2))\n        self.check_func(lambda x: x.str.slice(start=0, stop=5, step=3))", "label": 1}
{"function": "    def test_string_slice_replace(self):\n        self.check_func(lambda x: x.str.slice_replace(1, repl=\"X\"))\n        self.check_func(lambda x: x.str.slice_replace(stop=2, repl=\"X\"))\n        self.check_func(lambda x: x.str.slice_replace(start=1, stop=3, repl=\"X\"))", "label": 1}
{"function": "    def test_string_split(self):\n        self.check_func_on_series(lambda x: x.str.split().apply(str), self.pser[:-1])\n        self.check_func_on_series(lambda x: x.str.split(r\"p*\").apply(str), self.pser[:-1])\n        pser = pd.Series([\"This is a sentence.\", \"This-is-a-long-word.\"])\n        self.check_func_on_series(lambda x: x.str.split(n=2).apply(str), pser)\n        self.check_func_on_series(lambda x: x.str.split(pat=\"-\", n=2).apply(str), pser)\n        self.check_func_on_series(lambda x: x.str.split(n=2, expand=True), pser, almost=True)\n        with self.assertRaises(NotImplementedError):\n            self.check_func(lambda x: x.str.split(expand=True))", "label": 1}
{"function": "    def test_string_rsplit(self):\n        self.check_func_on_series(lambda x: x.str.rsplit().apply(str), self.pser[:-1])\n        self.check_func_on_series(lambda x: x.str.rsplit(r\"p*\").apply(str), self.pser[:-1])\n        pser = pd.Series([\"This is a sentence.\", \"This-is-a-long-word.\"])\n        self.check_func_on_series(lambda x: x.str.rsplit(n=2).apply(str), pser)\n        self.check_func_on_series(lambda x: x.str.rsplit(pat=\"-\", n=2).apply(str), pser)\n        self.check_func_on_series(lambda x: x.str.rsplit(n=2, expand=True), pser, almost=True)\n        with self.assertRaises(NotImplementedError):\n            self.check_func(lambda x: x.str.rsplit(expand=True))", "label": 1}
{"function": "    def test_string_translate(self):\n        m = str.maketrans({\"a\": \"X\", \"e\": \"Y\", \"i\": None})\n        self.check_func(lambda x: x.str.translate(m))", "label": 1}
{"function": "    def test_string_wrap(self):\n        self.check_func(lambda x: x.str.wrap(5))\n        self.check_func(lambda x: x.str.wrap(5, expand_tabs=False))\n        self.check_func(lambda x: x.str.wrap(5, replace_whitespace=False))\n        self.check_func(lambda x: x.str.wrap(5, drop_whitespace=False))\n        self.check_func(lambda x: x.str.wrap(5, break_long_words=False))\n        self.check_func(lambda x: x.str.wrap(5, break_on_hyphens=False))", "label": 1}
{"function": "    def test_string_zfill(self):\n        self.check_func(lambda x: x.str.zfill(10))", "label": 1}
{"function": "    def test_string_get_dummies(self):\n        with self.assertRaises(NotImplementedError):\n            self.check_func(lambda x: x.str.get_dummies())", "label": 1}
{"function": "    def test_get_signal_signal_wrong_data_shape(self):\n        s = self.signal\n        s = s.transpose(signal_axes=1)\n        with pytest.raises(ValueError):\n            s._get_signal_signal(data=np.zeros((3, 2)))", "label": 1}
{"function": "    def test_get_signal_signal_wrong_data_shape_dim0(self):\n        s = self.signal\n        s = s.transpose(signal_axes=0)\n        with pytest.raises(ValueError):\n            s._get_signal_signal(data=np.asarray(0))", "label": 1}
{"function": "    def test_get_signal_signal_given_data(self):\n        s = self.signal\n        s = s.transpose(signal_axes=2)\n        data = np.zeros(s.axes_manager._signal_shape_in_array)\n        ns = s._get_signal_signal(data=data)\n        assert ns.data is data", "label": 1}
{"function": "    def test_get_navigation_signal_dtype(self):\n        s = self.signal\n        assert s._get_navigation_signal().data.dtype.name == s.data.dtype.name", "label": 1}
{"function": "    def test_get_signal_signal_dtype(self):\n        s = self.signal\n        assert s._get_signal_signal().data.dtype.name == s.data.dtype.name", "label": 1}
{"function": "    def test_get_navigation_signal_given_dtype(self):\n        s = self.signal\n        assert s._get_navigation_signal(dtype=\"bool\").data.dtype.name == \"bool\"", "label": 1}
{"function": "    def test_get_signal_signal_given_dtype(self):\n        s = self.signal\n        assert s._get_signal_signal(dtype=\"bool\").data.dtype.name == \"bool\"", "label": 1}
{"function": "    def __init__(self, base_params={}, base_url='http://localhost:9001',\n                       api_version='1.1', timeout=20):\n        self.api_version = api_version\n        self.base_params = base_params\n        self.base_url = base_url\n        self.timeout = timeout", "label": 1}
{"function": "    def __call__(self, path, **params):\n        data = urlencode(dict(self.base_params, **params)).encode('ascii')\n        url = '%s/%s/%s' % (self.base_url + '/api', self.api_version, path)\n        r = json.loads(urlopen(url, data, self.timeout).read().decode('utf-8'))\n        if not r or not isinstance(r, dict):\n            raise EtherpadException('API returned: %s' % r)\n        if r.get('code') != 0:\n            raise EtherpadException(r.get('message', r))\n        return r.get('data')", "label": 1}
{"function": "    def __getattr__(self, name):\n        return partial(self, name)", "label": 1}
{"function": "def compute_chunk(chunk, chunk_expr, part):\n    return compute(chunk_expr, {chunk: part})", "label": 1}
{"function": "def compute_down(expr, data, map=None, **kwargs):\n    if map is None:\n        map = get_default_pmap()\n\n    leaf = expr._leaves()[0]\n\n    (chunk, chunk_expr), (agg, agg_expr) = split(leaf, expr)\n\n    parts = list(map(curry(compute_chunk, chunk, chunk_expr), data))\n\n    if isinstance(parts[0], np.ndarray):\n        intermediate = np.concatenate(parts)\n    elif isinstance(parts[0], pd.DataFrame):\n        intermediate = pd.concat(parts)\n    elif isinstance(parts[0], (Iterable, Iterator)):\n        intermediate = list(concat(parts))\n\n    return compute(agg_expr, {agg: intermediate})", "label": 1}
{"function": "def compute_down(expr, data, **kwargs):\n    leaf = expr._leaves()[0]\n    if all(isinstance(e, Cheap) for e in path(expr, leaf)):\n        return compute(expr, {leaf: into(Iterator, data)}, **kwargs)\n    else:\n        raise MDNotImplementedError()", "label": 1}
{"function": "    def __init__(self, kernel_size=1):\n        super(Resample2d, self).__init__()\n        self.kernel_size = kernel_size", "label": 1}
{"function": "    def forward(self, input1, input2):\n        input1_c = input1.contiguous()\n\n        result = Resample2dFunction(self.kernel_size)(input1_c, input2)\n\n        return result", "label": 1}
{"function": "def index():\n    return {}", "label": 1}
{"function": "def register_get():\n    return {}", "label": 1}
{"function": "def register_post():\n    return {}", "label": 1}
{"function": "def login_get():\n    return {}", "label": 1}
{"function": "def login_post():\n    return {}", "label": 1}
{"function": "def convert_openai_checkpoint_to_pytorch(openai_checkpoint_folder_path, openai_config_file, pytorch_dump_folder_path):\n    # Construct model\n    if openai_config_file == \"\":\n        config = OpenAIGPTConfig()\n    else:\n        config = OpenAIGPTConfig.from_json_file(openai_config_file)\n    model = OpenAIGPTModel(config)\n\n    # Load weights from numpy\n    load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path)\n\n    # Save pytorch-model\n    pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME\n    pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME\n    print(\"Save PyTorch model to {}\".format(pytorch_weights_dump_path))\n    torch.save(model.state_dict(), pytorch_weights_dump_path)\n    print(\"Save configuration file to {}\".format(pytorch_config_dump_path))\n    with open(pytorch_config_dump_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(config.to_json_string())", "label": 1}
{"function": "def parse_arguments(argv):\n  \"\"\"Parse command line arguments.\"\"\"\n  parser = argparse.ArgumentParser(\n      description=\"Runs Preprocessing on Steam product data.\")\n  parser.add_argument(\n      \"--cloud\",\n      action=\"store_true\",\n      help=\"Run preprocessing on the cloud.\")\n  parser.add_argument(\n      \"--job_name\",\n      help=\"Dataflow job name.\")\n  parser.add_argument(\n      \"--job_dir\",\n      help=\"Bucket to stage code and write temp outputs for cloud runs.\")\n  parser.add_argument(\n      \"--output_dir\",\n      required=True,\n      help=\"Dir or bucket (if cloud run) to write train, val and test data.\")\n  parser.add_argument(\n      \"--tft_dir\",\n      required=True,\n      help=\"Dir or bucket (if cloud run) where tft outputs are written.\")\n  parser.add_argument(\n      \"--user_min_count\",\n      required=True,\n      type=int,\n      help=\"Min number of users owning an item to include it in the vocab.\")\n  parser.add_argument(\n      \"--item_min_count\",\n      required=True,\n      type=int,\n      help=\"Min number of items owned by a user to include it in the vocab.\")\n  parser.add_argument(\n      \"--plain_text\",\n      action=\"store_true\",\n      help=\"Write pipeline output to plain text instead of tf-record.\")\n  args = parser.parse_args(args=argv[1:])\n  return args", "label": 1}
{"function": "def set_logging(log_level):\n  \"\"\"Set the logging.\"\"\"\n  logging.getLogger().setLevel(getattr(logging, log_level.upper()))", "label": 1}
{"function": "def get_relative_path(path):\n  \"\"\"Return the given path relative to this module.\"\"\"\n  module_dir = os.path.dirname(__file__)\n  return os.path.join(module_dir, path)", "label": 1}
{"function": "def parse_config(env, config_file_path):\n  \"\"\"Parses configuration file.\n\n  Args:\n    env: The environment in which the preprocessing job will be run.\n    config_file_path: Path to the configuration file to be parsed.\n\n  Returns:\n    A dictionary containing the parsed runtime config.\n  \"\"\"\n  config = configparser.ConfigParser()\n  config.read(config_file_path)\n  return dict(config.items(env))", "label": 1}
{"function": "def get_pipeline_options(args, config):\n  \"\"\"Returns pipeline options based on args and confs.\"\"\"\n  options = {\"project\": str(config.get(\"project\"))}\n  if args.cloud:\n    if not args.job_name:\n      raise ValueError(\"Job name must be specified for cloud runs.\")\n    if not args.job_dir:\n      raise ValueError(\"Job dir must be specified for cloud runs.\")\n    options.update({\n        \"job_name\": args.job_name,\n        \"max_num_workers\": int(config.get(\"max_num_workers\")),\n        \"setup_file\": os.path.abspath(get_relative_path(\n            \"../setup.py\")),\n        \"staging_location\": os.path.join(args.job_dir, \"staging\"),\n        \"temp_location\": os.path.join(args.job_dir, \"tmp\"),\n        \"region\": config.get(\"region\"),\n    })\n  pipeline_options = beam.pipeline.PipelineOptions(flags=[], **options)\n  return pipeline_options", "label": 1}
{"function": "def main():\n  \"\"\"Configures and runs a pipeline.\"\"\"\n  args = parse_arguments(sys.argv)\n  config = parse_config(\n      \"CLOUD\" if args.cloud else \"LOCAL\",\n      get_relative_path(\"config.ini\"))\n  set_logging(config.get(\"log_level\"))\n  options = get_pipeline_options(args, config)\n  runner = str(config.get(\"runner\"))\n\n  with beam.Pipeline(runner, options=options) as pipeline:\n    with beam_impl.Context(\n        temp_dir=os.path.join(args.tft_dir, constants.TMP_DIR)):\n      preprocess.run(pipeline, args)", "label": 1}
{"function": "def env():\n    return DjangoEnv()", "label": 1}
{"function": "def symbol_by_name(patching):\n    return patching('thorn.environment.django.symbol_by_name')", "label": 1}
{"function": "def test_autodetect__when_no_django(env):\n    assert not env.autodetect()", "label": 1}
{"function": "    def is_user_validator(user_id: int) -> bool:\n        \"\"\" Determines if user is a validator \"\"\"\n        user = UserService.get_user_by_id(user_id)\n\n        if UserRole(user.role) in [\n            UserRole.ADMIN,\n        ]:\n            return True\n\n        return False", "label": 1}
{"function": "    def is_user_blocked(user_id: int) -> bool:\n        \"\"\" Determines if a user is blocked \"\"\"\n        user = UserService.get_user_by_id(user_id)\n\n        if UserRole(user.role) == UserRole.READ_ONLY:\n            return True\n\n        return False", "label": 1}
{"function": "    def get_countries_contributed(user_id: int):\n        query = (\n            TaskHistory.query.with_entities(\n                func.unnest(Project.country).label(\"country\"),\n                TaskHistory.action_text,\n                func.count(TaskHistory.action_text).label(\"count\"),\n            )\n            .filter(TaskHistory.user_id == user_id)\n            .filter(\n                TaskHistory.action_text.in_(\n                    [\n                        TaskStatus.MAPPED.name,\n                        TaskStatus.BADIMAGERY.name,\n                        TaskStatus.VALIDATED.name,\n                    ]\n                )\n            )\n            .group_by(\"country\", TaskHistory.action_text)\n            .outerjoin(Project, Project.id == TaskHistory.project_id)\n            .all()\n        )\n        countries = list(set([q.country for q in query]))\n        result = []\n        for country in countries:\n            values = [q for q in query if q.country == country]\n\n            # Filter element to sum mapped values.\n            mapped = sum(\n                [\n                    v.count\n                    for v in values\n                    if v.action_text\n                    in [TaskStatus.MAPPED.name, TaskStatus.BADIMAGERY.name]\n                ]\n            )\n            validated = sum(\n                [v.count for v in values if v.action_text == TaskStatus.VALIDATED.name]\n            )\n            dto = UserCountryContributed(\n                dict(\n                    name=country,\n                    mapped=mapped,\n                    validated=validated,\n                    total=mapped + validated,\n                )\n            )\n            result.append(dto)\n\n        # Order by total\n        result = sorted(result, reverse=True, key=lambda i: i.total)\n        countries_dto = UserCountriesContributed()\n        countries_dto.countries_contributed = result\n        countries_dto.total = len(result)\n\n        return countries_dto", "label": 1}
{"function": "    def upsert_mapped_projects(user_id: int, project_id: int):\n        \"\"\" Add project to mapped projects if it doesn't exist, otherwise return \"\"\"\n        User.upsert_mapped_projects(user_id, project_id)", "label": 1}
{"function": "    def get_mapped_projects(user_name: str, preferred_locale: str):\n        \"\"\" Gets all projects a user has mapped or validated on \"\"\"\n        user = UserService.get_user_by_username(user_name)\n        return User.get_mapped_projects(user.id, preferred_locale)", "label": 1}
{"function": "    def get_recommended_projects(user_name: str, preferred_locale: str):\n        \"\"\" Gets all projects a user has mapped or validated on \"\"\"\n        from backend.services.project_search_service import ProjectSearchService\n\n        limit = 20\n        user = (\n            User.query.with_entities(User.id, User.mapping_level)\n            .filter(User.username == user_name)\n            .one_or_none()\n        )\n        if user is None:\n            raise NotFound()\n\n        # Get all projects that the user has contributed\n        sq = (\n            TaskHistory.query.with_entities(TaskHistory.project_id.label(\"project_id\"))\n            .distinct(TaskHistory.project_id)\n            .filter(TaskHistory.user_id == user.id)\n            .subquery()\n        )\n        # Get all campaigns for all contributed projects.\n        campaign_tags = (\n            Project.query.with_entities(Project.campaign.label(\"tag\"))\n            .filter(or_(Project.author_id == user.id, Project.id == sq.c.project_id))\n            .subquery()\n        )\n        # Get projects with given campaign tags but without user contributions.\n        query = ProjectSearchService.create_search_query()\n        projs = (\n            query.filter(Project.campaign.any(campaign_tags.c.tag)).limit(limit).all()\n        )\n\n        # Get only user mapping level projects.\n        len_projs = len(projs)\n        if len_projs < limit:\n            remaining_projs = (\n                query.filter(Project.mapper_level == user.mapping_level)\n                .limit(limit - len_projs)\n                .all()\n            )\n            projs.extend(remaining_projs)\n\n        dto = ProjectSearchResultsDTO()\n\n        # Get all total contributions for each paginated project.\n        contrib_counts = ProjectSearchService.get_total_contributions(projs)\n\n        zip_items = zip(projs, contrib_counts)\n\n        dto.results = [\n            ProjectSearchService.create_result_dto(p, \"en\", t) for p, t in zip_items\n        ]\n\n        return dto", "label": 1}
{"function": "    def add_role_to_user(admin_user_id: int, username: str, role: str):\n        \"\"\"\n        Add role to user\n        :param admin_user_id: ID of admin attempting to add the role\n        :param username: Username of user the role should be added to\n        :param role: The requested role\n        :raises UserServiceError\n        \"\"\"\n        try:\n            requested_role = UserRole[role.upper()]\n        except KeyError:\n            raise UserServiceError(\n                f\"Unknown role {role} accepted values are ADMIN, PROJECT_MANAGER, VALIDATOR\"\n            )\n\n        admin = UserService.get_user_by_id(admin_user_id)\n        admin_role = UserRole(admin.role)\n\n        if admin_role != UserRole.ADMIN and requested_role == UserRole.ADMIN:\n            raise UserServiceError(\"You must be an Admin to assign Admin role\")\n\n        user = UserService.get_user_by_username(username)\n        user.set_user_role(requested_role)", "label": 1}
{"function": "    def set_user_mapping_level(username: str, level: str) -> User:\n        \"\"\"\n        Sets the users mapping level\n        :raises: UserServiceError\n        \"\"\"\n        try:\n            requested_level = MappingLevel[level.upper()]\n        except KeyError:\n            raise UserServiceError(\n                f\"Unknown role {level} accepted values are BEGINNER, INTERMEDIATE, ADVANCED\"\n            )\n\n        user = UserService.get_user_by_username(username)\n        user.set_mapping_level(requested_level)\n\n        return user", "label": 1}
{"function": "    def set_user_is_expert(user_id: int, is_expert: bool) -> User:\n        \"\"\"\n        Enabled or disables expert mode for the user\n        :raises: UserServiceError\n        \"\"\"\n        user = UserService.get_user_by_id(user_id)\n        user.set_is_expert(is_expert)\n\n        return user", "label": 1}
{"function": "    def accept_license_terms(user_id: int, license_id: int):\n        \"\"\" Saves the fact user has accepted license terms \"\"\"\n        user = UserService.get_user_by_id(user_id)\n        user.accept_license_terms(license_id)", "label": 1}
{"function": "def failed(message=\"Please, reload the task and try again.\", name=None):\n    \"\"\" Reports failure \"\"\"\n    if not name:\n        name = sys._getframe().f_back.f_code.co_name\n    print(\"#educational_plugin \" + name + \" FAILED + \" + message)", "label": 1}
{"function": "def passed(name=None):\n    \"\"\" Reports success \"\"\"\n    if not name:\n        name = sys._getframe().f_back.f_code.co_name\n    print(\"#educational_plugin \" + name + \" test OK\")", "label": 1}
{"function": "def get_answer_placeholders():\n    \"\"\"\n        Returns all answer placeholders text\n    \"\"\"\n    prefix = \"#educational_plugin_window = \"\n    path = sys.argv[-1]\n    import os\n\n    file_name_without_extension = os.path.splitext(path)[0]\n    windows_path = file_name_without_extension + \"_windows\"\n    windows = []\n    f = open(windows_path, \"r\")\n    window_text = \"\"\n    first = True\n    for line in f.readlines():\n        if line.startswith(prefix):\n            if not first:\n                windows.append(window_text.strip())\n            else:\n                first = False\n            window_text = line[len(prefix):]\n        else:\n            window_text += line\n\n    if window_text:\n        windows.append(window_text.strip())\n\n    f.close()\n    return windows", "label": 1}
{"function": "def check_samples(samples=()):\n    \"\"\"\n      Check script output for all samples. Sample is a two element list, where the first is input and\n      the second is output.\n    \"\"\"\n    for sample in samples:\n        if len(sample) == 2:\n            output = get_file_output(arg_string=str(sample[0]))\n            if \"\\n\".join(output) != sample[1]:\n                failed(\n                    \"Test from samples failed: \\n \\n\"\n                    \"Input:\\n{}\"\n                    \"\\n \\n\"\n                    \"Expected:\\n{}\"\n                    \"\\n \\n\"\n                    \"Your result:\\n{}\".format(str.strip(sample[0]), str.strip(sample[1]), \"\\n\".join(output)))\n                return\n        set_congratulation_message(\"All test from samples passed. Now we are checking your solution on Stepic server.\")\n\n    passed()", "label": 1}
{"function": "def do_not_run_on_check():\n    \"\"\"\n    Do not execute task file on check\n    \"\"\"\n    print(\"#educational_plugin DO_NOT_RUN_ON_CHECK\")", "label": 1}
{"function": "def run_common_tests(error_text=\"Please, reload file and try again\"):\n    test_is_initial_text()\n    test_is_not_empty()\n    test_answer_placeholders_text_deleted()\n    test_file_importable()", "label": 1}
{"function": "    def mock_fun(_m=\"\"):\n        return \"mock\"", "label": 1}
{"function": "    def setUp(self):\n        super().setUp()\n\n        self.thread = test.post_thread(self.category, poster=self.user)\n        self.api_link = self.thread.get_api_url()\n\n        self.other_user = create_test_user(\"OtherUser\", \"otheruser@example.com\")", "label": 1}
{"function": "    def patch(self, api_link, ops):\n        return self.client.patch(\n            api_link, json.dumps(ops), content_type=\"application/json\"\n        )", "label": 1}
{"function": "    def test_add_participant_not_owner(self):\n        \"\"\"non-owner can't add participant\"\"\"\n        ThreadParticipant.objects.add_participants(self.thread, [self.user])\n\n        response = self.patch(\n            self.api_link,\n            [{\"op\": \"add\", \"path\": \"participants\", \"value\": self.user.username}],\n        )\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(\n            response.json(),\n            {\n                \"id\": self.thread.pk,\n                \"detail\": [\n                    \"You have to be thread owner to add new participants to it.\"\n                ],\n            },\n        )", "label": 1}
{"function": "    def _wrap_object(self, space, obj):\n        return space.newlong(obj)", "label": 1}
{"function": "    def _unwrap_object(self, space, w_obj):\n        return space.int_w(w_obj)", "label": 1}
{"function": "    def cffi_type(self, space):\n        state = space.fromcache(State)\n        return state.c_long", "label": 1}
{"function": "    def _wrap_object(self, space, obj):\n        return space.newlong_from_rarith_int(obj)", "label": 1}
{"function": "    def _unwrap_object(self, space, w_obj):\n        return space.uint_w(w_obj)", "label": 1}
{"function": "    def cffi_type(self, space):\n        state = space.fromcache(State)\n        return state.c_ulong", "label": 1}
{"function": "    def _wrap_object(self, space, obj):\n        return space.newlong_from_rarith_int(obj)", "label": 1}
{"function": "    def _unwrap_object(self, space, w_obj):\n        return space.r_longlong_w(w_obj)", "label": 1}
{"function": "    def cffi_type(self, space):\n        state = space.fromcache(State)\n        return state.c_llong", "label": 1}
{"function": "    def _wrap_object(self, space, obj):\n        return space.newlong_from_rarith_int(obj)", "label": 1}
{"function": "    def getAdhesionArea(self) -> Optional[Polygon]:\n        \"\"\"The polygon representing the 2D adhesion area.\n\n        If no adhesion is used, the regular convex hull is returned\n        \"\"\"\n        if self._node is None:\n            return None\n\n        hull = self._compute2DConvexHull()\n        if hull is None:\n            return None\n\n        return self._add2DAdhesionMargin(hull)", "label": 1}
{"function": "    def getConvexHull(self) -> Optional[Polygon]:\n        \"\"\"Get the unmodified 2D projected convex hull of the node (if any)\n\n        In case of one-at-a-time, this includes adhesion and head+fans clearance\n        \"\"\"\n        if self._node is None:\n            return None\n        if self._node.callDecoration(\"isNonPrintingMesh\"):\n            return None\n\n        # Parent can be None if node is just loaded.\n        if self._isSingularOneAtATimeNode():\n            hull = self.getConvexHullHeadFull()\n            if hull is None:\n                return None\n            hull = self._add2DAdhesionMargin(hull)\n            return hull\n\n        return self._compute2DConvexHull()", "label": 1}
{"function": "    def getConvexHullHeadFull(self) -> Optional[Polygon]:\n        \"\"\"For one at the time this is the convex hull of the node with the full head size\n\n        In case of printing all at once this is None.\n        \"\"\"\n        if self._node is None:\n            return None\n\n        if self._isSingularOneAtATimeNode():\n            return self._compute2DConvexHeadFull()\n\n        return None", "label": 1}
{"function": "    def hasGroupAsParent(node: \"SceneNode\") -> bool:\n        parent = node.getParent()\n        if parent is None:\n            return False\n        return bool(parent.callDecoration(\"isGroup\"))", "label": 1}
{"function": "    def getConvexHullHead(self) -> Optional[Polygon]:\n        \"\"\"Get convex hull of the object + head size\n\n        In case of printing all at once this is None.\n        For one at the time this is area with intersection of mirrored head\n        \"\"\"\n        if self._node is None:\n            return None\n        if self._node.callDecoration(\"isNonPrintingMesh\"):\n            return None\n        if self._isSingularOneAtATimeNode():\n            head_with_fans = self._compute2DConvexHeadMin()\n            if head_with_fans is None:\n                return None\n            head_with_fans_with_adhesion_margin = self._add2DAdhesionMargin(head_with_fans)\n            return head_with_fans_with_adhesion_margin\n        return None", "label": 1}
{"function": "    def getConvexHullBoundary(self) -> Optional[Polygon]:\n        \"\"\"Get convex hull of the node\n\n        In case of printing all at once this None??\n        For one at the time this is the area without the head.\n        \"\"\"\n        if self._node is None:\n            return None\n\n        if self._node.callDecoration(\"isNonPrintingMesh\"):\n            return None\n\n        if self._isSingularOneAtATimeNode():\n            # Printing one at a time and it's not an object in a group\n            return self._compute2DConvexHull()\n        return None", "label": 1}
{"function": "    def getPrintingArea(self) -> Optional[Polygon]:\n        \"\"\"Get the buildplate polygon where will be printed\n\n        In case of printing all at once this is the same as convex hull (no individual adhesion)\n        For one at the time this includes the adhesion area\n        \"\"\"\n        if self._isSingularOneAtATimeNode():\n            # In one-at-a-time mode, every printed object gets it's own adhesion\n            printing_area = self.getAdhesionArea()\n        else:\n            printing_area = self.getConvexHull()\n        return printing_area", "label": 1}
{"function": "    def recomputeConvexHullDelayed(self) -> None:\n        \"\"\"The same as recomputeConvexHull, but using a timer if it was set.\"\"\"\n        if self._recompute_convex_hull_timer is not None:\n            self._recompute_convex_hull_timer.start()\n        else:\n            from cura.CuraApplication import CuraApplication\n            if not self._timer_scheduled_to_be_created:\n                # The timer is not created and we never scheduled it. Time to create it now!\n                CuraApplication.getInstance().callLater(self.createRecomputeConvexHullTimer)\n            # Now we know for sure that the timer has been scheduled for creation, so we can try this again.\n            CuraApplication.getInstance().callLater(self.recomputeConvexHullDelayed)", "label": 1}
{"function": "    def recomputeConvexHull(self) -> None:\n        if self._node is None or not self.__isDescendant(self._root, self._node):\n            if self._convex_hull_node:\n                # Convex hull node still exists, but the node is removed or no longer in the scene.\n                self._convex_hull_node.setParent(None)\n                self._convex_hull_node = None\n            return\n\n        if self._convex_hull_node:\n            self._convex_hull_node.setParent(None)\n        hull_node = ConvexHullNode.ConvexHullNode(self._node, self.getPrintingArea(), self._raft_thickness, self._root)\n        self._convex_hull_node = hull_node", "label": 1}
{"function": "    def _onSettingValueChanged(self, key: str, property_name: str) -> None:\n        if property_name != \"value\":  # Not the value that was changed.\n            return\n\n        if key in self._affected_settings:\n            self._onChanged()\n        if key in self._influencing_settings:\n            self._init2DConvexHullCache()  # Invalidate the cache.\n            self._onChanged()", "label": 1}
{"function": "    def __repr__(self) -> str: ...", "label": 1}
{"function": "    def __eq__(self, other: Any) -> bool: ...", "label": 1}
{"function": "    def __le__(self, other: Any) -> bool: ...", "label": 1}
{"function": "    def __lt__(self, other: Any) -> bool: ...", "label": 1}
{"function": "    def __ge__(self, other: Any) -> bool: ...", "label": 1}
{"function": "    def __gt__(self, other: Any) -> bool: ...", "label": 1}
{"function": "    def __hash__(self) -> int: ...", "label": 1}
{"function": "    def __truediv__(self, name: str) -> URL: ...", "label": 1}
{"function": "    def __mod__(self, query: _Query) -> URL: ...", "label": 1}
{"function": "    def is_absolute(self) -> bool: ...", "label": 1}
{"function": "    def __eq__(self, other):\n        if type(other) != type(self):\n            return False\n        return (self.display_name == other.display_name and\n                self.addresses == other.addresses)", "label": 1}
{"function": "    def __new__(cls, name, value):\n        kwds = {'defects': []}\n        cls.parse(value, kwds)\n        if utils._has_surrogates(kwds['decoded']):\n            kwds['decoded'] = utils._sanitize(kwds['decoded'])\n        self = str.__new__(cls, kwds['decoded'])\n        # del kwds['decoded']\n        self.init(name, **kwds)\n        return self", "label": 1}
{"function": "    def init(self, name, **_3to2kwargs):\n        defects = _3to2kwargs['defects']; del _3to2kwargs['defects']\n        parse_tree = _3to2kwargs['parse_tree']; del _3to2kwargs['parse_tree']\n        self._name = name\n        self._parse_tree = parse_tree\n        self._defects = defects", "label": 1}
{"function": "    def name(self):\n        return self._name", "label": 1}
{"function": "    def defects(self):\n        return tuple(self._defects)", "label": 1}
{"function": "    def __reduce__(self):\n        return (\n            _reconstruct_header,\n            (\n                self.__class__.__name__,\n                self.__class__.__bases__,\n                str(self),\n            ),\n            self.__dict__)", "label": 1}
{"function": "    def _reconstruct(cls, value):\n        return str.__new__(cls, value)", "label": 1}
{"function": "    def fold(self, **_3to2kwargs):\n        policy = _3to2kwargs['policy']; del _3to2kwargs['policy']\n        \"\"\"Fold header according to policy.\n\n        The parsed representation of the header is folded according to\n        RFC5322 rules, as modified by the policy.  If the parse tree\n        contains surrogateescaped bytes, the bytes are CTE encoded using\n        the charset 'unknown-8bit\".\n\n        Any non-ASCII characters in the parse tree are CTE encoded using\n        charset utf-8. XXX: make this a policy setting.\n\n        The returned value is an ASCII-only string possibly containing linesep\n        characters, and ending with a linesep character.  The string includes\n        the header name and the ': ' separator.\n\n        \"\"\"\n        # At some point we need to only put fws here if it was in the source.\n        header = parser.Header([\n            parser.HeaderLabel([\n                parser.ValueTerminal(self.name, 'header-name'),\n                parser.ValueTerminal(':', 'header-sep')]),\n            parser.CFWSList([parser.WhiteSpaceTerminal(' ', 'fws')]),\n                             self._parse_tree])\n        return header.fold(policy=policy)", "label": 1}
{"function": "    def parse(cls, value, kwds):\n        kwds['parse_tree'] = cls.value_parser(value)\n        kwds['decoded'] = str(kwds['parse_tree'])", "label": 1}
{"function": "    def parse(cls, value, kwds):\n        if not value:\n            kwds['defects'].append(errors.HeaderMissingRequiredValue())\n            kwds['datetime'] = None\n            kwds['decoded'] = ''\n            kwds['parse_tree'] = parser.TokenList()\n            return\n        if isinstance(value, str):\n            value = utils.parsedate_to_datetime(value)\n        kwds['datetime'] = value\n        kwds['decoded'] = utils.format_datetime(kwds['datetime'])\n        kwds['parse_tree'] = cls.value_parser(kwds['decoded'])", "label": 1}
{"function": "    def mousePressEvent(self, event):\n        \"\"\" Check for left mouse button presses in order to enable drag and\n            drop.\n        \"\"\"\n        if event.button() == Qt.LeftButton:\n            self.dragStartPosition = event.pos()", "label": 1}
{"function": "    def pixmap(self):\n        return self.imageLabel.pixmap()", "label": 1}
{"function": "    def setPixmap(self, pixmap):\n        self.imageLabel.setPixmap(pixmap)\n        self.hasImage = True", "label": 1}
{"function": "    def __init__(self, parent, initialColor, name, mask, labelSize):\n        \"\"\" Initializes the paint color, the mask color (cyan, magenta, or\n        yellow), connects the color selector and invert checkbox to functions,\n        and creates a two-by-two grid layout.\n        \"\"\"\n        super(ScreenWidget, self).__init__(parent)\n\n        self.originalImage = QImage()\n        self.newImage = QImage()\n\n        self.paintColor = initialColor\n        self.maskColor = mask\n        self.inverted = False\n\n        self.imageLabel = QLabel()\n        self.imageLabel.setFrameShadow(QFrame.Sunken)\n        self.imageLabel.setFrameShape(QFrame.StyledPanel)\n        self.imageLabel.setMinimumSize(labelSize)\n\n        self.nameLabel = QLabel(name)\n        self.colorButton = QPushButton(\"Modify...\")\n        self.colorButton.setBackgroundRole(QPalette.Button)\n        self.colorButton.setMinimumSize(32, 32)\n\n        palette = QPalette(self.colorButton.palette())\n        palette.setColor(QPalette.Button, initialColor)\n        self.colorButton.setPalette(palette)\n\n        self.invertButton = QPushButton(\"Invert\")\n        self.invertButton.setEnabled(False)\n\n        self.colorButton.clicked.connect(self.setColor)\n        self.invertButton.clicked.connect(self.invertImage)\n\n        gridLayout = QGridLayout()\n        gridLayout.addWidget(self.imageLabel, 0, 0, 1, 2)\n        gridLayout.addWidget(self.nameLabel, 1, 0)\n        gridLayout.addWidget(self.colorButton, 1, 1)\n        gridLayout.addWidget(self.invertButton, 2, 1, 1, 1)\n        self.setLayout(gridLayout)", "label": 1}
{"function": "    def createImage(self):\n        \"\"\" Creates a new image by separating out the cyan, magenta, or yellow\n            component, depending on the mask color specified in the constructor.\n            The amount of the component found in each pixel of the image is used\n            to determine how much of a user-selected ink is used for each pixel\n            in the new image for the label widget.\n        \"\"\"\n        self.newImage = newImage = self.originalImage.copy()\n\n        # Create CMY components for the ink being used.\n        cyanInk = float(255 - QColor(self.paintColor).red()) / 255.0\n        magentaInk = float(255 - QColor(self.paintColor).green()) / 255.0\n        yellowInk = float(255 - QColor(self.paintColor).blue()) / 255.0\n\n        convert = self.convertMap[self.maskColor]\n\n        for y in range(newImage.height()):\n            for x in range(newImage.width()):\n                p = self.originalImage.pixel(x, y)\n\n                # Separate the source pixel into its cyan component.\n                if self.inverted:\n                    amount = convert(p)\n                else:\n                    amount = 255 - convert(p)\n\n                newColor = QColor(\n                    255 - min(int(amount * cyanInk), 255),\n                    255 - min(int(amount * magentaInk), 255),\n                    255 - min(int(amount * yellowInk), 255))\n\n                newImage.setPixel(x, y, newColor.rgb())\n\n        self.imageLabel.setPixmap(QPixmap.fromImage(newImage))", "label": 1}
{"function": "    def image(self):\n        \"\"\" Returns a reference to the modified image. \"\"\"\n        return self.newImage", "label": 1}
{"function": "    def invertImage(self):\n        \"\"\" Sets whether the amount of ink applied to the canvas is to be\n            inverted (subtracted from the maximum value) before the ink is\n            applied.\n        \"\"\"\n        self.inverted = not self.inverted\n        self.createImage()\n        self.imageChanged.emit()", "label": 1}
{"function": "    def setColor(self):\n        \"\"\" Separate the current image into cyan, magenta, and yellow\n            components.  Create a representation of how each component might\n            appear when applied to a blank white piece of paper.\n        \"\"\"\n        newColor = QColorDialog.getColor(self.paintColor)\n\n        if newColor.isValid():\n            self.paintColor = newColor\n            palette = QPalette(self.colorButton.palette())\n            palette.setColor(QPalette.Button, self.paintColor)\n            self.colorButton.setPalette(palette)\n            self.createImage()\n            self.imageChanged.emit()", "label": 1}
{"function": "    def setImage(self, image):\n        \"\"\" Records the original image selected by the user, creates a color\n            separation, and enables the invert image checkbox.\n        \"\"\"\n        self.originalImage = image\n        self.createImage()\n        self.invertButton.setEnabled(True)", "label": 1}
{"function": "    def __init__(self):\n        \"\"\" Constructor initializes a default value for the brightness, creates\n            the main menu entries, and constructs a central widget that contains\n            enough space for images to be displayed.\n        \"\"\"\n        super(Viewer, self).__init__()\n\n        self.scaledImage = QImage()\n        self.menuMap = {}\n        self.path = ''\n        self.brightness = 255\n\n        self.setWindowTitle(\"QImage Color Separations\")\n\n        self.createMenus()\n        self.setCentralWidget(self.createCentralWidget())", "label": 1}
{"function": "    def close(self):\n        \"\"\"Immediately unwind the context stack\"\"\"\n        self.__exit__(None, None, None)", "label": 1}
{"function": "    def __enter__(self):\n        return self", "label": 1}
{"function": "    def __exit__(self, *exc_details):\n        received_exc = exc_details[0] is not None\n\n        # We manipulate the exception state so it behaves as though\n        # we were actually nesting multiple with statements\n        frame_exc = sys.exc_info()[1]\n        _fix_exception_context = _make_context_fixer(frame_exc)\n\n        # Callbacks are invoked in LIFO order to match the behaviour of\n        # nested context managers\n        suppressed_exc = False\n        pending_raise = False\n        while self._exit_callbacks:\n            cb = self._exit_callbacks.pop()\n            try:\n                if cb(*exc_details):\n                    suppressed_exc = True\n                    pending_raise = False\n                    exc_details = (None, None, None)\n            except:\n                new_exc_details = sys.exc_info()\n                # simulate the stack of exceptions by setting the context\n                _fix_exception_context(new_exc_details[1], exc_details[1])\n                pending_raise = True\n                exc_details = new_exc_details\n        if pending_raise:\n            _reraise_with_existing_context(exc_details)\n        return received_exc and suppressed_exc", "label": 1}
{"function": "    def __init__(self):\n        warnings.warn(\"ContextStack has been renamed to ExitStack\",\n                      DeprecationWarning)\n        super(ContextStack, self).__init__()", "label": 1}
{"function": "    def register_exit(self, callback):\n        return self.push(callback)", "label": 1}
{"function": "    def register(self, callback, *args, **kwds):\n        return self.callback(callback, *args, **kwds)", "label": 1}
{"function": "    def preserve(self):\n        return self.pop_all()", "label": 1}
{"function": "    def __init__(self, enter_result=None):\n        self.enter_result = enter_result", "label": 1}
{"function": "    def __enter__(self):\n        return self.enter_result", "label": 1}
{"function": "    def __exit__(self, *excinfo):\n        pass", "label": 1}
{"function": "    def end_before(self, key=None, **kwargs):\n        \"\"\"End document before this point\"\"\"\n        if key:\n            self._end_before = self._firestore_doc(key)\n        else:\n            self._end_before = self._fields_by_column_name(**kwargs)\n        return self", "label": 1}
{"function": "    def end_at(self, key=None, **kwargs):\n        \"\"\"End document at this point\"\"\"\n        if key:\n            self._end_at = self._firestore_doc(key)\n        else:\n            self._end_at = self._fields_by_column_name(**kwargs)\n        return self", "label": 1}
{"function": "    def filter(self, *args, **kwargs):\n        \"\"\"Apply filter for querying document\n\n        Apply where filter as many as you want\n\n        Parameters\n        ---------\n        args: Tuple\n            Contain three things 1- field name, 2-operation, 3-value\n\n        kwargs:\n            keyword args Direct assign for equal filter\n\n        Returns\n        -------\n        self:\n            Return self object\n        \"\"\"\n        if args:\n            self.select_query.append(args)\n        elif kwargs:\n            for k, v in kwargs.items():\n                self.select_query.append((k, '==', v))\n        return self", "label": 1}
{"function": "    def limit(self, count):\n        \"\"\"Apply limit for query\"\"\"\n        # save the Limit in cursor for next fetch\n        self.cursor_dict['limit'] = count\n\n        if count:\n            self.n_limit = count\n        return self", "label": 1}
{"function": "    def offset(self, num_to_skip):\n        \"\"\"Offset for query\"\"\"\n        self._offset = num_to_skip\n        return self", "label": 1}
{"function": "    def order(self, field_name):\n        \"\"\"Order document by field name\n\n        By default, a query retrieves all documents that satisfy the query in ascending order by document ID.\n        You can specify the sort order for your data using `order()`, and you can limit the number of documents\n        retrieved using `limit()`\n\n        Put a dash(-) in front of field name if you want to sort it in descending order. You can also combine\n        filter with order\n\n        Parameters\n        ----------\n        field_name : str\n            Name of the field on which base order is applied\n\n        Returns\n        -------\n            Self object\n        \"\"\"\n        # Save order in cursor dict for next fetch\n        if 'order' in self.cursor_dict:\n            self.cursor_dict['order'] = self.cursor_dict['order'] + ',' + field_name\n        else:\n            self.cursor_dict['order'] = field_name\n\n        order_direction = 'Asc'\n        name = field_name\n\n        # If this is in Desc order\n        if field_name[0] == '-':\n            order_direction = 'Desc'\n            name = field_name[1:]  # Get the field name after dash(-) e.g -age name will be age\n        f_name = self.model._meta.get_field(name).db_column_name\n        self.order_by.append((f_name, order_direction))\n        return self", "label": 1}
{"function": "    def fetch(self, limit=None):\n        \"\"\"Fetch the result from firestore\n\n        Parameters\n        ---------\n        limit : optional\n            Apply limit to firestore documents, how much documents you want to retrieve\n        \"\"\"\n        # save the Limit in cursor for next fetch\n        self.cursor_dict['limit'] = limit\n\n        if limit:\n            self.n_limit = limit\n        return QueryIterator(self)", "label": 1}
{"function": "    def group_fetch(self, limit=None):\n        super().set_group_collection(True)\n        return self.fetch(limit)", "label": 1}
{"function": "    def get(self):\n        \"\"\"Get the first matching document from firestore\n\n        Get first matching document and convert it into model and return it\n        This is same as `fetch(limit=1)` the only difference is `get()` method\n        return **model instance** and the `fetch()` method return the **generator**\n        \"\"\"\n        self.n_limit = 1\n        doc = next(self.query().stream(self.query_transaction), None)\n        if doc:\n            m = query_wrapper.ModelWrapper.from_query_result(self.model, doc)\n            m._update_doc = self._update_doc_key(m)\n            return m\n        return None", "label": 1}
{"function": "    def delete(self, child=False):\n        \"\"\"Delete the filter documents\n\n        if child is True then delete nested collection and documents also\n        \"\"\"\n        transaction_or_batch = self.query_transaction if self.query_transaction else self.query_batch\n        q = self.query()\n        DeleteQuery(self.model, query=q, child=child).exec(transaction_or_batch)", "label": 1}
{"function": "    def valDataCenter(self, obj):\n        \"\"\"Function to add DataCenter\n\n        :param obj: element to add DataCenter\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retDataCenter())\n        for i in set(values):\n            dci = self.ElementTree.SubElement(obj, 'DataCenterId')\n            dci.text = i", "label": 1}
{"function": "    def valGranuleUR(self, obj):\n        \"\"\"Function to add GranuleUR\n\n        :param obj: element to add GranuleUR\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retGranuleUR())\n        for i in set(values):\n            gur = self.ElementTree.SubElement(obj, 'GranuleUR')\n            gur.text = i", "label": 1}
{"function": "    def valDbID(self, obj):\n        \"\"\"Function to add DbID\n\n        :param obj: element to add DbID\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retDbID())\n        for i in set(values):\n            dbid = self.ElementTree.SubElement(obj, 'DbID')\n            dbid.text = i", "label": 1}
{"function": "    def valInsTime(self, obj):\n        \"\"\"Function to add the minimum of InsertTime\n\n        :param obj: element to add InsertTime\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retInsertTime())\n        obj.text = self._minval(values)", "label": 1}
{"function": "    def valCollectionMetaData(self, obj):\n        \"\"\"Function to add CollectionMetaData\n\n        :param obj: element to add CollectionMetaData\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retCollectionMetaData())\n        self._cicle_values(obj, self._checkvaldict(values))", "label": 1}
{"function": "    def valDataFiles(self, obj):\n        \"\"\"Function to add DataFileContainer\n\n        :param obj: element to add DataFileContainer\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retDataFiles())\n        for i in values:\n            dfc = self.ElementTree.SubElement(obj, 'DataFileContainer')\n            self._cicle_values(dfc, i)", "label": 1}
{"function": "    def valPGEVersion(self, obj):\n        \"\"\"Function to add PGEVersion\n\n        :param obj: element to add PGEVersion\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retPGEVersion())\n        for i in set(values):\n            pge = self.ElementTree.SubElement(obj, 'PGEVersion')\n            pge.text = i", "label": 1}
{"function": "    def valRangeTime(self, obj):\n        \"\"\"Function to add RangeDateTime\n\n        :param obj: element to add RangeDateTime\n        \"\"\"\n        values = []\n        for i in self.parModis:\n            values.append(i.retRangeTime())\n        self._cicle_values(obj, self._checkvaldict(values))", "label": 1}
{"function": "    def valBound(self):\n        \"\"\"Function return the Bounding Box of mosaic\"\"\"\n        boundary = self.parModis[0].retBoundary()\n        for i in range(1, len(self.parModis)):\n            bound = self.parModis[i].retBoundary()\n            if bound['min_lat'] < boundary['min_lat']:\n                boundary['min_lat'] = bound['min_lat']\n            if bound['min_lon'] < boundary['min_lon']:\n                boundary['min_lon'] = bound['min_lon']\n            if bound['max_lat'] > boundary['max_lat']:\n                boundary['max_lat'] = bound['max_lat']\n            if bound['max_lon'] > boundary['max_lon']:\n                boundary['max_lon'] = bound['max_lon']\n        self.boundary = boundary", "label": 1}
{"function": "    def valMeasuredParameter(self, obj):\n        \"\"\"Function to add ParameterName\n\n        :param obj: element to add ParameterName\n        \"\"\"\n        valuesQAStats = []\n        valuesQAFlags = []\n        valuesParameter = []\n        for i in self.parModis:\n            for val in i.retMeasure().values():\n                valuesQAStats.append(val['QAStats'])\n                valuesQAFlags.append(val['QAFlags'])\n                valuesParameter.append(val['ParameterName'])\n        for i in set(valuesParameter):\n            pn = self.ElementTree.SubElement(obj, 'ParameterName')\n            pn.text = i", "label": 1}
{"function": "    def name(self):\n        \"\"\"Return the name of the climate device.\"\"\"\n        return self._name", "label": 1}
{"function": "    def temperature_unit(self):\n        \"\"\"Return the unit of measurement.\"\"\"\n        return self._unit_of_measurement", "label": 1}
{"function": "    def current_temperature(self):\n        \"\"\"Return the current temperature.\"\"\"\n        return self._current_temperature", "label": 1}
{"function": "    def target_temperature(self):\n        \"\"\"Return the temperature we try to reach.\"\"\"\n        return self._target_temperature", "label": 1}
{"function": "    def target_temperature_high(self):\n        \"\"\"Return the highbound target temperature we try to reach.\"\"\"\n        return self._target_temperature_high", "label": 1}
{"function": "    def target_temperature_low(self):\n        \"\"\"Return the lowbound target temperature we try to reach.\"\"\"\n        return self._target_temperature_low", "label": 1}
{"function": "    def current_humidity(self):\n        \"\"\"Return the current humidity.\"\"\"\n        return self._current_humidity", "label": 1}
{"function": "    def target_humidity(self):\n        \"\"\"Return the humidity we try to reach.\"\"\"\n        return self._target_humidity", "label": 1}
{"function": "    def hvac_action(self):\n        \"\"\"Return current operation ie. heat, cool, idle.\"\"\"\n        return self._hvac_action", "label": 1}
{"function": "    def hvac_mode(self):\n        \"\"\"Return hvac target hvac state.\"\"\"\n        return self._hvac_mode", "label": 1}
{"function": "    def rule_schema_version(self):\n        \"\"\"Returns the schema version of the rules.\n\n        Returns:\n            int. The schema version of the rules.\n        \"\"\"\n        return self._rule_schema_version", "label": 1}
{"function": "    def default_value(self):\n        \"\"\"Returns the default value of the platform parameter.\n\n        Returns:\n            *. The default value of the platform parameter.\n        \"\"\"\n        return self._default_value", "label": 1}
{"function": "    def is_feature(self):\n        \"\"\"Returns whether this parameter is also a feature flag.\n\n        Returns:\n            bool. True if the parameter is a feature flag.\n        \"\"\"\n        return self._is_feature", "label": 1}
{"function": "    def feature_stage(self):\n        \"\"\"Returns the stage of the feature flag.\n\n        Returns:\n            FEATURE_STAGES|None. The stage of the feature flag, None if the\n            parameter isn't a feature flag.\n        \"\"\"\n        return self._feature_stage", "label": 1}
{"function": "    def validate(self):\n        \"\"\"Validates the PlatformParameter domain object.\"\"\"\n        if re.match(self.PARAMETER_NAME_REGEXP, self._name) is None:\n            raise utils.ValidationError(\n                'Invalid parameter name \\'%s\\', expected to match regexp '\n                '%s.' % (self._name, self.PARAMETER_NAME_REGEXP))\n\n        if self._data_type not in self.DATA_TYPE_PREDICATES_DICT:\n            raise utils.ValidationError(\n                'Unsupported data type \\'%s\\'.' % self._data_type)\n\n        predicate = self.DATA_TYPE_PREDICATES_DICT[self.data_type]\n        if not predicate(self._default_value):\n            raise utils.ValidationError(\n                'Expected %s, received \\'%s\\' in default value.' % (\n                    self._data_type, self._default_value))\n        for rule in self._rules:\n            if not predicate(rule.value_when_matched):\n                raise utils.ValidationError(\n                    'Expected %s, received \\'%s\\' in value_when_matched.' % (\n                        self._data_type, rule.value_when_matched))\n            if not rule.has_server_mode_filter():\n                raise utils.ValidationError(\n                    'All rules must have a server_mode filter.')\n            rule.validate()\n\n        if self._is_feature:\n            self._validate_feature_flag()", "label": 1}
{"function": "    def evaluate(self, context):\n        \"\"\"Evaluates the value of the platform parameter in the given context.\n        The value of first matched rule is returned as the result.\n\n        Args:\n            context: EvaluationContext. The context for evaluation.\n\n        Returns:\n            *. The evaluate result of the platform parameter.\n        \"\"\"\n        for rule in self._rules:\n            if rule.evaluate(context):\n                return rule.value_when_matched\n        return self._default_value", "label": 1}
{"function": "    def to_dict(self):\n        \"\"\"Returns a dict representation of the PlatformParameter domain\n        object.\n\n        Returns:\n            dict. A dict mapping of all fields of PlatformParameter object.\n        \"\"\"\n        return {\n            'name': self._name,\n            'description': self._description,\n            'data_type': self._data_type,\n            'rules': [rule.to_dict() for rule in self._rules],\n            'rule_schema_version': self._rule_schema_version,\n            'default_value': self._default_value,\n            'is_feature': self._is_feature,\n            'feature_stage': self._feature_stage\n        }", "label": 1}
{"function": "    def _validate_feature_flag(self):\n        \"\"\"Validates the PlatformParameter domain object that is a feature\n        flag.\n        \"\"\"\n        if self._data_type != DATA_TYPES.bool:\n            raise utils.ValidationError(\n                'Data type of feature flags must be bool, got \\'%s\\' '\n                'instead.' % self._data_type)\n        if self._feature_stage not in ALLOWED_FEATURE_STAGES:\n            raise utils.ValidationError(\n                'Invalid feature stage, got \\'%s\\', expected one of %s.' % (\n                    self._feature_stage, ALLOWED_FEATURE_STAGES))\n\n        enabling_rules = [\n            rule for rule in self._rules if rule.value_when_matched]\n        for rule in enabling_rules:\n            server_mode_filters = [\n                server_mode_filter for server_mode_filter in rule.filters\n                if server_mode_filter.type == 'server_mode']\n            for server_mode_filter in server_mode_filters:\n                server_modes = [\n                    value for _, value in server_mode_filter.conditions]\n                if self._feature_stage == FEATURE_STAGES.dev:\n                    if (\n                            SERVER_MODES.test in server_modes or\n                            SERVER_MODES.prod in server_modes):\n                        raise utils.ValidationError(\n                            'Feature in dev stage cannot be enabled in test or'\n                            ' production environments.')\n                elif self._feature_stage == FEATURE_STAGES.test:\n                    if SERVER_MODES.prod in server_modes:\n                        raise utils.ValidationError(\n                            'Feature in test stage cannot be enabled in '\n                            'production environment.')", "label": 1}
{"function": "    def from_dict(cls, param_dict):\n        \"\"\"Returns an PlatformParameter object from a dict.\n\n        Args:\n            param_dict: dict. A dict mapping of all fields of\n                PlatformParameter object.\n\n        Returns:\n            PlatformParameter. The corresponding PlatformParameter domain\n            object.\n        \"\"\"\n        if (param_dict['rule_schema_version'] !=\n                feconf.CURRENT_PLATFORM_PARAMETER_RULE_SCHEMA_VERSION):\n            # NOTE: When there's a new rule schema version, a new method with\n            # name of the form '_convert_rule_v1_dict_to_v2_dict` should be\n            # added to the class and called here to convert the rule dicts to\n            # the latest schema.\n            raise Exception(\n                'Current platform parameter rule schema version is v%s, '\n                'received v%s, and there\\'s no convert method from v%s to '\n                'v%s.' % (\n                    feconf.CURRENT_PLATFORM_PARAMETER_RULE_SCHEMA_VERSION,\n                    param_dict['rule_schema_version'],\n                    feconf.CURRENT_PLATFORM_PARAMETER_RULE_SCHEMA_VERSION,\n                    param_dict['rule_schema_version']))\n\n        return cls(\n            param_dict['name'],\n            param_dict['description'],\n            param_dict['data_type'],\n            [\n                PlatformParameterRule.from_dict(rule_dict)\n                for rule_dict in param_dict['rules']],\n            param_dict['rule_schema_version'],\n            param_dict['default_value'],\n            param_dict['is_feature'],\n            param_dict['feature_stage'],\n        )", "label": 1}
{"function": "    def serialize(self):\n        \"\"\"Returns the object serialized as a JSON string.\n\n        Returns:\n            str. JSON-encoded string encoding all of the information composing\n            the object.\n        \"\"\"\n        platform_parameter_dict = self.to_dict()\n\n        return json.dumps(platform_parameter_dict)", "label": 1}
{"function": "    def files(self):\n        if len(self.metapartitions) > 1:\n            raise AttributeError(\n                \"Accessing `files` attribute is not allowed while nested\"\n            )\n        return self.metapartitions[0][\"files\"]", "label": 1}
{"function": "    def is_sentinel(self):\n        return len(self.metapartitions) == 1 and self.label is None", "label": 1}
{"function": "    def label(self):\n        if len(self.metapartitions) > 1:\n            raise AttributeError(\n                \"Accessing `label` attribute is not allowed while nested\"\n            )\n        assert isinstance(self.metapartitions[0], dict), self.metapartitions[0]\n        return self.metapartitions[0][\"label\"]", "label": 1}
{"function": "    def indices(self):\n        if len(self.metapartitions) > 1:\n            raise AttributeError(\n                \"Accessing `indices` attribute is not allowed while nested\"\n            )\n        return self.metapartitions[0][\"indices\"]", "label": 1}
{"function": "    def tables(self):\n        return list(set(self.data.keys()).union(set(self.files.keys())))", "label": 1}
{"function": "    def partition(self):\n        return Partition(label=self.label, files=self.files)", "label": 1}
{"function": "    def __eq__(self, other):\n        if not isinstance(other, MetaPartition):\n            return False\n\n        if self.metadata_version != other.metadata_version:\n            return False\n\n        for table, meta in self.table_meta.items():\n            # https://issues.apache.org/jira/browse/ARROW-5873\n            other_meta = other.table_meta.get(table, None)\n            if other_meta is None:\n                return False\n            if not meta.equals(other_meta):\n                return False\n\n        if self.dataset_metadata != other.dataset_metadata:\n            return False\n\n        if len(self.metapartitions) != len(other.metapartitions):\n            return False\n\n        # In the case both MetaPartitions are nested, we need to ensure a match\n        # for all sub-partitions.\n        # Since the label is unique, this can be used as a distinguishing key to sort and compare\n        # the nested metapartitions.\n        if len(self.metapartitions) > 1:\n            for mp_self, mp_other in zip(\n                sorted(self.metapartitions, key=lambda x: x[\"label\"]),\n                sorted(other.metapartitions, key=lambda x: x[\"label\"]),\n            ):\n                if mp_self == mp_other:\n                    continue\n                # If a single metapartition does not match, the whole object is considered different\n                return False\n            return True\n\n        # This is unnested only\n\n        self_keys = set(self.data.keys())\n        other_keys = set(other.data.keys())\n        if not (self_keys == other_keys):\n            return False\n\n        if self.label != other.label:\n            return False\n\n        if self.files != other.files:\n            return False\n\n        for label, df in self.data.items():\n            if not (df.equals(other.data[label])):\n                return False\n\n        return True", "label": 1}
{"function": "    def from_partition(\n        partition,\n        data=None,\n        dataset_metadata=None,\n        indices=None,\n        metadata_version=None,\n        table_meta=None,\n        partition_keys=None,\n        logical_conjunction=None,\n    ):\n        \"\"\"\n        Transform a kartothek :class:`~kartothek.core.partition.Partition` into a\n        :class:`~kartothek.io_components.metapartition.MetaPartition`.\n\n        Parameters\n        ----------\n        partition : :class:`~kartothek.core.partition.Partition`\n            The kartothek partition to be wrapped\n        data : dict, optional\n            A dictionaries with materialised :class:`~pandas.DataFrame`\n        dataset_metadata : dict of basestring, optional\n            The metadata of the original dataset\n        indices : dict\n            The index dictionary of the dataset\n        table_meta: Union[None, Dict[String, pyarrow.Schema]]\n            Type metadata for each table, optional\n        metadata_version: int, optional\n        partition_keys: Union[None, List[String]]\n            A list of the primary partition keys\n        Returns\n        -------\n        :class:`~kartothek.io_components.metapartition.MetaPartition`\n        \"\"\"\n        return MetaPartition(\n            label=partition.label,\n            files=partition.files,\n            data=data,\n            dataset_metadata=dataset_metadata,\n            indices=indices,\n            metadata_version=metadata_version,\n            table_meta=table_meta,\n            partition_keys=partition_keys,\n            logical_conjunction=logical_conjunction,\n        )", "label": 1}
{"function": "    def add_metapartition(\n        self, metapartition, metadata_merger=None, schema_validation=True\n    ):\n        \"\"\"\n        Adds a metapartition to the internal list structure to enable batch processing.\n\n        The top level `dataset_metadata` dictionary is combined with the existing dict and\n        all other attributes are stored in the `metapartitions` list\n\n        Parameters\n        ----------\n        metapartition: [MetaPartition]\n            The MetaPartition to be added.\n        metadata_merger: [callable]\n            A callable to perform the metadata merge. By default [kartothek.io_components.utils.combine_metadata] is used\n        schema_validation : [bool]\n            If True (default), ensure that the `table_meta` of both `MetaPartition` objects are the same\n        \"\"\"\n        if self.is_sentinel:\n            return metapartition\n\n        table_meta = metapartition.table_meta\n        existing_label = [mp_[\"label\"] for mp_ in self.metapartitions]\n\n        if any(\n            [mp_[\"label\"] in existing_label for mp_ in metapartition.metapartitions]\n        ):\n            raise RuntimeError(\n                \"Duplicate labels for nested metapartitions are not allowed!\"\n            )\n\n        if schema_validation:\n            table_meta = {}\n            for table, meta in self.table_meta.items():\n                other = metapartition.table_meta.get(table, None)\n                # This ensures that only schema-compatible metapartitions can be nested\n                # The returned schema by validate_compatible is the reference schema with the most\n                # information, i.e. the fewest null columns\n                table_meta[table] = validate_compatible([meta, other])\n\n        metadata_merger = metadata_merger or combine_metadata\n        new_dataset_metadata = metadata_merger(\n            [self.dataset_metadata, metapartition.dataset_metadata]\n        )\n\n        new_object = MetaPartition(\n            label=\"NestedMetaPartition\",\n            dataset_metadata=new_dataset_metadata,\n            metadata_version=metapartition.metadata_version,\n            table_meta=table_meta,\n            partition_keys=metapartition.partition_keys or None,\n            logical_conjunction=metapartition.logical_conjunction or None,\n        )\n\n        # Add metapartition information to the new object\n        new_metapartitions = self.metapartitions.copy()\n        new_metapartitions.extend(metapartition.metapartitions.copy())\n        new_object.metapartitions = new_metapartitions\n\n        return new_object", "label": 1}
{"function": "    def from_dict(dct):\n        \"\"\"\n        Create a :class:`~kartothek.io_components.metapartition.MetaPartition` from a dictionary.\n\n        Parameters\n        ----------\n        dct : dict\n            Dictionary containing constructor arguments as keys\n\n        Returns\n        -------\n\n        \"\"\"\n        return MetaPartition(\n            label=dct[\"label\"],\n            files=dct.get(\"files\", {}),\n            metadata=dct.get(\"metadata\", {}),\n            data=dct.get(\"data\", {}),\n            indices=dct.get(\"indices\", {}),\n            metadata_version=dct.get(\"metadata_version\", None),\n            dataset_metadata=dct.get(\"dataset_metadata\", {}),\n            table_meta=dct.get(\"table_meta\", {}),\n            partition_keys=dct.get(\"partition_keys\", None),\n            logical_conjunction=dct.get(\"logical_conjunction\", None),\n        )", "label": 1}
{"function": "    def __init__(self, *args, **kwargs):\n        super(FormForm, self).__init__(*args, **kwargs)\n\n        if not self.user.profile.is_superuser:\n            if 'status_detail' in self.fields:\n                self.fields.pop('status_detail')", "label": 1}
{"function": "    def clean_slug(self):\n        slug = slugify(self.cleaned_data['slug'])\n        i = 0\n        while True:\n            if i > 0:\n                if i > 1:\n                    slug = slug.rsplit(\"-\", 1)[0]\n                slug = \"%s-%s\" % (slug, i)\n            match = Form.objects.filter(slug=slug)\n            if self.instance:\n                match = match.exclude(pk=self.instance.pk)\n            if not match:\n                break\n            i += 1\n        return slug", "label": 1}
{"function": "    def clean(self):\n        cleaned_data = self.cleaned_data\n        field_function = cleaned_data.get(\"field_function\")\n        choices = cleaned_data.get(\"choices\")\n        field_type = cleaned_data.get(\"field_type\")\n        #required = cleaned_data.get(\"required\")\n        visible = cleaned_data.get(\"visible\")\n\n        if field_function == \"GroupSubscription\":\n            if field_type != \"BooleanField\":\n                raise forms.ValidationError(_(\"This field's function requires Checkbox as a field type\"))\n            if not choices:\n                raise forms.ValidationError(_(\"This field's function requires at least 1 group specified.\"))\n            else:\n                for val in choices.split(','):\n                    try:\n                        g = Group.objects.get(name=val.strip())\n                        if not g.allow_self_add:\n                            raise forms.ValidationError(_(\"The group \\\"%(val)s\\\" does not allow self-add.\" % { 'val' : val }))\n                    except Group.DoesNotExist:\n                        raise forms.ValidationError(_(\"The group \\\"%(val)s\\\" does not exist\" % { 'val' : val }))\n\n        if field_function == \"GroupSubscriptionAuto\":\n            # field_type doesn't matter since this field shouldn't be rendered anyway.\n            if visible:\n                raise forms.ValidationError(_(\"This field must not be visible to users.\"))\n            if not choices:\n                raise forms.ValidationError(_(\"This field's function requires at least 1 group specified.\"))\n            else:\n                for val in choices.split(','):\n                    try:\n                        g = Group.objects.get(name=val.strip())\n                        if not g.allow_self_add:\n                            raise forms.ValidationError(_(\"The group \\\"%(val)s\\\" does not allow self-add.\" % { 'val' : val }))\n                    except Group.DoesNotExist:\n                        raise forms.ValidationError(_(\"The group \\\"%(val)s\\\" does not exist\" % { 'val' : val }))\n\n        if field_function == \"Recipients\":\n            if (field_type != \"MultipleChoiceField/django.forms.CheckboxSelectMultiple\" and\n                field_type != \"MultipleChoiceField\" and\n                field_type != \"BooleanField\" and\n                field_type != \"ChoiceField\"):\n                raise forms.ValidationError(_(\"The \\\"Email to Recipients\\\" function requires Multi-select - Checkboxes \"\n                                            + \"or Multi-select - Select Many as field type\"))\n\n            if field_type == \"BooleanField\":\n                if not choices:\n                    raise forms.ValidationError(_(\"The \\\"Email to Recipients\\\" function requires at least 1 email specified.\"))\n                else:\n                    for val in choices.split(','):\n                        if not validate_email(val.strip()):\n                            raise forms.ValidationError(_(\"\\\"%(val)s\\\" is not a valid email address\" % {'val':val}))\n            else:\n                if not choices:\n                    raise forms.ValidationError(_(\"The \\\"Email to Recipients\\\" function requires at least 1 choice specified.\"))\n                else:\n                    for val in choices.split(','):\n                        val = val.split(':')\n                        if len(val) < 2:\n                            raise forms.ValidationError(_(\"The \\\"Email to Recipients\\\" function requires choices to be in the following format: <choice_label>:<email_address>.\"))\n                        if not validate_email(val[1].strip()):\n                            raise forms.ValidationError(_(\"\\\"%(val)s\\\" is not a valid email address\" % {'val':val[1]}))\n\n        if field_function is not None and field_function.startswith(\"Email\"):\n            if field_type != \"CharField\":\n                raise forms.ValidationError(_(\"This field's function requires Text as a field type\"))\n\n        #unrequire the display only fields\n        if field_type == \"CharField/tendenci.apps.forms_builder.forms.widgets.Description\":\n            cleaned_data['required'] = False\n        elif field_type == \"CharField/tendenci.apps.forms_builder.forms.widgets.Header\":\n            cleaned_data['required'] = False\n\n        return cleaned_data", "label": 1}
{"function": "    def __init__(self, *args, **kwargs):\n        if kwargs.get('empty_permitted', True):\n            kwargs['use_required_attribute'] = False\n        super(PricingForm, self).__init__(*args, **kwargs)\n        # Setup initial values for billing_cycle and billing_dt_select\n        # in order to have empty values for extra forms.\n        if self.instance.pk:\n            self.fields['billing_dt_select'].initial = [self.instance.num_days,\n                                                        self.instance.due_sore]\n            self.fields['billing_cycle'].initial = [self.instance.billing_frequency,\n                                                    self.instance.billing_period]\n        else:\n            self.fields['billing_dt_select'].initial = [0, u'start']\n            self.fields['billing_cycle'].initial = [1, u'month']\n\n        # Add class for recurring payment fields\n        recurring_payment_fields = [\n            'taxable', 'tax_rate', 'billing_cycle', 'billing_dt_select',\n            'has_trial_period', 'trial_period_days'\n        ]\n\n        for field in recurring_payment_fields:\n            class_attr = self.fields[field].widget.attrs.get('class', None)\n            if class_attr and 'recurring-payment' not in class_attr:\n                class_attr += ' recurring-payment'\n\n                self.fields[field].widget.attrs.update({'class': class_attr})", "label": 1}
{"function": "    def clean_tax_rate(self):\n        return self.cleaned_data.get('tax_rate') or 0", "label": 1}
{"function": "    def save(self, **kwargs):\n        pricing = super(PricingForm, self).save(**kwargs)\n        if self.cleaned_data.get('billing_dt_select'):\n            dt_select = self.cleaned_data.get('billing_dt_select').split(',')\n            pricing.num_days = dt_select[0]\n            pricing.due_sore = dt_select[1]\n        if self.cleaned_data.get('billing_cycle'):\n            cycle = self.cleaned_data.get('billing_cycle').split(',')\n            pricing.billing_frequency = cycle[0]\n            pricing.billing_period = cycle[1]\n        #pricing.save()\n        return pricing", "label": 1}
{"function": "        def add_fields(form, form_fields):\n            for field in form_fields:\n                field_key = \"field_%s\" % field.id\n                if \"/\" in field.field_type:\n                    field_class, field_widget = field.field_type.split(\"/\")\n                else:\n                    field_class, field_widget = field.field_type, None\n\n                if field.field_type == 'EmailVerificationField':\n                    one_email = get_setting('module', 'forms', 'one_email')\n                    if one_email:\n                        field_class = forms.EmailField\n                    else:\n                        field_class = EmailVerificationField\n\n                elif field.field_type == 'BooleanField' and len(field.choices) > 0:\n                    field_class = forms.MultipleChoiceField\n                    field_widget = 'django.forms.CheckboxSelectMultiple'\n\n                elif field.field_type == 'CountryField':\n                    field_class = CountrySelectField\n                elif field.field_type == 'StateProvinceField':\n                    field_class = getattr(forms, 'ChoiceField')\n                else:\n                    field_class = getattr(forms, field_class)\n                field_args = {\"label\": mark_safe(field.label), \"required\": field.required}\n                arg_names = field_class.__init__.__code__.co_varnames\n                if \"max_length\" in arg_names:\n                    field_args[\"max_length\"] = FIELD_MAX_LENGTH\n                if \"choices\" in arg_names and field.field_type != 'CountryField':\n                    field_args[\"choices\"] = field.get_choices()\n                    #field_args[\"choices\"] = zip(choices, choices)\n                if \"initial\" in arg_names:\n                    default = field.default.lower()\n                    if field_class == \"BooleanField\":\n                        if default == \"checked\" or default == \"true\" or \\\n                            default == \"on\" or default == \"1\":\n                                default = True\n                        else:\n                            default = False\n                    field_args[\"initial\"] = field.default\n\n                if field_widget is not None:\n                    module, widget = field_widget.rsplit(\".\", 1)\n                    # django.forms.extras moved to django.forms.widgets since Django 1.9\n                    if module == 'django.forms.extras':\n                        module = 'django.forms.widgets'\n                    field_args[\"widget\"] = getattr(import_module(module), widget)\n\n                if field.field_function == 'EmailFirstName':\n                    field_args[\"max_length\"] = FIELD_FNAME_LENGTH\n                elif field.field_function == 'EmailLastName':\n                    field_args[\"max_length\"] = FIELD_LNAME_LENGTH\n                elif field.field_function == 'EmailFullName':\n                    field_args[\"max_length\"] = FIELD_NAME_LENGTH\n                elif field.field_function == 'EmailPhoneNumber':\n                    field_args[\"max_length\"] = FIELD_PHONE_LENGTH\n                elif field.field_type == 'FileField':\n                    field_args[\"validators\"] = [FileValidator()]\n\n                form.fields[field_key] = field_class(**field_args)\n\n                if not field_class == EmailVerificationField:\n                    form.fields[field_key].widget.attrs['title'] = field.label\n                    form.fields[field_key].widget.attrs['class'] = 'formforform-field'\n                else:\n                    form.fields[field_key].widget.widgets[0].attrs['class'] += ' formforform-field'\n                    form.fields[field_key].widget.widgets[1].attrs['class'] += ' formforform-field'\n                widget_name = form.fields[field_key].widget.__class__.__name__.lower()\n                if widget_name == 'selectdatewidget':\n                    form.fields[field_key].widget.years = list(range(1920, THIS_YEAR + 10))\n                if widget_name in ('dateinput', 'selectdatewidget', 'datetimeinput'):\n                    form.fields[field_key].initial = datetime.now()", "label": 1}
{"function": "        def add_pricing_fields(form, formforform):\n            # include pricing options if any\n            if (formforform.custom_payment or formforform.recurring_payment) and formforform.pricing_set.all():\n\n                #currency_symbol = get_setting('site', 'global', 'currencysymbol')\n\n                pricing_options = []\n                for pricing in formforform.pricing_set.all():\n\n                    if pricing.price is None:\n                        pricing_options.append(\n                            (pricing.pk, mark_safe(\n                                '<input type=\"text\" class=\"custom-price\" name=\"custom_price_%s\" value=\"%s\"/> <strong>%s</strong><br>%s' %\n                                (pricing.pk, form.data.get('custom_price_%s' %pricing.pk, str()), pricing.label, pricing.description)))\n                        )\n                    else:\n                        if formforform.recurring_payment:\n                            pricing_options.append(\n                                (pricing.pk, mark_safe('<strong>%s per %s %s - %s</strong><br>%s' %\n                                                        (tcurrency(pricing.price),\n                                                         pricing.billing_frequency, pricing.billing_period,\n                                                         pricing.label, pricing.description)))\n                            )\n                        else:\n                            pricing_options.append(\n                                (pricing.pk, mark_safe('<strong>%s %s</strong><br>%s' %\n                                                       (tcurrency(pricing.price),\n                                                        pricing.label, pricing.description)))\n                            )\n\n                form.fields['pricing_option'] = forms.ChoiceField(\n                    label=formforform.pricing_name or _('Pricing'),\n                    choices = pricing_options,\n                    widget=forms.RadioSelect(attrs={'class': 'pricing-field'})\n                )\n\n                form.fields['payment_option'] = forms.ModelChoiceField(\n                        label=_('Payment Method'),\n                        empty_label=None,\n                        queryset=formforform.payment_methods.all(),\n                        widget=forms.RadioSelect(attrs={'class': 'payment-field'}),\n                        initial=1,\n                    )", "label": 1}
{"function": "def get_processes():\n  tasklist = subprocess.Popen(\"tasklist.exe /fo csv\", stdout=subprocess.PIPE, universal_newlines=True)\n  tasklist = tasklist.stdout.read()\n  tasklist = tasklist.split(\"\\n\")\n  tasklist = list(map(lambda x: x.split(\",\")[0][1:-1].lower(), tasklist))\n  tasklist = list(filter(lambda x: x[-4:] == \".exe\", tasklist))\n  return tasklist", "label": 1}
{"function": "def main():\n  if getattr(sys, \"frozen\", False):\n    scriptpath = os.path.dirname(sys.executable)\n  else:\n    scriptpath = os.path.realpath(__file__)\n\n  if \"steam.exe\" not in get_processes():\n    print(\"WARNING: Steam is not running. Build will most likely fail.\\n\")\n\n  print(\"{} Binarizer\".format(PROJECTNAME))\n  print(\"Authors: {}\".format(\", \".join(AUTHORS)))\n  b = Binarizer(scriptpath)\n\n  try:\n    b.check_paths()\n  except:\n    print(\" Failed to get tool paths. \".center(79, \"=\"))\n    print(\"\")\n    raise\n\n  print(\"\")\n  print(\" Tools found, binarizing. \".center(79, \"=\"))\n  print(\"\")\n\n  b.remove_obsolete()\n  attempted = b.binarize()\n\n  failed = b.verify()\n  succeeded = attempted - failed\n\n  result = \" {} / {} modules binarized. \".format(succeeded, attempted)\n  print(\"\")\n\n  try:\n    import colorama\n  except ImportError:\n    print(result.center(79, \"=\"))\n  else:\n    colorama.init()\n    result = result.center(79, \"=\")\n    if succeeded == attempted:\n      result = \"\\33[32m\" + result + \"\\33[39m\"\n    else:\n      result = \"\\33[31m\" + result + \"\\33[39m\"\n    print(result)\n  print(\"\")\n  print(\"[Finished at {}]\".format(time.ctime()))\n\n  if getattr(sys, \"frozen\", False):\n    input(\"\\nPress any key to exit...\\n\")\n\n  if failed > 0:\n    sys.exit(1)", "label": 1}
{"function": "    def __eq__(self, other):\n        return other and self.__class__ is other.__class__\\\n        and self.__dict__ == other.__dict__", "label": 1}
{"function": "    def _split(self, string):\n        bound = self.boundary\n\n        # Yields (startchar, endchar) pairs for each indexable substring in\n        # the given string, e.g. \"WikiWord\" -> (0, 4), (4, 8)\n\n        # Whether we're splitting on transitions (case changes, letter -> num,\n        # num -> letter, etc.)\n        splitting = self.splitting\n\n        # Make a list (dispos, for \"dispossessed\") of (startchar, endchar)\n        # pairs for runs of text between \"'s\"\n        if \"'\" in string:\n            # Split on possessive 's\n            dispos = []\n            prev = 0\n            for match in self.possessive.finditer(string):\n                dispos.append((prev, match.start()))\n                prev = match.end()\n            if prev < len(string):\n                dispos.append((prev, len(string)))\n        else:\n            # Shortcut if there's no apostrophe in the string\n            dispos = ((0, len(string)),)\n\n        # For each run between 's\n        for sc, ec in dispos:\n            # Split on boundary characters\n            for part_match in self.between.finditer(string, sc, ec):\n                part_start = part_match.start()\n                part_end = part_match.end()\n\n                if splitting:\n                    # The point to start splitting at\n                    prev = part_start\n                    # Find transitions (e.g. \"iW\" or \"a0\")\n                    for bmatch in bound.finditer(string, part_start, part_end):\n                        # The point in the middle of the transition\n                        pivot = bmatch.start() + 1\n                        # Yield from the previous match to the transition\n                        yield (prev, pivot)\n                        # Make the transition the new starting point\n                        prev = pivot\n\n                    # If there's leftover text at the end, yield it too\n                    if prev < part_end:\n                        yield (prev, part_end)\n                else:\n                    # Not splitting on transitions, just yield the part\n                    yield (part_start, part_end)", "label": 1}
{"function": "    def _merge(self, parts):\n        mergewords = self.mergewords\n        mergenums = self.mergenums\n\n        # Current type (1=alpah, 2=digit)\n        last = 0\n        # Where to insert a merged term in the original list\n        insertat = 0\n        # Buffer for parts to merge\n        buf = []\n        # Iterate on a copy of the parts list so we can modify the original as\n        # we go\n\n        def insert_item(buf, at, newpos):\n            newtext = \"\".join(item[0] for item in buf)\n            newsc = buf[0][2]  # start char of first item in buffer\n            newec = buf[-1][3]  # end char of last item in buffer\n            parts.insert(insertat, (newtext, newpos, newsc, newec))\n\n        for item in list(parts):\n            # item = (text, pos, startchar, endchar)\n            text = item[0]\n            pos = item[1]\n\n            # Set the type of this part\n            if text.isalpha():\n                this = 1\n            elif text.isdigit():\n                this = 2\n            else:\n                this = None\n\n            # Is this the same type as the previous part?\n            if (buf and (this == last == 1 and mergewords)\n                or (this == last == 2 and mergenums)):\n                # This part is the same type as the previous. Add it to the\n                # buffer of parts to merge.\n                buf.append(item)\n            else:\n                # This part is different than the previous.\n                if len(buf) > 1:\n                    # If the buffer has at least two parts in it, merge them\n                    # and add them to the original list of parts.\n                    insert_item(buf, insertat, pos - 1)\n                    insertat += 1\n                # Reset the buffer\n                buf = [item]\n                last = this\n            insertat += 1\n\n        # If there are parts left in the buffer at the end, merge them and add\n        # them to the original list.\n        if len(buf) > 1:\n            insert_item(buf, len(parts), pos)", "label": 1}
{"function": "    def __call__(self, tokens):\n        mergewords = self.mergewords\n        mergenums = self.mergenums\n\n        # This filter renumbers tokens as it expands them. New position\n        # counter.\n        newpos = None\n        for t in tokens:\n            text = t.text\n\n            # If this is the first token we've seen, use it to set the new\n            # position counter\n            if newpos is None:\n                if t.positions:\n                    newpos = t.pos\n                else:\n                    # Token doesn't have positions, just use 0\n                    newpos = 0\n\n            if ((text.isalpha() and (text.islower() or text.isupper()))\n                or text.isdigit()):\n                # Short-circuit the common cases of no delimiters, no case\n                # transitions, only digits, etc.\n                t.pos = newpos\n                yield t\n                newpos += 1\n            else:\n                # Split the token text on delimiters, word and/or number\n                # boundaries into a list of (text, pos, startchar, endchar)\n                # tuples\n                ranges = self._split(text)\n                parts = [(text[sc:ec], i + newpos, sc, ec)\n                         for i, (sc, ec) in enumerate(ranges)]\n\n                # Did the split yield more than one part?\n                if len(parts) > 1:\n                    # If the options are set, merge consecutive runs of all-\n                    # letters and/or all-numbers.\n                    if mergewords or mergenums:\n                        self._merge(parts)\n\n                # Yield tokens for the parts\n                chars = t.chars\n                if chars:\n                    base = t.startchar\n                for text, pos, startchar, endchar in parts:\n                    t.text = text\n                    t.pos = pos\n                    if t.chars:\n                        t.startchar = base + startchar\n                        t.endchar = base + endchar\n                    yield t\n\n                if parts:\n                    # Set the new position counter based on the last part\n                    newpos = parts[-1][1] + 1", "label": 1}
{"function": "        def make_token():\n            tk = buf[0]\n            tk.text = sep.join([t.text for t in buf])\n            if tk.chars:\n                tk.endchar = buf[-1].endchar\n            return tk", "label": 1}
{"function": "        def insert_item(buf, at, newpos):\n            newtext = \"\".join(item[0] for item in buf)\n            newsc = buf[0][2]  # start char of first item in buffer\n            newec = buf[-1][3]  # end char of last item in buffer\n            parts.insert(insertat, (newtext, newpos, newsc, newec))", "label": 1}
{"function": "def _hash_name_to_id(h_alg):\n    \"\"\"Try to convert hash algorithm name to HashAlgorithm TLS ID.\n\n    accepts also a string with a single number in it\n    \"\"\"\n    try:\n        return int(h_alg)\n    except ValueError:\n        return getattr(HashAlgorithm, h_alg)", "label": 1}
{"function": "def _sign_alg_name_to_id(s_alg):\n    \"\"\"Try to convert signature algorithm name to SignatureAlgorithm TLS ID.\n\n    accepts also a string with a single number in it\n    \"\"\"\n    try:\n        return int(s_alg)\n    except ValueError:\n        return getattr(SignatureAlgorithm, s_alg)", "label": 1}
{"function": "def sig_algs_to_ids(names):\n    \"\"\"Convert a string with signature algorithm names to list of IDs.\n\n    :type names: str\n    :param names: whitespace separated list of names of hash algorithm\n        names. Names can be specified as the legacy (TLS1.2) hash algorithm\n        and hash type pairs (e.g. ``sha256+rsa``), as a pair of numbers (e.g\n        ``4+1``) or as the new TLS 1.3 signature scheme (e.g.\n        ``rsa_pkcs1_sha256``).\n        Full parameter string then can look like: ``sha256+rsa 5+rsa\n        rsa_pss_pss_sha256``.\n    :raises AttributeError: when the specified identifier is not defined in\n        HashAlgorithm, SignatureAlgorithm or SignatureScheme\n    :return: list of tuples\n    \"\"\"\n    ids = []\n\n    for name in names.split():\n        if '+' in name:\n            h_alg, s_alg = name.split('+')\n\n            hash_id = _hash_name_to_id(h_alg)\n            sign_id = _sign_alg_name_to_id(s_alg)\n\n            ids.append((hash_id, sign_id))\n        else:\n            ids.append(getattr(SignatureScheme, name))\n\n    return ids", "label": 1}
{"function": "def _ext_name_to_id(name):\n    \"\"\"\n    Convert a string with a name of extension to numerical ID.\n\n    Handles both numerical IDs and names.\n\n    :raises AttributeError: when the specified identifier is not defined\n        in ExtensionType\n    \"\"\"\n    try:\n        return int(name)\n    except ValueError:\n        return getattr(ExtensionType, name)", "label": 1}
{"function": "    def process_bind_param(self, value, dialect):\n        return utils.to_json_str(value)", "label": 1}
{"function": "    def process_result_value(self, value, dialect):\n        return utils.from_json_str(value)", "label": 1}
{"function": "    def coerce(cls, key, value):\n        \"\"\"Convert plain lists to MutableList.\"\"\"\n        if not isinstance(value, MutableList):\n            if isinstance(value, list):\n                return MutableList(value)\n\n            # this call will raise ValueError\n            return mutable.Mutable.coerce(key, value)\n        return value", "label": 1}
{"function": "    def __add__(self, value):\n        \"\"\"Detect list add events and emit change events.\"\"\"\n        list.__add__(self, value)\n        self.changed()", "label": 1}
{"function": "    def append(self, value):\n        \"\"\"Detect list add events and emit change events.\"\"\"\n        list.append(self, value)\n        self.changed()", "label": 1}
{"function": "    def __setitem__(self, key, value):\n        \"\"\"Detect list set events and emit change events.\"\"\"\n        list.__setitem__(self, key, value)\n        self.changed()", "label": 1}
{"function": "    def __delitem__(self, i):\n        \"\"\"Detect list del events and emit change events.\"\"\"\n        list.__delitem__(self, i)\n        self.changed()", "label": 1}
{"function": "def _convert_train_id_to_eval_id(prediction, train_id_to_eval_id):\n  \"\"\"Converts the predicted label for evaluation.\n\n  There are cases where the training labels are not equal to the evaluation\n  labels. This function is used to perform the conversion so that we could\n  evaluate the results on the evaluation server.\n\n  Args:\n    prediction: Semantic segmentation prediction.\n    train_id_to_eval_id: A list mapping from train id to evaluation id.\n\n  Returns:\n    Semantic segmentation prediction whose labels have been changed.\n  \"\"\"\n  converted_prediction = prediction.copy()\n  for train_id, eval_id in enumerate(train_id_to_eval_id):\n    converted_prediction[prediction == train_id] = eval_id\n\n  return converted_prediction", "label": 1}
{"function": "def _process_batch(sess, original_images, semantic_predictions, image_names,\n                   image_heights, image_widths, image_id_offset, save_dir,\n                   raw_save_dir, train_id_to_eval_id=None):\n  \"\"\"Evaluates one single batch qualitatively.\n\n  Args:\n    sess: TensorFlow session.\n    original_images: One batch of original images.\n    semantic_predictions: One batch of semantic segmentation predictions.\n    image_names: Image names.\n    image_heights: Image heights.\n    image_widths: Image widths.\n    image_id_offset: Image id offset for indexing images.\n    save_dir: The directory where the predictions will be saved.\n    raw_save_dir: The directory where the raw predictions will be saved.\n    train_id_to_eval_id: A list mapping from train id to eval id.\n  \"\"\"\n  (original_images,\n   semantic_predictions,\n   image_names,\n   image_heights,\n   image_widths) = sess.run([original_images, semantic_predictions,\n                             image_names, image_heights, image_widths])\n\n  num_image = semantic_predictions.shape[0]\n  for i in range(num_image):\n    image_height = np.squeeze(image_heights[i])\n    image_width = np.squeeze(image_widths[i])\n    original_image = np.squeeze(original_images[i])\n    semantic_prediction = np.squeeze(semantic_predictions[i])\n    crop_semantic_prediction = semantic_prediction[:image_height, :image_width]\n\n    # Save image.\n    save_annotation.save_annotation(\n        original_image, save_dir, _IMAGE_FORMAT % (image_id_offset + i),\n        add_colormap=False)\n\n    # Save prediction.\n    save_annotation.save_annotation(\n        crop_semantic_prediction, save_dir,\n        _PREDICTION_FORMAT % (image_id_offset + i), add_colormap=True,\n        colormap_type=FLAGS.colormap_type)\n\n    if FLAGS.also_save_raw_predictions:\n      image_filename = os.path.basename(image_names[i])\n\n      if train_id_to_eval_id is not None:\n        crop_semantic_prediction = _convert_train_id_to_eval_id(\n            crop_semantic_prediction,\n            train_id_to_eval_id)\n      save_annotation.save_annotation(\n          crop_semantic_prediction, raw_save_dir, image_filename,\n          add_colormap=False)", "label": 1}
{"function": "def main(unused_argv):\n  tf.logging.set_verbosity(tf.logging.INFO)\n  # Get dataset-dependent information.\n  dataset = segmentation_dataset.get_dataset(\n      FLAGS.dataset, FLAGS.vis_split, dataset_dir=FLAGS.dataset_dir)\n  train_id_to_eval_id = None\n  if dataset.name == segmentation_dataset.get_cityscapes_dataset_name():\n    tf.logging.info('Cityscapes requires converting train_id to eval_id.')\n    train_id_to_eval_id = _CITYSCAPES_TRAIN_ID_TO_EVAL_ID\n\n  # Prepare for visualization.\n  tf.gfile.MakeDirs(FLAGS.vis_logdir)\n  save_dir = os.path.join(FLAGS.vis_logdir, _SEMANTIC_PREDICTION_SAVE_FOLDER)\n  tf.gfile.MakeDirs(save_dir)\n  raw_save_dir = os.path.join(\n      FLAGS.vis_logdir, _RAW_SEMANTIC_PREDICTION_SAVE_FOLDER)\n  tf.gfile.MakeDirs(raw_save_dir)\n\n  tf.logging.info('Visualizing on %s set', FLAGS.vis_split)\n\n  g = tf.Graph()\n  with g.as_default():\n    samples = input_generator.get(dataset,\n                                  FLAGS.vis_crop_size,\n                                  FLAGS.vis_batch_size,\n                                  min_resize_value=FLAGS.min_resize_value,\n                                  max_resize_value=FLAGS.max_resize_value,\n                                  resize_factor=FLAGS.resize_factor,\n                                  dataset_split=FLAGS.vis_split,\n                                  is_training=False,\n                                  model_variant=FLAGS.model_variant)\n\n    model_options = common.ModelOptions(\n        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_classes},\n        crop_size=FLAGS.vis_crop_size,\n        atrous_rates=FLAGS.atrous_rates,\n        output_stride=FLAGS.output_stride)\n\n    if tuple(FLAGS.eval_scales) == (1.0,):\n      tf.logging.info('Performing single-scale test.')\n      predictions = model.predict_labels(\n          samples[common.IMAGE],\n          model_options=model_options,\n          image_pyramid=FLAGS.image_pyramid)\n    else:\n      tf.logging.info('Performing multi-scale test.')\n      predictions = model.predict_labels_multi_scale(\n          samples[common.IMAGE],\n          model_options=model_options,\n          eval_scales=FLAGS.eval_scales,\n          add_flipped_images=FLAGS.add_flipped_images)\n    predictions = predictions[common.OUTPUT_TYPE]\n\n    if FLAGS.min_resize_value and FLAGS.max_resize_value:\n      # Only support batch_size = 1, since we assume the dimensions of original\n      # image after tf.squeeze is [height, width, 3].\n      assert FLAGS.vis_batch_size == 1\n\n      # Reverse the resizing and padding operations performed in preprocessing.\n      # First, we slice the valid regions (i.e., remove padded region) and then\n      # we reisze the predictions back.\n      original_image = tf.squeeze(samples[common.ORIGINAL_IMAGE])\n      original_image_shape = tf.shape(original_image)\n      predictions = tf.slice(\n          predictions,\n          [0, 0, 0],\n          [1, original_image_shape[0], original_image_shape[1]])\n      resized_shape = tf.to_int32([tf.squeeze(samples[common.HEIGHT]),\n                                   tf.squeeze(samples[common.WIDTH])])\n      predictions = tf.squeeze(\n          tf.image.resize_images(tf.expand_dims(predictions, 3),\n                                 resized_shape,\n                                 method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n                                 align_corners=True), 3)\n\n    tf.train.get_or_create_global_step()\n    saver = tf.train.Saver(slim.get_variables_to_restore())\n    sv = tf.train.Supervisor(graph=g,\n                             logdir=FLAGS.vis_logdir,\n                             init_op=tf.global_variables_initializer(),\n                             summary_op=None,\n                             summary_writer=None,\n                             global_step=None,\n                             saver=saver)\n    num_batches = int(math.ceil(\n        dataset.num_samples / float(FLAGS.vis_batch_size)))\n    last_checkpoint = None\n\n    # Loop to visualize the results when new checkpoint is created.\n    num_iters = 0\n    while (FLAGS.max_number_of_iterations <= 0 or\n           num_iters < FLAGS.max_number_of_iterations):\n      num_iters += 1\n      last_checkpoint = slim.evaluation.wait_for_new_checkpoint(\n          FLAGS.checkpoint_dir, last_checkpoint)\n      start = time.time()\n      tf.logging.info(\n          'Starting visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n                                                       time.gmtime()))\n      tf.logging.info('Visualizing with model %s', last_checkpoint)\n\n      with sv.managed_session(FLAGS.master,\n                              start_standard_services=False) as sess:\n        sv.start_queue_runners(sess)\n        sv.saver.restore(sess, last_checkpoint)\n\n        image_id_offset = 0\n        for batch in range(num_batches):\n          tf.logging.info('Visualizing batch %d / %d', batch + 1, num_batches)\n          _process_batch(sess=sess,\n                         original_images=samples[common.ORIGINAL_IMAGE],\n                         semantic_predictions=predictions,\n                         image_names=samples[common.IMAGE_NAME],\n                         image_heights=samples[common.HEIGHT],\n                         image_widths=samples[common.WIDTH],\n                         image_id_offset=image_id_offset,\n                         save_dir=save_dir,\n                         raw_save_dir=raw_save_dir,\n                         train_id_to_eval_id=train_id_to_eval_id)\n          image_id_offset += FLAGS.vis_batch_size\n\n      tf.logging.info(\n          'Finished visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n                                                       time.gmtime()))\n      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()\n      if time_to_next_eval > 0:\n        time.sleep(time_to_next_eval)", "label": 1}
{"function": "    def consumer(self, partition_id, type):\n        \"\"\"\n        Creates spider log consumer with BaseStreamConsumer interface\n        :param partition_id: can be None or integer\n        :param type: either 'db' or 'sw'\n        :return:\n        \"\"\"\n        group = self._sw_group if type == b'sw' else self._db_group\n        c = Consumer(self._location, self._enable_ssl, self._cert_path, self._topic, group, partition_id)\n        assert len(c._consumer.partitions_for_topic(self._topic)) == self._partitions\n        return c", "label": 1}
{"function": "    def __init__(self, messagebus):\n        self._location = messagebus.kafka_location\n        self._general_group = messagebus.spider_feed_group\n        self._topic = messagebus.topic_todo\n        self._max_next_requests = messagebus.max_next_requests\n        self._hostname_partitioning = messagebus.hostname_partitioning\n        self._enable_ssl = messagebus.enable_ssl\n        self._cert_path = messagebus.cert_path\n        kwargs = {\n            'bootstrap_servers': self._location,\n            'topic': self._topic,\n            'group_id': self._general_group,\n        }\n        if self._enable_ssl:\n            kwargs.update(_prepare_kafka_ssl_kwargs(self._cert_path))\n        self._offset_fetcher = OffsetsFetcherAsync(**kwargs)\n        self._codec = messagebus.codec\n        self._partitions = messagebus.spider_feed_partitions", "label": 1}
{"function": "    def consumer(self, partition_id):\n        c = Consumer(self._location, self._enable_ssl, self._cert_path, self._topic, self._general_group, partition_id)\n        assert len(c._consumer.partitions_for_topic(self._topic)) == self._partitions, \\\n            \"Number of kafka topic partitions doesn't match value in config for spider feed\"\n        return c", "label": 1}
{"function": "    def available_partitions(self):\n        partitions = []\n        lags = self._offset_fetcher.get()\n        for partition, lag in six.iteritems(lags):\n            if lag < self._max_next_requests:\n                partitions.append(partition)\n        return partitions", "label": 1}
{"function": "    def producer(self):\n        partitioner = Crc32NamePartitioner(self._partitions) if self._hostname_partitioning \\\n            else FingerprintPartitioner(self._partitions)\n        return KeyedProducer(self._location, self._enable_ssl, self._cert_path, self._topic, partitioner, self._codec,\n                             batch_size=DEFAULT_BATCH_SIZE,\n                             buffer_memory=DEFAULT_BUFFER_MEMORY)", "label": 1}
{"function": "    def __init__(self, messagebus):\n        self._topic = messagebus.topic_scoring\n        self._group = messagebus.scoringlog_dbw_group\n        self._location = messagebus.kafka_location\n        self._codec = messagebus.codec\n        self._cert_path = messagebus.cert_path\n        self._enable_ssl = messagebus.enable_ssl", "label": 1}
{"function": "    def consumer(self):\n        return Consumer(self._location, self._enable_ssl, self._cert_path, self._topic, self._group, partition_id=None)", "label": 1}
{"function": "    def producer(self):\n        return SimpleProducer(self._location, self._enable_ssl, self._cert_path, self._topic, self._codec,\n                              batch_size=DEFAULT_BATCH_SIZE,\n                              buffer_memory=DEFAULT_BUFFER_MEMORY)", "label": 1}
{"function": "    def __init__(self, messagebus):\n        super(StatsLogStream, self).__init__(messagebus)\n        self._topic = messagebus.topic_stats\n        self._group = messagebus.statslog_reader_group", "label": 1}
{"function": "    def __init__(self, settings):\n        self.topic_todo = settings.get('SPIDER_FEED_TOPIC')\n        self.topic_done = settings.get('SPIDER_LOG_TOPIC')\n        self.topic_scoring = settings.get('SCORING_LOG_TOPIC')\n        self.topic_stats = settings.get('STATS_LOG_TOPIC')\n\n        self.spiderlog_dbw_group = settings.get('SPIDER_LOG_DBW_GROUP')\n        self.spiderlog_sw_group = settings.get('SPIDER_LOG_SW_GROUP')\n        self.scoringlog_dbw_group = settings.get('SCORING_LOG_DBW_GROUP')\n        self.statslog_reader_group = settings.get('STATS_LOG_READER_GROUP')\n        self.spider_feed_group = settings.get('SPIDER_FEED_GROUP')\n        self.spider_partition_id = settings.get('SPIDER_PARTITION_ID')\n        self.max_next_requests = settings.MAX_NEXT_REQUESTS\n        self.hostname_partitioning = settings.get('QUEUE_HOSTNAME_PARTITIONING')\n        self.codec = settings.get('KAFKA_CODEC')\n        self.kafka_location = settings.get('KAFKA_LOCATION')\n        self.enable_ssl = settings.get('KAFKA_ENABLE_SSL')\n        self.cert_path = settings.get('KAFKA_CERT_PATH')\n        self.spider_log_partitions = settings.get('SPIDER_LOG_PARTITIONS')\n        self.spider_feed_partitions = settings.get('SPIDER_FEED_PARTITIONS')", "label": 1}
{"function": "    def __init__(self, name, toolpath=None, **kw):\n        if toolpath is None:\n            toolpath = []\n\n        # Rename if there's a TOOL_ALIAS for this tool\n        self.name = TOOL_ALIASES.get(name, name)\n        self.toolpath = toolpath + DefaultToolpath\n        # remember these so we can merge them into the call\n        self.init_kw = kw\n\n        module = self._tool_module()\n        self.generate = module.generate\n        self.exists = module.exists\n        if hasattr(module, 'options'):\n            self.options = module.options", "label": 1}
{"function": "    def _load_dotted_module_py2(self, short_name, full_name, searchpaths=None):\n        import imp\n\n        splitname = short_name.split('.')\n        index = 0\n        srchpths = searchpaths\n        for item in splitname:\n            file, path, desc = imp.find_module(item, srchpths)\n            mod = imp.load_module(full_name, file, path, desc)\n            srchpths = [path]\n        return mod, file", "label": 1}
{"function": "    def _tool_module(self):\n        oldpythonpath = sys.path\n        sys.path = self.toolpath + sys.path\n        # sys.stderr.write(\"Tool:%s\\nPATH:%s\\n\"%(self.name,sys.path))\n\n        # From: http://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path/67692#67692\n        # import importlib.util\n        # spec = importlib.util.spec_from_file_location(\"module.name\", \"/path/to/file.py\")\n        # foo = importlib.util.module_from_spec(spec)\n        # spec.loader.exec_module(foo)\n        # foo.MyClass()\n        # Py 3 code\n\n\n        # sys.stderr.write(\"toolpath:%s\\n\" % self.toolpath)\n        # sys.stderr.write(\"SCONS.TOOL path:%s\\n\" % sys.modules['SCons.Tool'].__path__)\n        debug = False\n        spec = None\n        found_name = self.name\n        add_to_scons_tools_namespace = False\n        for path in self.toolpath:\n            sepname = self.name.replace('.', os.path.sep)\n            file_path = os.path.join(path, \"%s.py\" % sepname)\n            file_package = os.path.join(path, sepname)\n\n            if debug: sys.stderr.write(\"Trying:%s %s\\n\" % (file_path, file_package))\n\n            if os.path.isfile(file_path):\n                spec = importlib.util.spec_from_file_location(self.name, file_path)\n                if debug: print(\"file_Path:%s FOUND\" % file_path)\n                break\n            elif os.path.isdir(file_package):\n                file_package = os.path.join(file_package, '__init__.py')\n                spec = importlib.util.spec_from_file_location(self.name, file_package)\n                if debug: print(\"PACKAGE:%s Found\" % file_package)\n                break\n\n            else:\n                continue\n\n        if spec is None:\n            if debug: sys.stderr.write(\"NO SPEC :%s\\n\" % self.name)\n            spec = importlib.util.find_spec(\".\" + self.name, package='SCons.Tool')\n            if spec:\n                found_name = 'SCons.Tool.' + self.name\n                add_to_scons_tools_namespace = True\n            if debug: sys.stderr.write(\"Spec Found? .%s :%s\\n\" % (self.name, spec))\n\n        if spec is None:\n            sconstools = os.path.normpath(sys.modules['SCons.Tool'].__path__[0])\n            if self.toolpath:\n                sconstools = \", \".join(self.toolpath) + \", \" + sconstools\n            error_string = \"No tool module '%s' found in %s\" % (self.name, sconstools)\n            raise SCons.Errors.UserError(error_string)\n\n        module = importlib.util.module_from_spec(spec)\n        if module is None:\n            if debug: print(\"MODULE IS NONE:%s\" % self.name)\n            error_string = \"Tool module '%s' failed import\" % self.name\n            raise SCons.Errors.SConsEnvironmentError(error_string)\n\n        # Don't reload a tool we already loaded.\n        sys_modules_value = sys.modules.get(found_name, False)\n\n        found_module = None\n        if sys_modules_value and sys_modules_value.__file__ == spec.origin:\n            found_module = sys.modules[found_name]\n        else:\n            # Not sure what to do in the case that there already\n            # exists sys.modules[self.name] but the source file is\n            # different.. ?\n            module = spec.loader.load_module(spec.name)\n\n            sys.modules[found_name] = module\n            if add_to_scons_tools_namespace:\n                # If we found it in SCons.Tool, then add it to the module\n                setattr(SCons.Tool, self.name, module)\n\n            found_module = module\n\n        if found_module is not None:\n            sys.path = oldpythonpath\n            return found_module\n\n        sys.path = oldpythonpath\n\n        full_name = 'SCons.Tool.' + self.name\n        try:\n            return sys.modules[full_name]\n        except KeyError:\n            try:\n                smpath = sys.modules['SCons.Tool'].__path__\n                try:\n                    module, file = self._load_dotted_module_py2(self.name, full_name, smpath)\n                    setattr(SCons.Tool, self.name, module)\n                    if file:\n                        file.close()\n                    return module\n                except ImportError as e:\n                    if str(e) != \"No module named %s\" % self.name:\n                        raise SCons.Errors.SConsEnvironmentError(e)\n                    try:\n                        import zipimport\n                        importer = zipimport.zipimporter(sys.modules['SCons.Tool'].__path__[0])\n                        module = importer.load_module(full_name)\n                        setattr(SCons.Tool, self.name, module)\n                        return module\n                    except ImportError as e:\n                        m = \"No tool named '%s': %s\" % (self.name, e)\n                        raise SCons.Errors.SConsEnvironmentError(m)\n            except ImportError as e:\n                m = \"No tool named '%s': %s\" % (self.name, e)\n                raise SCons.Errors.SConsEnvironmentError(m)", "label": 1}
{"function": "    def __call__(self, env, *args, **kw):\n        if self.init_kw is not None:\n            # Merge call kws into init kws;\n            # but don't bash self.init_kw.\n            if kw is not None:\n                call_kw = kw\n                kw = self.init_kw.copy()\n                kw.update(call_kw)\n            else:\n                kw = self.init_kw\n        env.Append(TOOLS=[self.name])\n        if hasattr(self, 'options'):\n            import SCons.Variables\n            if 'options' not in env:\n                from SCons.Script import ARGUMENTS\n                env['options'] = SCons.Variables.Variables(args=ARGUMENTS)\n            opts = env['options']\n\n            self.options(opts)\n            opts.Update(env)\n\n        self.generate(env, *args, **kw)", "label": 1}
{"function": "    def __str__(self):\n        return self.name", "label": 1}
{"function": "    def __init__(self, name, initializer):\n        \"\"\"\n        Note:  we store the tool name as __name__ so it can be used by\n        the class that attaches this to a construction environment.\n        \"\"\"\n        self.__name__ = name\n        self.initializer = initializer", "label": 1}
{"function": "    def get_builder(self, env):\n        \"\"\"\n        Returns the appropriate real Builder for this method name\n        after having the associated ToolInitializer object apply\n        the appropriate Tool module.\n        \"\"\"\n        builder = getattr(env, self.__name__)\n\n        self.initializer.apply_tools(env)\n\n        builder = getattr(env, self.__name__)\n        if builder is self:\n            # There was no Builder added, which means no valid Tool\n            # for this name was found (or possibly there's a mismatch\n            # between the name we were called by and the Builder name\n            # added by the Tool module).\n            return None\n\n        self.initializer.remove_methods(env)\n\n        return builder", "label": 1}
{"function": "    def __call__(self, env, *args, **kw):\n        \"\"\"\n        \"\"\"\n        builder = self.get_builder(env)\n        if builder is None:\n            return [], []\n        return builder(*args, **kw)", "label": 1}
{"function": "    def __init__(self, env, tools, names):\n        if not SCons.Util.is_List(tools):\n            tools = [tools]\n        if not SCons.Util.is_List(names):\n            names = [names]\n        self.env = env\n        self.tools = tools\n        self.names = names\n        self.methods = {}\n        for name in names:\n            method = ToolInitializerMethod(name, self)\n            self.methods[name] = method\n            env.AddMethod(method)", "label": 1}
{"function": "    def remove_methods(self, env):\n        \"\"\"\n        Removes the methods that were added by the tool initialization\n        so we no longer copy and re-bind them when the construction\n        environment gets cloned.\n        \"\"\"\n        for method in self.methods.values():\n            env.RemoveMethod(method)", "label": 1}
{"function": "    def test_host_power_action(self):\n        self._test_compute_api('host_power_action', 'call', action='action',\n                host='host')", "label": 1}
{"function": "    def test_inject_network_info(self):\n        self._test_compute_api('inject_network_info', 'cast',\n                instance=self.fake_instance_obj)", "label": 1}
{"function": "    def test_live_migration(self):\n        self._test_compute_api('live_migration', 'cast',\n                instance=self.fake_instance_obj, dest='dest',\n                block_migration='blockity_block', host='tsoh',\n                migration='migration',\n                migrate_data={}, version='5.0')", "label": 1}
{"function": "    def test_live_migration_force_complete(self):\n        migration = migration_obj.Migration()\n        migration.id = 1\n        migration.source_compute = 'fake'\n        ctxt = context.RequestContext('fake_user', 'fake_project')\n        version = '5.0'\n        rpcapi = compute_rpcapi.ComputeAPI()\n        rpcapi.router.client = mock.Mock()\n        mock_client = mock.MagicMock()\n        rpcapi.router.client.return_value = mock_client\n        mock_client.can_send_version.return_value = True\n        mock_cctx = mock.MagicMock()\n        mock_client.prepare.return_value = mock_cctx\n        rpcapi.live_migration_force_complete(ctxt, self.fake_instance_obj,\n                                             migration)\n        mock_client.prepare.assert_called_with(server=migration.source_compute,\n                                               version=version)\n        mock_cctx.cast.assert_called_with(ctxt,\n                                          'live_migration_force_complete',\n                                          instance=self.fake_instance_obj)", "label": 1}
{"function": "    def test_live_migration_abort(self):\n        self._test_compute_api('live_migration_abort', 'cast',\n                instance=self.fake_instance_obj,\n                migration_id='1', version='5.0')", "label": 1}
{"function": "    def test_post_live_migration_at_destination(self):\n        self.flags(long_rpc_timeout=1234)\n        self._test_compute_api('post_live_migration_at_destination', 'call',\n                instance=self.fake_instance_obj,\n                block_migration='block_migration', host='host', version='5.0',\n                timeout=1234, call_monitor_timeout=60)", "label": 1}
{"function": "    def test_pause_instance(self):\n        self._test_compute_api('pause_instance', 'cast',\n                               instance=self.fake_instance_obj)", "label": 1}
{"function": "    def test_soft_delete_instance(self):\n        self._test_compute_api('soft_delete_instance', 'cast',\n                instance=self.fake_instance_obj)", "label": 1}
{"function": "    def test_swap_volume(self):\n        self._test_compute_api('swap_volume', 'cast',\n                instance=self.fake_instance_obj, old_volume_id='oldid',\n                new_volume_id='newid', new_attachment_id=uuids.attachment_id,\n                version='5.0')", "label": 1}
{"function": "    def test_restore_instance(self):\n        self._test_compute_api('restore_instance', 'cast',\n                instance=self.fake_instance_obj, version='5.0')", "label": 1}
{"function": "    def __repr__(self):\n        return '[%s] type=%s, value=%s' % (self.name, self.type, str(self.value))", "label": 1}
{"function": "    def __init__(self, name, value=None):\n        RegistryValue.__init__(self, name, 'REG_MULTI_SZ', value)", "label": 1}
{"function": "    def __str__(self):\n        return ' '.join(self.value)", "label": 1}
{"function": "    def __repr__(self):\n        return '[%s] %s %s' % (self.name, self.type, ' '.join(self.value))", "label": 1}
{"function": "    def __init__(self, id):\n        if id < 0:\n            self.id = c_uint32(id).value\n        else:\n            self.id = id\n        if zibe_errors.error_codes.has_key(self.id):\n            error_data = zibe_errors.error_codes[self.id]\n            self.message = error_data[1]\n            self.name = error_data[0]\n        else:\n            self.message = 'Unknown error 0x%x' % self.id", "label": 1}
{"function": "    def __str__(self):\n        return self.message + '\\n'", "label": 1}
{"function": "    def __init__(self, hCtx):\n        self.hCtx = hCtx", "label": 1}
{"function": "    def localdir(self):\n        return os.getcwd()", "label": 1}
{"function": "    def lcd(self, path):\n        path = os.path.normpath(path)\n        if not os.path.exists(path):\n            raise exceptions.EnvironmentError(\"Path '%s' does not exist on the local system\" % path)\n        os.chdir(path)", "label": 1}
{"function": "    def close(self):\n        pass", "label": 1}
{"function": "def decode_jpeg_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def decode_png_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def draw_bounding_boxes_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def encode_jpeg_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def encode_png_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def extract_glimpse_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def hsv_to_rgb_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def non_max_suppression_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def non_max_suppression_v2_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "def quantized_resize_bilinear_handler(converter: TensorFlowConverter, tf_op: \"tf.Operation\"):\n    raise NotImplementedError(f\"[TensorFlowConverter] {tf_op.type} is not supported yet.\")", "label": 1}
{"function": "\tdef set_ResourceOwnerId(self,ResourceOwnerId):\n\t\tself.add_query_param('ResourceOwnerId',ResourceOwnerId)", "label": 1}
{"function": "\tdef get_ClientToken(self):\n\t\treturn self.get_query_params().get('ClientToken')", "label": 1}
{"function": "\tdef set_ClientToken(self,ClientToken):\n\t\tself.add_query_param('ClientToken',ClientToken)", "label": 1}
{"function": "\tdef get_ResourceOwnerAccount(self):\n\t\treturn self.get_query_params().get('ResourceOwnerAccount')", "label": 1}
{"function": "\tdef set_ResourceOwnerAccount(self,ResourceOwnerAccount):\n\t\tself.add_query_param('ResourceOwnerAccount',ResourceOwnerAccount)", "label": 1}
{"function": "\tdef get_OwnerAccount(self):\n\t\treturn self.get_query_params().get('OwnerAccount')", "label": 1}
{"function": "\tdef set_OwnerAccount(self,OwnerAccount):\n\t\tself.add_query_param('OwnerAccount',OwnerAccount)", "label": 1}
{"function": "\tdef get_OwnerId(self):\n\t\treturn self.get_query_params().get('OwnerId')", "label": 1}
{"function": "\tdef set_OwnerId(self,OwnerId):\n\t\tself.add_query_param('OwnerId',OwnerId)", "label": 1}
{"function": "\tdef get_VpcId(self):\n\t\treturn self.get_query_params().get('VpcId')", "label": 1}
{"function": "def _maketup(descr, val):\n    dt = dtype(descr)\n    # Place val in all scalar tuples:\n    fields = dt.fields\n    if fields is None:\n        return val\n    else:\n        res = [_maketup(fields[name][0], val) for name in dt.names]\n        return tuple(res)", "label": 1}
{"function": "def identity(n, dtype=None):\n    \"\"\"\n    Return the identity array.\n\n    The identity array is a square array with ones on\n    the main diagonal.\n\n    Parameters\n    ----------\n    n : int\n        Number of rows (and columns) in `n` x `n` output.\n    dtype : data-type, optional\n        Data-type of the output.  Defaults to ``float``.\n\n    Returns\n    -------\n    out : ndarray\n        `n` x `n` array with its main diagonal set to one,\n        and all other elements 0.\n\n    Examples\n    --------\n    >>> np.identity(3)\n    array([[ 1.,  0.,  0.],\n           [ 0.,  1.,  0.],\n           [ 0.,  0.,  1.]])\n\n    \"\"\"\n    from numpy import eye\n    return eye(n, dtype=dtype)", "label": 1}
{"function": "def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    \"\"\"\n    Returns True if two arrays are element-wise equal within a tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    If either array contains one or more NaNs, False is returned.\n    Infs are treated as equal if they are in the same place and of the same\n    sign in both arrays.\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : float\n        The relative tolerance parameter (see Notes).\n    atol : float\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n        .. versionadded:: 1.10.0\n\n    Returns\n    -------\n    allclose : bool\n        Returns True if the two arrays are equal within the given\n        tolerance; False otherwise.\n\n    See Also\n    --------\n    isclose, all, any, equal\n\n    Notes\n    -----\n    If the following equation is element-wise True, then allclose returns\n    True.\n\n     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n    The above equation is not symmetric in `a` and `b`, so that\n    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in\n    some rare cases.\n\n    The comparison of `a` and `b` uses standard broadcasting, which\n    means that `a` and `b` need not have the same shape in order for\n    ``allclose(a, b)`` to evaluate to True.  The same is true for\n    `equal` but not `array_equal`.\n\n    Examples\n    --------\n    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])\n    False\n    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])\n    True\n    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])\n    False\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan])\n    False\n    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    True\n\n    \"\"\"\n    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n    return bool(res)", "label": 1}
{"function": "def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):\n    \"\"\"\n    Returns a boolean array where two arrays are element-wise equal within a\n    tolerance.\n\n    The tolerance values are positive, typically very small numbers.  The\n    relative difference (`rtol` * abs(`b`)) and the absolute difference\n    `atol` are added together to compare against the absolute difference\n    between `a` and `b`.\n\n    .. warning:: The default `atol` is not appropriate for comparing numbers\n                 that are much smaller than one (see Notes).\n\n    Parameters\n    ----------\n    a, b : array_like\n        Input arrays to compare.\n    rtol : float\n        The relative tolerance parameter (see Notes).\n    atol : float\n        The absolute tolerance parameter (see Notes).\n    equal_nan : bool\n        Whether to compare NaN's as equal.  If True, NaN's in `a` will be\n        considered equal to NaN's in `b` in the output array.\n\n    Returns\n    -------\n    y : array_like\n        Returns a boolean array of where `a` and `b` are equal within the\n        given tolerance. If both `a` and `b` are scalars, returns a single\n        boolean value.\n\n    See Also\n    --------\n    allclose\n\n    Notes\n    -----\n    .. versionadded:: 1.7.0\n\n    For finite values, isclose uses the following equation to test whether\n    two floating point values are equivalent.\n\n     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))\n\n    Unlike the built-in `math.isclose`, the above equation is not symmetric\n    in `a` and `b` -- it assumes `b` is the reference value -- so that\n    `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,\n    the default value of atol is not zero, and is used to determine what\n    small values should be considered close to zero. The default value is\n    appropriate for expected values of order unity: if the expected values\n    are significantly smaller than one, it can result in false positives.\n    `atol` should be carefully selected for the use case at hand. A zero value\n    for `atol` will result in `False` if either `a` or `b` is zero.\n\n    Examples\n    --------\n    >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])\n    array([True, False])\n    >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])\n    array([True, True])\n    >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])\n    array([False, True])\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan])\n    array([True, False])\n    >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)\n    array([True, True])\n    >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])\n    array([ True, False], dtype=bool)\n    >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)\n    array([False, False], dtype=bool)\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])\n    array([ True,  True], dtype=bool)\n    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)\n    array([False,  True], dtype=bool)\n    \"\"\"\n    def within_tol(x, y, atol, rtol):\n        with errstate(invalid='ignore'):\n            return less_equal(abs(x-y), atol + rtol * abs(y))\n\n    x = asanyarray(a)\n    y = asanyarray(b)\n\n    # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).\n    # This will cause casting of x later. Also, make sure to allow subclasses\n    # (e.g., for numpy.ma).\n    dt = multiarray.result_type(y, 1.)\n    y = array(y, dtype=dt, copy=False, subok=True)\n\n    xfin = isfinite(x)\n    yfin = isfinite(y)\n    if all(xfin) and all(yfin):\n        return within_tol(x, y, atol, rtol)\n    else:\n        finite = xfin & yfin\n        cond = zeros_like(finite, subok=True)\n        # Because we're using boolean indexing, x & y must be the same shape.\n        # Ideally, we'd just do x, y = broadcast_arrays(x, y). It's in\n        # lib.stride_tricks, though, so we can't import it here.\n        x = x * ones_like(cond)\n        y = y * ones_like(cond)\n        # Avoid subtraction with infinite/nan values...\n        cond[finite] = within_tol(x[finite], y[finite], atol, rtol)\n        # Check for equality of infinite values...\n        cond[~finite] = (x[~finite] == y[~finite])\n        if equal_nan:\n            # Make NaN == NaN\n            both_nan = isnan(x) & isnan(y)\n\n            # Needed to treat masked arrays correctly. = True would not work.\n            cond[both_nan] = both_nan[both_nan]\n\n        return cond[()]  # Flatten 0d arrays to scalars", "label": 1}
{"function": "def array_equal(a1, a2):\n    \"\"\"\n    True if two arrays have the same shape and elements, False otherwise.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n\n    Returns\n    -------\n    b : bool\n        Returns True if the arrays are equal.\n\n    See Also\n    --------\n    allclose: Returns True if two arrays are element-wise equal within a\n              tolerance.\n    array_equiv: Returns True if input arrays are shape consistent and all\n                 elements equal.\n\n    Examples\n    --------\n    >>> np.array_equal([1, 2], [1, 2])\n    True\n    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))\n    True\n    >>> np.array_equal([1, 2], [1, 2, 3])\n    False\n    >>> np.array_equal([1, 2], [1, 4])\n    False\n\n    \"\"\"\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    if a1.shape != a2.shape:\n        return False\n    return bool(asarray(a1 == a2).all())", "label": 1}
{"function": "def array_equiv(a1, a2):\n    \"\"\"\n    Returns True if input arrays are shape consistent and all elements equal.\n\n    Shape consistent means they are either the same shape, or one input array\n    can be broadcasted to create the same shape as the other one.\n\n    Parameters\n    ----------\n    a1, a2 : array_like\n        Input arrays.\n\n    Returns\n    -------\n    out : bool\n        True if equivalent, False otherwise.\n\n    Examples\n    --------\n    >>> np.array_equiv([1, 2], [1, 2])\n    True\n    >>> np.array_equiv([1, 2], [1, 3])\n    False\n\n    Showing the shape equivalence:\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])\n    True\n    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])\n    False\n\n    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])\n    False\n\n    \"\"\"\n    try:\n        a1, a2 = asarray(a1), asarray(a2)\n    except Exception:\n        return False\n    try:\n        multiarray.broadcast(a1, a2)\n    except Exception:\n        return False\n\n    return bool(asarray(a1 == a2).all())", "label": 1}
{"function": "def seterr(all=None, divide=None, over=None, under=None, invalid=None):\n    \"\"\"\n    Set how floating-point errors are handled.\n\n    Note that operations on integer scalar types (such as `int16`) are\n    handled like floating point, and are affected by these settings.\n\n    Parameters\n    ----------\n    all : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Set treatment for all types of floating-point errors at once:\n\n        - ignore: Take no action when the exception occurs.\n        - warn: Print a `RuntimeWarning` (via the Python `warnings` module).\n        - raise: Raise a `FloatingPointError`.\n        - call: Call a function specified using the `seterrcall` function.\n        - print: Print a warning directly to ``stdout``.\n        - log: Record error in a Log object specified by `seterrcall`.\n\n        The default is not to change the current behavior.\n    divide : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for division by zero.\n    over : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for floating-point overflow.\n    under : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for floating-point underflow.\n    invalid : {'ignore', 'warn', 'raise', 'call', 'print', 'log'}, optional\n        Treatment for invalid floating-point operation.\n\n    Returns\n    -------\n    old_settings : dict\n        Dictionary containing the old settings.\n\n    See also\n    --------\n    seterrcall : Set a callback function for the 'call' mode.\n    geterr, geterrcall, errstate\n\n    Notes\n    -----\n    The floating-point exceptions are defined in the IEEE 754 standard [1]_:\n\n    - Division by zero: infinite result obtained from finite numbers.\n    - Overflow: result too large to be expressed.\n    - Underflow: result so close to zero that some precision\n      was lost.\n    - Invalid operation: result is not an expressible number, typically\n      indicates that a NaN was produced.\n\n    .. [1] http://en.wikipedia.org/wiki/IEEE_754\n\n    Examples\n    --------\n    >>> old_settings = np.seterr(all='ignore')  #seterr to known value\n    >>> np.seterr(over='raise')\n    {'over': 'ignore', 'divide': 'ignore', 'invalid': 'ignore',\n     'under': 'ignore'}\n    >>> np.seterr(**old_settings)  # reset to default\n    {'over': 'raise', 'divide': 'ignore', 'invalid': 'ignore',\n     'under': 'ignore'}\n\n    >>> np.int16(32000) * np.int16(3)\n    30464\n    >>> old_settings = np.seterr(all='warn', over='raise')\n    >>> np.int16(32000) * np.int16(3)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n    FloatingPointError: overflow encountered in short_scalars\n\n    >>> old_settings = np.seterr(all='print')\n    >>> np.geterr()\n    {'over': 'print', 'divide': 'print', 'invalid': 'print', 'under': 'print'}\n    >>> np.int16(32000) * np.int16(3)\n    Warning: overflow encountered in short_scalars\n    30464\n\n    \"\"\"\n\n    pyvals = umath.geterrobj()\n    old = geterr()\n\n    if divide is None:\n        divide = all or old['divide']\n    if over is None:\n        over = all or old['over']\n    if under is None:\n        under = all or old['under']\n    if invalid is None:\n        invalid = all or old['invalid']\n\n    maskvalue = ((_errdict[divide] << SHIFT_DIVIDEBYZERO) +\n                 (_errdict[over] << SHIFT_OVERFLOW) +\n                 (_errdict[under] << SHIFT_UNDERFLOW) +\n                 (_errdict[invalid] << SHIFT_INVALID))\n\n    pyvals[1] = maskvalue\n    umath.seterrobj(pyvals)\n    return old", "label": 1}
{"function": "def geterr():\n    \"\"\"\n    Get the current way of handling floating-point errors.\n\n    Returns\n    -------\n    res : dict\n        A dictionary with keys \"divide\", \"over\", \"under\", and \"invalid\",\n        whose values are from the strings \"ignore\", \"print\", \"log\", \"warn\",\n        \"raise\", and \"call\". The keys represent possible floating-point\n        exceptions, and the values define how these exceptions are handled.\n\n    See Also\n    --------\n    geterrcall, seterr, seterrcall\n\n    Notes\n    -----\n    For complete documentation of the types of floating-point exceptions and\n    treatment options, see `seterr`.\n\n    Examples\n    --------\n    >>> np.geterr()\n    {'over': 'warn', 'divide': 'warn', 'invalid': 'warn',\n    'under': 'ignore'}\n    >>> np.arange(3.) / np.arange(3.)\n    array([ NaN,   1.,   1.])\n\n    >>> oldsettings = np.seterr(all='warn', over='raise')\n    >>> np.geterr()\n    {'over': 'raise', 'divide': 'warn', 'invalid': 'warn', 'under': 'warn'}\n    >>> np.arange(3.) / np.arange(3.)\n    __main__:1: RuntimeWarning: invalid value encountered in divide\n    array([ NaN,   1.,   1.])\n\n    \"\"\"\n    maskvalue = umath.geterrobj()[1]\n    mask = 7\n    res = {}\n    val = (maskvalue >> SHIFT_DIVIDEBYZERO) & mask\n    res['divide'] = _errdict_rev[val]\n    val = (maskvalue >> SHIFT_OVERFLOW) & mask\n    res['over'] = _errdict_rev[val]\n    val = (maskvalue >> SHIFT_UNDERFLOW) & mask\n    res['under'] = _errdict_rev[val]\n    val = (maskvalue >> SHIFT_INVALID) & mask\n    res['invalid'] = _errdict_rev[val]\n    return res", "label": 1}
{"function": "def setbufsize(size):\n    \"\"\"\n    Set the size of the buffer used in ufuncs.\n\n    Parameters\n    ----------\n    size : int\n        Size of buffer.\n\n    \"\"\"\n    if size > 10e6:\n        raise ValueError(\"Buffer size, %s, is too big.\" % size)\n    if size < 5:\n        raise ValueError(\"Buffer size, %s, is too small.\" % size)\n    if size % 16 != 0:\n        raise ValueError(\"Buffer size, %s, is not a multiple of 16.\" % size)\n\n    pyvals = umath.geterrobj()\n    old = getbufsize()\n    pyvals[0] = size\n    umath.seterrobj(pyvals)\n    return old", "label": 1}
{"function": "def getbufsize():\n    \"\"\"\n    Return the size of the buffer used in ufuncs.\n\n    Returns\n    -------\n    getbufsize : int\n        Size of ufunc buffer in bytes.\n\n    \"\"\"\n    return umath.geterrobj()[0]", "label": 1}
{"function": "    def bulk_create(self, *args): ...", "label": 1}
{"function": "    def using(self, *args): ...", "label": 1}
{"function": "    def defer(self, *args, **kwargs): ...", "label": 1}
{"function": "    def describe(self):\n        return self.deep_extend(super(okcoinusd, self).describe(), {\n            'id': 'okcoinusd',\n            'name': 'OKCoin USD',\n            'countries': ['CN', 'US'],\n            'version': 'v1',\n            'rateLimit': 1000,  # up to 3000 requests per 5 minutes \u2248 600 requests per minute \u2248 10 requests per second \u2248 100 ms\n            'has': {\n                'CORS': False,\n                'fetchOHLCV': True,\n                'fetchOrder': True,\n                'fetchOrders': False,\n                'fetchOpenOrders': True,\n                'fetchClosedOrders': True,\n                'withdraw': True,\n                'futures': False,\n            },\n            'extension': '.do',  # appended to endpoint URL\n            'timeframes': {\n                '1m': '1min',\n                '3m': '3min',\n                '5m': '5min',\n                '15m': '15min',\n                '30m': '30min',\n                '1h': '1hour',\n                '2h': '2hour',\n                '4h': '4hour',\n                '6h': '6hour',\n                '12h': '12hour',\n                '1d': '1day',\n                '3d': '3day',\n                '1w': '1week',\n            },\n            'api': {\n                'web': {\n                    'get': [\n                        'spot/markets/currencies',\n                        'spot/markets/products',\n                    ],\n                },\n                'public': {\n                    'get': [\n                        'depth',\n                        'exchange_rate',\n                        'future_depth',\n                        'future_estimated_price',\n                        'future_hold_amount',\n                        'future_index',\n                        'future_kline',\n                        'future_price_limit',\n                        'future_ticker',\n                        'future_trades',\n                        'kline',\n                        'otcs',\n                        'ticker',\n                        'tickers',\n                        'trades',\n                    ],\n                },\n                'private': {\n                    'post': [\n                        'account_records',\n                        'batch_trade',\n                        'borrow_money',\n                        'borrow_order_info',\n                        'borrows_info',\n                        'cancel_borrow',\n                        'cancel_order',\n                        'cancel_otc_order',\n                        'cancel_withdraw',\n                        'funds_transfer',\n                        'future_batch_trade',\n                        'future_cancel',\n                        'future_devolve',\n                        'future_explosive',\n                        'future_order_info',\n                        'future_orders_info',\n                        'future_position',\n                        'future_position_4fix',\n                        'future_trade',\n                        'future_trades_history',\n                        'future_userinfo',\n                        'future_userinfo_4fix',\n                        'lend_depth',\n                        'order_fee',\n                        'order_history',\n                        'order_info',\n                        'orders_info',\n                        'otc_order_history',\n                        'otc_order_info',\n                        'repayment',\n                        'submit_otc_order',\n                        'trade',\n                        'trade_history',\n                        'trade_otc_order',\n                        'wallet_info',\n                        'withdraw',\n                        'withdraw_info',\n                        'unrepayments_info',\n                        'userinfo',\n                    ],\n                },\n            },\n            'urls': {\n                'logo': 'https://user-images.githubusercontent.com/1294454/27766791-89ffb502-5ee5-11e7-8a5b-c5950b68ac65.jpg',\n                'api': {\n                    'web': 'https://www.okcoin.com/v2',\n                    'public': 'https://www.okcoin.com/api',\n                    'private': 'https://www.okcoin.com/api',\n                },\n                'www': 'https://www.okcoin.com',\n                'doc': [\n                    'https://www.okcoin.com/rest_getStarted.html',\n                    'https://www.npmjs.com/package/okcoin.com',\n                ],\n            },\n            'fees': {\n                'trading': {\n                    'taker': 0.002,\n                    'maker': 0.002,\n                },\n            },\n            'exceptions': {\n                '1009': OrderNotFound,  # for spot markets, cancelling closed order\n                '1051': OrderNotFound,  # for spot markets, cancelling \"just closed\" order\n                '1019': OrderNotFound,  # order closed?\n                '20015': OrderNotFound,  # for future markets\n                '1013': InvalidOrder,  # no contract type(PR-1101)\n                '1027': InvalidOrder,  # createLimitBuyOrder(symbol, 0, 0): Incorrect parameter may exceeded limits\n                '1002': InsufficientFunds,  # \"The transaction amount exceed the balance\"\n                '1050': InvalidOrder,  # returned when trying to cancel an order that was filled or canceled previously\n                '10000': ExchangeError,  # createLimitBuyOrder(symbol, None, None)\n                '10005': AuthenticationError,  # bad apiKey\n                '10008': ExchangeError,  # Illegal URL parameter\n            },\n            'options': {\n                'warnOnFetchOHLCVLimitArgument': True,\n                'fiats': ['USD', 'CNY'],\n                'futures': {\n                    'BCH': True,\n                    'BTC': True,\n                    'BTG': True,\n                    'EOS': True,\n                    'ETC': True,\n                    'ETH': True,\n                    'LTC': True,\n                    'NEO': True,\n                    'QTUM': True,\n                    'USDT': True,\n                    'XUC': True,\n                },\n            },\n        })", "label": 1}
{"function": "    def fetch_markets(self):\n        response = self.webGetSpotMarketsProducts()\n        markets = response['data']\n        result = []\n        for i in range(0, len(markets)):\n            id = markets[i]['symbol']\n            baseId, quoteId = id.split('_')\n            baseIdUppercase = baseId.upper()\n            quoteIdUppercase = quoteId.upper()\n            base = self.common_currency_code(baseIdUppercase)\n            quote = self.common_currency_code(quoteIdUppercase)\n            symbol = base + '/' + quote\n            precision = {\n                'amount': markets[i]['maxSizeDigit'],\n                'price': markets[i]['maxPriceDigit'],\n            }\n            lot = math.pow(10, -precision['amount'])\n            minAmount = markets[i]['minTradeSize']\n            minPrice = math.pow(10, -precision['price'])\n            active = (markets[i]['online'] != 0)\n            baseNumericId = markets[i]['baseCurrency']\n            quoteNumericId = markets[i]['quoteCurrency']\n            market = self.extend(self.fees['trading'], {\n                'id': id,\n                'symbol': symbol,\n                'base': base,\n                'quote': quote,\n                'baseId': baseId,\n                'quoteId': quoteId,\n                'baseNumericId': baseNumericId,\n                'quoteNumericId': quoteNumericId,\n                'info': markets[i],\n                'type': 'spot',\n                'spot': True,\n                'future': False,\n                'lot': lot,\n                'active': active,\n                'precision': precision,\n                'limits': {\n                    'amount': {\n                        'min': minAmount,\n                        'max': None,\n                    },\n                    'price': {\n                        'min': minPrice,\n                        'max': None,\n                    },\n                    'cost': {\n                        'min': minAmount * minPrice,\n                        'max': None,\n                    },\n                },\n            })\n            result.append(market)\n            if (self.has['futures']) and(market['base'] in list(self.options['futures'].keys())):\n                fiats = self.options['fiats']\n                for j in range(0, len(fiats)):\n                    fiat = fiats[j]\n                    lowercaseFiat = fiat.lower()\n                    result.append(self.extend(market, {\n                        'quote': fiat,\n                        'symbol': market['base'] + '/' + fiat,\n                        'id': market['base'].lower() + '_' + lowercaseFiat,\n                        'quoteId': lowercaseFiat,\n                        'type': 'future',\n                        'spot': False,\n                        'future': True,\n                    }))\n        return result", "label": 1}
{"function": "    def fetch_order_book(self, symbol, limit=None, params={}):\n        self.load_markets()\n        market = self.market(symbol)\n        method = 'publicGet'\n        request = {\n            'symbol': market['id'],\n        }\n        if limit is not None:\n            request['size'] = limit\n        if market['future']:\n            method += 'Future'\n            request['contract_type'] = 'this_week'  # next_week, quarter\n        method += 'Depth'\n        orderbook = getattr(self, method)(self.extend(request, params))\n        return self.parse_order_book(orderbook)", "label": 1}
{"function": "    def parse_ticker(self, ticker, market=None):\n        timestamp = ticker['timestamp']\n        symbol = None\n        if not market:\n            if 'symbol' in ticker:\n                marketId = ticker['symbol']\n                if marketId in self.markets_by_id:\n                    market = self.markets_by_id[marketId]\n        if market:\n            symbol = market['symbol']\n        last = self.safe_float(ticker, 'last')\n        return {\n            'symbol': symbol,\n            'timestamp': timestamp,\n            'datetime': self.iso8601(timestamp),\n            'high': self.safe_float(ticker, 'high'),\n            'low': self.safe_float(ticker, 'low'),\n            'bid': self.safe_float(ticker, 'buy'),\n            'bidVolume': None,\n            'ask': self.safe_float(ticker, 'sell'),\n            'askVolume': None,\n            'vwap': None,\n            'open': None,\n            'close': last,\n            'last': last,\n            'previousClose': None,\n            'change': None,\n            'percentage': None,\n            'average': None,\n            'baseVolume': self.safe_float(ticker, 'vol'),\n            'quoteVolume': None,\n            'info': ticker,\n        }", "label": 1}
{"function": "    def fetch_ticker(self, symbol, params={}):\n        self.load_markets()\n        market = self.market(symbol)\n        method = 'publicGet'\n        request = {\n            'symbol': market['id'],\n        }\n        if market['future']:\n            method += 'Future'\n            request['contract_type'] = 'this_week'  # next_week, quarter\n        method += 'Ticker'\n        response = getattr(self, method)(self.extend(request, params))\n        ticker = self.safe_value(response, 'ticker')\n        if ticker is None:\n            raise ExchangeError(self.id + ' fetchTicker returned an empty response: ' + self.json(response))\n        timestamp = self.safe_integer(response, 'date')\n        if timestamp is not None:\n            timestamp *= 1000\n            ticker = self.extend(ticker, {'timestamp': timestamp})\n        return self.parse_ticker(ticker, market)", "label": 1}
{"function": "    def parse_trade(self, trade, market=None):\n        symbol = None\n        if market:\n            symbol = market['symbol']\n        return {\n            'info': trade,\n            'timestamp': trade['date_ms'],\n            'datetime': self.iso8601(trade['date_ms']),\n            'symbol': symbol,\n            'id': str(trade['tid']),\n            'order': None,\n            'type': None,\n            'side': trade['type'],\n            'price': self.safe_float(trade, 'price'),\n            'amount': self.safe_float(trade, 'amount'),\n        }", "label": 1}
{"function": "    def fetch_trades(self, symbol, since=None, limit=None, params={}):\n        self.load_markets()\n        market = self.market(symbol)\n        method = 'publicGet'\n        request = {\n            'symbol': market['id'],\n        }\n        if market['future']:\n            method += 'Future'\n            request['contract_type'] = 'this_week'  # next_week, quarter\n        method += 'Trades'\n        response = getattr(self, method)(self.extend(request, params))\n        return self.parse_trades(response, market, since, limit)", "label": 1}
{"function": "    def set_repository(self, *args, **kwargs):\n        \"\"\"\n        Alias for :meth:`Client.get_or_create_repository()`.\n\n        \"\"\"\n        return self.get_or_create_repository(*args, **kwargs)", "label": 1}
{"function": "    def get_or_create_registered_model(self, name=None, desc=None, labels=None, workspace=None, public_within_org=None, id=None):\n        \"\"\"\n        Attaches a registered_model to this Client.\n\n        If an accessible registered_model with name `name` does not already exist, it will be created\n        and initialized with specified metadata parameters. If such a registered_model does already exist,\n        it will be retrieved; specifying metadata parameters in this case will raise an exception.\n\n        Parameters\n        ----------\n        name : str, optional\n            Name of the registered_model. If no name is provided, one will be generated.\n        desc : str, optional\n            Description of the registered_model.\n        labels: list of str, optional\n            Labels of the registered_model.\n        workspace : str, optional\n            Workspace under which the registered_model with name `name` exists. If not provided, the current\n            user's personal workspace will be used.\n        public_within_org : bool, default False\n            If creating a registered_model in an organization's workspace, whether to make this registered_model\n            accessible to all members of that organization.\n        id : str, optional\n            ID of the registered_model. This parameter cannot be provided alongside `name`, and other\n            parameters will be ignored.\n\n        Returns\n        -------\n        :class:`~verta._registry.model.RegisteredModel`\n\n        Raises\n        ------\n        ValueError\n            If a registered_model with `name` already exists, but metadata parameters are passed in.\n\n        \"\"\"\n        if name is not None and id is not None:\n            raise ValueError(\"cannot specify both `name` and `id`\")\n\n        name = self._set_from_config_if_none(name, \"registered_model\")\n        workspace = self._set_from_config_if_none(workspace, \"workspace\")\n\n        if workspace is None:\n            workspace = self._get_personal_workspace()\n\n        self._ctx = _Context(self._conn, self._conf)\n        self._ctx.workspace_name = workspace\n\n        if id is not None:\n            registered_model = RegisteredModel._get_by_id(self._conn, self._conf, id)\n        else:\n            registered_model = RegisteredModel._get_or_create_by_name(self._conn, name,\n                                                                                lambda name: RegisteredModel._get_by_name(self._conn, self._conf, name, self._ctx.workspace_name),\n                                                                                lambda name: RegisteredModel._create(self._conn, self._conf, self._ctx, name=name, desc=desc, tags=labels, public_within_org=public_within_org))\n\n        return registered_model", "label": 1}
{"function": "    def get_registered_model(self, name=None, workspace=None, id=None):\n        \"\"\"\n        Retrieve an already created Registered Model. Only one of name or id can be provided.\n\n        Parameters\n        ----------\n        name : str, optional\n            Name of the Registered Model.\n        id : str, optional\n            ID of the Registered Model. This parameter cannot be provided alongside `name`.\n\n        Returns\n        -------\n        :class:`~verta._registry.model.RegisteredModel`\n        \"\"\"\n        if name is not None and id is not None:\n            raise ValueError(\"cannot specify both `name` and `id`\")\n\n        name = self._set_from_config_if_none(name, \"registered_model\")\n        if name is None and id is None:\n            raise ValueError(\"must specify either `name` or `id`\")\n        workspace = self._set_from_config_if_none(workspace, \"workspace\")\n        if workspace is None:\n            workspace = self._get_personal_workspace()\n\n        self._ctx = _Context(self._conn, self._conf)\n        self._ctx.workspace_name = workspace\n\n        if id is not None:\n            registered_model = RegisteredModel._get_by_id(self._conn, self._conf, id)\n        else:\n            registered_model =  RegisteredModel._get_by_name(self._conn, self._conf, name, self._ctx.workspace_name)\n\n        if registered_model is None:\n            raise ValueError(\"Registered model not found\")\n\n        return registered_model", "label": 1}
{"function": "    def set_registered_model(self, *args, **kwargs):\n        \"\"\"\n        Alias for :meth:`Client.get_or_create_registered_model()`.\n\n        \"\"\"\n        return self.get_or_create_registered_model(*args, **kwargs)", "label": 1}
{"function": "    def get_registered_model_version(self, id):\n        \"\"\"\n        Retrieve an already created Model Version.\n\n        Parameters\n        ----------\n        id : str\n            ID of the Model Version.\n\n        Returns\n        -------\n        :class:`~verta._registry.modelversion.RegisteredModelVersion`\n        \"\"\"\n        return RegisteredModelVersion._get_by_id(self._conn, self._conf, id)", "label": 1}
{"function": "    def registered_models(self):\n        return RegisteredModels(self._conn, self._conf)", "label": 1}
{"function": "    def registered_model_versions(self):\n        return RegisteredModelVersions(self._conn, self._conf)", "label": 1}
{"function": "    def get_or_create_endpoint(self, path=None, description=None, workspace=None, id=None):\n        \"\"\"\n        Attaches an endpoint to this Client.\n\n        If an accessible endpoint with name `path` does not already exist, it will be created\n        and initialized with specified metadata parameters. If such an endpoint does already exist,\n        it will be retrieved; specifying metadata parameters in this case will raise an exception.\n\n        Parameters\n        ----------\n        path : str, optional\n            Path for the endpoint.\n        description : str, optional\n            Description of the endpoint.\n        workspace : str, optional\n            Workspace under which the endpoint with name `name` exists. If not provided, the current\n            user's personal workspace will be used.\n        id : str, optional\n            ID of the endpoint. This parameter cannot be provided alongside `name`, and other\n            parameters will be ignored.\n\n        Returns\n        -------\n        :class:`~verta.endpoint._endpoint.Endpoint`\n\n        Raises\n        ------\n        ValueError\n            If an endpoint with `path` already exists, but metadata parameters are passed in.\n\n        \"\"\"\n        if path is not None and id is not None:\n            raise ValueError(\"cannot specify both `path` and `id`\")\n        if path is None and id is None:\n            raise ValueError(\"must specify either `path` or `id`\")\n\n        workspace = self._set_from_config_if_none(workspace, \"workspace\")\n        if workspace is None:\n            workspace = self._get_personal_workspace()\n        if id is not None:\n            return Endpoint._get_by_id(self._conn, self._conf, workspace, id)\n        else:\n            return Endpoint._get_or_create_by_name(self._conn, path,\n                                            lambda name: Endpoint._get_by_path(self._conn, self._conf, workspace, path),\n                                            lambda name: Endpoint._create( self._conn, self._conf, workspace, path, description))", "label": 1}
{"function": "    def get_endpoint(self, path=None, workspace=None, id=None):\n        \"\"\"\n        Retrieves an already created Endpoint. Only one of `path` or `id` can be provided.\n\n        Parameters\n        ----------\n        path : str, optional\n            Path of the Endpoint.\n        workspace : str, optional\n            Name of the workspace of the Endpoint.\n        id : str, optional\n            ID of the Endpoint. This parameter cannot be provided alongside `path`.\n\n        Returns\n        -------\n        :class:`~verta.endpoint._endpoint.Endpoint`\n\n        \"\"\"\n        if path is not None and id is not None:\n            raise ValueError(\"cannot specify both `path` and `id`\")\n        if path is None and id is None:\n            raise ValueError(\"must specify either `path` or `id`\")\n\n        workspace = self._set_from_config_if_none(workspace, \"workspace\")\n        if workspace is None:\n            workspace = self._get_personal_workspace()\n\n        if id is not None:\n            endpoint = Endpoint._get_by_id(self._conn, self._conf, workspace, id)\n        else:\n            endpoint = Endpoint._get_by_path(self._conn, self._conf, workspace, path)\n\n        if endpoint is None:\n            raise ValueError(\"Endpoint not found\")\n        return endpoint", "label": 1}
{"function": "    def set_endpoint(self, *args, **kwargs):\n        \"\"\"\n        Alias for :meth:`Client.get_or_create_endpoint()`.\n\n        \"\"\"\n        return self.get_or_create_endpoint(*args, **kwargs)", "label": 1}
{"function": "    def test_single(self):\n        self.l1.flush()\n\n        # Send a datagram...\n        dg = Datagram.create([1234], 4321, 1337)\n        dg.add_string('HELLO')\n        self.c1.send(dg)\n\n        # Make sure the MD passes it upward.\n        self.expect(self.l1, dg)", "label": 1}
{"function": "    def test_subscribe(self):\n        self.l1.flush()\n        self.c1.flush()\n        self.c2.flush()\n\n        # Subscribe to a channel...\n        dg = Datagram.create_add_channel(12345654321)\n        self.c1.send(dg)\n        self.expectNone(self.c1)\n        # Make sure the MD subscribes to its parent.\n        self.expect(self.l1, dg)\n\n        # Send a test datagram on second connection...\n        dg = Datagram()\n        dg = Datagram.create([12345654321], 0, 1234)\n        dg.add_uint32(0xDEADBEEF)\n        self.c2.send(dg)\n        self.expect(self.c1, dg)\n        # MD will, of course, relay this datagram upward.\n        self.expect(self.l1, dg)\n\n        # Subscribe on the second connection...\n        dg = Datagram.create_add_channel(12345654321)\n        self.c2.send(dg)\n        self.expectNone(self.c2)\n        # MD should not ask for the channel a second time.\n        self.expectNone(self.l1)\n\n        # Send a test datagram on first connection...\n        dg = Datagram.create([12345654321], 0, 1234)\n        dg.add_uint32(0xDEADBEEF)\n        self.c1.send(dg)\n        self.expect(self.c2, dg) # Should be relayed to second.\n        self.expect(self.l1, dg) # Should be sent upward.\n        #self.expectNone(self.c1) # Should NOT be echoed back.\n\n        # Unsubscribe on the first connection...\n        dg = Datagram.create_remove_channel(12345654321)\n        self.c1.send(dg)\n        self.expectNone(self.c1)\n        self.expectNone(self.c2)\n        # MD should NOT unsubscribe from parent!\n        self.expectNone(self.l1)\n\n        # Send another test datagram on second connection...\n        dg = Datagram.create([12345654321], 0, 1234)\n        dg.add_uint32(0xDEADBEEF)\n        self.c2.send(dg)\n        self.expect(self.l1, dg) # Should be sent upward.\n        self.expectNone(self.c2) # Should NOT be relayed.\n        self.expectNone(self.c1) # Should NOT be echoed back.\n\n        # Abandon the second connection, which should auto-unsubscribe it.\n        self.c2.close()\n        self.__class__.c2 = self.connectToServer()\n        self.expectNone(self.c1)\n        # MD should unsubscribe from parent.\n        self.expect(self.l1, Datagram.create_remove_channel(12345654321))", "label": 1}
{"function": "    def test_multi(self):\n        self.l1.flush()\n        self.c1.flush()\n        self.c2.flush()\n\n        # Subscribe to a pair of channels on c1.\n        for channel in [1111, 2222]:\n            dg = Datagram.create_add_channel(channel)\n            self.c1.send(dg)\n\n        # Subscribe to another pair of channels on c2.\n        for channel in [2222, 3333]:\n            dg = Datagram.create_add_channel(channel)\n            self.c2.send(dg)\n\n        self.l1.flush() # Don't care about the subscribe messages.\n\n        # Sanity check: A datagram on channel 2222 should be delivered to both.\n        dg = Datagram.create([2222], 0, 1234)\n        dg.add_uint32(0xDEADBEEF)\n        self.l1.send(dg)\n        self.expect(self.c1, dg)\n        self.expect(self.c2, dg)\n\n        # A datagram to channels 1111 and 3333 should be delievered to both.\n        dg = Datagram.create([1111, 3333], 0, 1234)\n        dg.add_uint32(0xDEADBEEF)\n        self.l1.send(dg)\n        self.expect(self.c1, dg)\n        self.expect(self.c2, dg)\n\n        # A datagram should only be delivered once if multiple channels match.\n        dg = Datagram.create([1111, 2222], 0, 1234)\n        dg.add_uint32(0xDEADBEEF)\n        self.l1.send(dg)\n        self.expect(self.c1, dg)\n        self.expectNone(self.c1)\n        self.expect(self.c2, dg)\n\n        # Let's try something really absurd:\n        dg = Datagram.create([1111, 2222, 3333, 1111, 1111,\n                              2222, 3333, 3333, 2222], 0, 1234)\n        dg.add_uint32(0xDEADBEEF)\n        self.l1.send(dg)\n        self.expect(self.c1, dg)\n        self.expectNone(self.c1)\n        self.expect(self.c2, dg)\n        self.expectNone(self.c2)\n\n        # And, of course, sending this monstrosity on c1 should result in it\n        # showing up on c2 and l1 once only; no echo back on c1.\n        self.c1.send(dg)\n        self.expect(self.l1, dg)\n        self.expectNone(self.l1)\n        self.expect(self.c2, dg)\n        self.expectNone(self.c2)\n        self.expectNone(self.c1)\n\n        # Unsubscribe the channels...\n        for channel in [1111, 2222, 3333]:\n            dg = Datagram.create_remove_channel(channel)\n            self.c1.send(dg)\n            self.c2.send(dg)", "label": 1}
{"function": "    def test_post_remove(self):\n        self.l1.flush()\n        self.c1.flush()\n        self.c2.flush()\n\n        # Create a datagram to be sent post-remove...\n        dg_pr = Datagram.create([555444333], 0, 4321)\n        dg_pr.add_string('Testing...')\n\n        # Hang it on c1...\n        dg_add_pr = Datagram.create_add_post_remove(171717, dg_pr)\n        self.c1.send(dg_add_pr)\n\n        # Expect post remove to be pre-routed upstream\n        self.expect(self.l1, dg_add_pr)\n\n        # Verify nothing else is happening yet...\n        self.expectNone(self.l1)\n        self.expectNone(self.c1)\n        self.expectNone(self.c2)\n\n        # Reconnect c1 and see if dg gets sent.\n        self.c1.close()\n        self.__class__.c1 = self.connectToServer()\n\n        # The upstream should receive the post remove and also receive a clear_post_removes\n        dg_clear_prs = Datagram.create_clear_post_removes(171717)\n        self.expectMany(self.l1, [dg_pr, dg_clear_prs])\n\n        # Reconnect c1, the message shouldn't be sent again\n        self.c1.close()\n        self.__class__.c1 = self.connectToServer()\n        self.expectNone(self.l1)\n\n        # Hang dg as a post-remove for c2...\n        dg_add_pr = Datagram.create_add_post_remove(181818, dg_pr)\n        self.c2.send(dg_add_pr)\n        self.expect(self.l1, dg_add_pr)\n\n        # Wait, nevermind...\n        dg_clear_prs = Datagram.create_clear_post_removes(181818)\n        self.c2.send(dg_clear_prs)\n        self.expect(self.l1, dg_clear_prs)\n\n        # Did the cancellation work?\n        self.c2.close()\n        self.__class__.c2 = self.connectToServer()\n        self.expectNone(self.l1)\n\n        # Try hanging multiple post-removes on c1\n        dg_add_pr = Datagram.create_add_post_remove(191919, dg_pr)\n        dg_pr2 = Datagram.create([987987987], 0, 6959)\n        dg_add_pr2 = Datagram.create_add_post_remove(191919, dg_pr2)\n        dg_pr3 = Datagram.create([986666687], 0, 1252)\n        dg_add_pr3 = Datagram.create_add_post_remove(202020, dg_pr3)\n        self.c1.send(dg_add_pr)\n        self.c1.send(dg_add_pr2)\n        self.c1.send(dg_add_pr3)\n\n        # Expect post removes to be pre-routed upstream\n        self.expectMany(self.l1, [dg_add_pr, dg_add_pr2, dg_add_pr3])\n\n        # After adding three, we don't want to see anything \"pushed\" or the like\n        self.expectNone(self.l1)\n        self.expectNone(self.c1)\n        self.expectNone(self.c2)\n\n        # Reconnect c1 and see if both datagrams gets sent ...\n        self.c1.close()\n        self.__class__.c1 = self.connectToServer()\n\n        # ... expecting the post removes ...\n        expected = [dg_pr, dg_pr2, dg_pr3]\n        # ... and a clear for each channel ...\n        dg_clear_prs = Datagram.create_clear_post_removes(191919)\n        dg_clear_prs2 = Datagram.create_clear_post_removes(202020)\n        expected += [dg_clear_prs, dg_clear_prs2]\n        # ... exactly 2 Post Removes, and 3 Clears ...\n        self.expectMany(self.l1, expected)\n        # ... and no more messages (duplicates or otherwise)\n        self.expectNone(self.l1)", "label": 1}
{"function": "    def test_ranges(self):\n        self.l1.flush()\n        self.c1.flush()\n        self.c2.flush()\n\n        # Subscribe to range 1000-1999...\n        dg = Datagram.create_add_range(1000, 1999)\n        self.c1.send(dg)\n        # Verify that l1 asks for the range as well...\n        self.expect(self.l1, dg)\n\n        # Send messages on a few channels on c2, see which ones c1 gets.\n        def check_channels(channels):\n            for channel, should_receive in channels:\n                dg = Datagram.create([channel], 123456789, 5858)\n                dg.add_uint16(channel) # For some semblance of uniqueness\n\n                self.c2.send(dg)\n\n                if should_receive:\n                    self.expect(self.c1, dg)\n                    self.expectNone(self.c1) # No repeats!\n                else:\n                    self.expectNone(self.c1)\n\n                # And, of course, l1 receives all of these:\n                self.expect(self.l1, dg)\n\n        check_channels([\n            (500, False),\n            (999, False),\n            (1000, True),\n            (1001, True),\n            (1299, True),\n            (1300, True),\n            (1500, True),\n            (1699, True),\n            (1700, True),\n            (1701, True),\n            (1900, True),\n            (1999, True),\n            (2000, False),\n            (2050, False),\n            (2500, False)])\n\n        # Ranged-subscriptions should still receive messages only once, even if\n        # multiple channels are in the range.\n        dg = Datagram.create([500, 1001, 1500], 0, 34)\n        dg.add_string('test')\n        self.c2.send(dg)\n        self.expect(self.c1, dg)\n        self.expectNone(self.c1) # No repeats!\n        self.expect(self.l1, dg)\n\n        # Now let's \"slice\" the range.\n        dg = Datagram.create_remove_range(1300, 1700)\n        self.c1.send(dg)\n        # l1 should request the slice upward\n        self.expect(self.l1, dg)\n\n        # And the slice should be gone:\n        check_channels([\n            (500, False),\n            (999, False),\n            (1000, True),\n            (1001, True),\n            (1299, True),\n            (1300, False),\n            (1500, False),\n            (1699, False),\n            (1700, False),\n            (1701, True),\n            (1900, True),\n            (1999, True),\n            (2000, False),\n            (2050, False),\n            (2500, False)])\n\n        # How about adding a second range that overlaps?\n        dg = Datagram.create_add_range(1900, 2100)\n        self.c1.send(dg)\n        # Verify that md asks for the entire range from l1...\n        self.expect(self.l1, dg)\n\n        # Now the subscriptions should be updated:\n        check_channels([\n            (500, False),\n            (999, False),\n            (1000, True),\n            (1001, True),\n            (1299, True),\n            (1300, False),\n            (1500, False),\n            (1699, False),\n            (1700, False),\n            (1701, True),\n            (1900, True),\n            (1999, True),\n            (2000, True),\n            (2050, True),\n            (2500, False)])\n\n        # Drop that first range...\n        dg = Datagram.create_remove_range(1000, 1999)\n        self.c1.send(dg)\n\n        # -- See comments after block --\n        ## And again, l1 should ask for difference only...\n        #expected = []\n        ## Difference #1: Drop 1000-1299\n        #expected.append(Datagram.create_remove_range(1000, 1299))\n        ## Difference #2: Drop 1701-1999\n        #expected.append(Datagram.create_remove_range(1701, 1999))\n        #self.expectMany(self.l1, expected)\n\n        # In this case because there are no remaining subscriptions in the\n        # entire removed range, it is more efficient for the network, CPU,\n        # and memory to just forward the entire range.\n        dg = Datagram.create_remove_range(1000, 1999)\n        self.expect(self.l1, dg)\n\n        # Now see if only the second range is active...\n        check_channels([\n            (500, False),\n            (999, False),\n            (1000, False),\n            (1001, False),\n            (1999, False),\n            (2000, True),\n            (2050, True),\n            (2500, False)])\n\n        # Grand finale: Cut c1 and see if the remaining range dies.\n        self.c1.close()\n        self.c1 = self.connectToServer()\n        self.expect(self.l1, Datagram.create_remove_range(2000, 2100))\n        self.expectNone(self.l1)\n\n        # ... but we lied! Now there are some more tests\n\n        # Lets add a new range to play with\n        dg = Datagram.create_add_range(3000, 5000)\n        self.c1.send(dg)\n        self.expect(self.l1, dg)\n        check_channels([\n            (2499, False),\n            (2501, False),\n            (2999, False),\n            (3000, True),\n            (4000, True),\n            (4372, True),\n            (5000, True),\n            (5001, False),\n            (5109, False)])\n\n        # Test removing a range that intersects with the front part\n        dg = Datagram.create_remove_range(2950, 3043)\n        self.c1.send(dg)\n        # --> and expect only the subscribed part to be removed\n        dg = Datagram.create_remove_range(3000, 3043)\n        self.expect(self.l1, dg)\n        check_channels([\n            (2913, False),\n            (2999, False),\n            (3000, False),\n            (3043, False),\n            (3044, True),\n            (4000, True),\n            (4372, True),\n            (5000, True),\n            (5001, False),\n            (5109, False)])\n\n        # Test removing a range that intersects with the end part\n        dg = Datagram.create_remove_range(4763, 6000)\n        self.c1.send(dg)\n        # --> and expect only the subscribed part to be removed\n        dg = Datagram.create_remove_range(4763, 5000)\n        self.expect(self.l1, dg)\n        check_channels([\n            (3000, False),\n            (3043, False),\n            (3044, True),\n            (4000, True),\n            (4372, True),\n            (4762, True),\n            (4763, False),\n            (5000, False),\n            (5001, False),\n            (5109, False)])\n\n        # Now remove some from the middle again so we can test weird intersections\n        dg = Datagram.create_remove_range(3951, 4049)\n        self.c1.send(dg)\n        # The entire range is subscribed, so it should all be unsubscribed\n        self.expect(self.l1, dg)\n        check_channels([\n            (3043, False),\n            (3044, True),\n            (3802, True),\n            (3950, True),\n            (3951, False),\n            (4049, False),\n            (4050, True),\n            (4133, True),\n            (4762, True),\n            (4763, False)])\n\n        # Ok... remove an intersection from the lower half of the upper range\n        dg = Datagram.create_remove_range(4030, 4070)\n        self.c1.send(dg)\n        # N.B. Its worth considering which of the following behaviors is preferred,\n        #      the first is a lot more work for the current MD, but may reduce the\n        #      work for other MDs. Consider performance testing both implementations.\n        ## --> and expect only the subscribed part to be removed\n        #self.expect(self.l1, Datagram.create_remove_range(4050, 4070))\n        self.expect(self.l1, dg)\n        check_channels([\n            (3043, False),\n            (3044, True),\n            (3802, True),\n            (3950, True),\n            (3951, False),\n            (4070, False),\n            (4071, True),\n            (4133, True),\n            (4762, True),\n            (4763, False)])\n\n        # Now remove an intersection from the upper half of the lower range\n        dg = Datagram.create_remove_range(3891, 4040)\n        self.c1.send(dg)\n        # N.B. Its worth considering which of the following behaviors is preferred,\n        #      the first is a lot more work for the current MD, but may reduce the\n        #      work for other MDs. Consider performance testing both implementations.\n        ## --> and expect only the subscribed part to be removed\n        #self.expect(self.l1, Datagram.create_remove_range(3891, 3950))\n        self.expect(self.l1, dg)\n        check_channels([\n            (3043, False),\n            (3044, True),\n            (3672, True),\n            (3890, True),\n            (3891, False),\n            (3893, False),\n            (4070, False),\n            (4071, True),\n            (4762, True),\n            (4763, False)])\n\n        # Now lets intersect part of both the upper and lower range\n        dg = Datagram.create_remove_range(3700, 4200)\n        self.c1.send(dg)\n        # N.B. Its worth considering which of the following behaviors is preferred,\n        #      the first is a lot more work for the current MD, but may reduce the\n        #      work for other MDs. Consider performance testing both implementations.\n        #      Additionally the first requires twice as much network traffice, but\n        #      it is still relatively small on a relatively infrequent operation.\n        ## --> and expect only the subscribed parts to be removed\n        #expected = []\n        #expected.append(Datagram.create_remove_range(3700, 3890))\n        #expected.append(Datagram.create_remove_range(4070, 4200))\n        #self.expectMany(self.l1, expected)\n        self.expect(self.l1, dg)\n        check_channels([\n            (3043, False),\n            (3044, True),\n            (3699, True),\n            (3700, False),\n            (4200, False),\n            (4201, True),\n            (4762, True),\n            (4763, False)])\n\n        # Now lets subscribe our 2nd client to an intersecting range\n        dg = Datagram.create_add_range(3500, 4500)\n        self.c2.send(dg)\n        self.expect(self.l1, dg)\n\n        # Now remove an upper part of the lower range that is contained within c2's range\n        dg = Datagram.create_remove_range(3650, 3800)\n        self.c1.send(dg)\n        # We shouldn't get a remove for this upstream, because the 2nd client is still interested\n        self.expectNone(self.l1)\n        check_channels([\n            # Lower range\n            (3043, False),\n            (3044, True), # lower bound\n            (3333, True),\n            (3480, True),\n            (3499, True),\n            (3500, True),\n            (3649, True), # upper bound\n            (3650, False),\n\n            (3787, False),\n            (4000, False),\n\n            # Upper range\n            (4200, False),\n            (4201, True)]) # lower bound\n\n        # Now remove part of the lower range that contains just the lower bound of c2's range,\n        # but not the upper bound of the lower range.\n        dg = Datagram.create_remove_range(3475, 3525)\n        self.c1.send(dg)\n        # We should expect to receive only the portion of the range which is outside c2's range\n        self.expect(self.l1, Datagram.create_remove_range(3475, 3499))\n        check_channels([\n            # Lower range\n            (3043, False),\n            (3044, True), # lower bound\n            (3474, True), # upper bound\n            (3475, False),\n\n            (3482, False),\n            (3499, False),\n\n            # Mid range\n            (3525, False),\n            (3526, True), # Lower bound\n            (3600, True),\n            (3649, True), # upper bound\n            (3650, False),\n\n            # Upper range\n            (4200, False),\n            (4201, True)]) # lower bound\n\n        # Now remove a range from c2 which contains the mid-range's upper bound\n        # and the upper-range's lower bound.\n        dg = Datagram.create_remove_range(3620, 4300)\n        self.c2.send(dg)\n        # We should expect to recieve only the portion of c2 which is between the two ranges\n        self.expect(self.l1, Datagram.create_remove_range(3650, 4200))\n        check_channels([\n            # Lower range\n            (3474, True), # upper bound\n            (3475, False),\n\n            # Mid range\n            (3525, False),\n            (3526, True), # Lower bound\n            (3649, True), # upper bound\n            (3650, False),\n\n            # Upper range\n            (4200, False),\n            (4201, True), # lower bound\n            (4762, True), # upper bound\n            (4763, False)])\n\n        # Cut c2 and watch the part die that is not in c1\n        self.c2.close()\n        self.c2 = self.connectToServer()\n        self.expect(self.l1, Datagram.create_remove_range(3500, 3525))\n        self.expectNone(self.l1)\n\n        # Now add c2 such that it contains all the c1 ranges\n        dg = Datagram.create_add_range(1000, 5000)\n        self.c2.send(dg)\n        self.expect(self.l1, dg)\n\n        # Then remove that range and see all the inbetween parts die\n        self.c2.send(Datagram.create_remove_range(1000, 5000))\n        expected = []\n        expected.append(Datagram.create_remove_range(1000, 3043))\n        expected.append(Datagram.create_remove_range(3475, 3525))\n        expected.append(Datagram.create_remove_range(3650, 4200))\n        expected.append(Datagram.create_remove_range(4763, 5000))\n        self.expectMany(self.l1, expected)\n        self.expectNone(self.l1)\n\n        # Cleanup\n        self.c1.close()\n        self.c2.close()\n        self.__class__.c1 = self.connectToServer()\n        self.__class__.c2 = self.connectToServer()\n        self.l1.flush()", "label": 1}
{"function": "    def test_malformed_control(self):\n        dg = Datagram()\n        dg.add_uint16(0) # Datagram length\n        dg.add_uint8(1) # Number of recipients\n        dg.add_channel(CONTROL_CHANNEL) # Recipient\n        dg.add_channel(0) # Sender\n        dg.add_uint16(CONTROL_ADD_CHANNEL)\n        dg.add_channel(0x1CEDC0FF)\n        self.c1.s.send(dg.get_data())\n        # The MD should drop this packet completely.\n        self.expectNone(self.l1)\n        # The MD should drop the misbehaving participant.\n        self.assertRaises(EOFError, self.c1.recv_maybe())\n\n        # Cleanup\n        self.c1.close()\n        self.__class__.c1 = self.connectToServer()\n        self.l1.flush()", "label": 1}
{"function": "    def test_malformed_single(self):\n        dg = Datagram()\n        dg.add_uint16(0) # Datagram length\n        dg.add_uint8(1) # Number of recipients\n        dg.add_channel(12341234) # Recipient\n        dg.add_channel(0) # Sender\n        dg.add_string('Ex nihilo, alea iacta est. Tohuvabohu!')\n        self.c1.s.send(dg.get_data())\n        # The MD should drop this packet completely.\n        self.expectNone(self.l1)\n        # The MD should drop the misbehaving participant.\n        self.assertRaises(EOFError, self.c1.recv_maybe())\n\n        # Cleanup\n        self.c1.close()\n        self.__class__.c1 = self.connectToServer()\n        self.l1.flush()", "label": 1}
{"function": "        def check_channels(channels):\n            for channel, should_receive in channels:\n                dg = Datagram.create([channel], 123456789, 5858)\n                dg.add_uint16(channel) # For some semblance of uniqueness\n\n                self.c2.send(dg)\n\n                if should_receive:\n                    self.expect(self.c1, dg)\n                    self.expectNone(self.c1) # No repeats!\n                else:\n                    self.expectNone(self.c1)\n\n                # And, of course, l1 receives all of these:\n                self.expect(self.l1, dg)", "label": 1}
{"function": "def form_data_consumer(request):\n    result_object = request.args['object']\n    if result_object == 'text':\n        return Response(repr(request.form['text']))\n    f = request.files[result_object]\n    return Response(b'\\n'.join((\n        repr(f.filename).encode('ascii'),\n        repr(f.name).encode('ascii'),\n        repr(f.content_type).encode('ascii'),\n        f.stream.read()\n    )))", "label": 1}
{"function": "def get_contents(filename):\n    with open(filename, 'rb') as f:\n        return f.read()", "label": 1}
{"function": "def _warn_if_string(iterable):\n    \"\"\"Helper for the response objects to check if the iterable returned\n    to the WSGI server is not a string.\n    \"\"\"\n    if isinstance(iterable, string_types):\n        from warnings import warn\n        warn(Warning('response iterable was set to a string.  This appears '\n                     'to work but means that the server will send the '\n                     'data to the client char, by char.  This is almost '\n                     'never intended behavior, use response.data to assign '\n                     'strings to the response object.'), stacklevel=2)", "label": 1}
{"function": "def _assert_not_shallow(request):\n    if request.shallow:\n        raise RuntimeError('A shallow request tried to consume '\n                           'form data.  If you really want to do '\n                           'that, set `shallow` to False.')", "label": 1}
{"function": "def _iter_encoded(iterable, charset):\n    for item in iterable:\n        if isinstance(item, text_type):\n            yield item.encode(charset)\n        else:\n            yield item", "label": 1}
{"function": "    def __init__(self, environ, populate_request=True, shallow=False):\n        self.environ = environ\n        if populate_request and not shallow:\n            self.environ['werkzeug.request'] = self\n        self.shallow = shallow", "label": 1}
{"function": "    def __repr__(self):\n        # make sure the __repr__ even works if the request was created\n        # from an invalid WSGI environment.  If we display the request\n        # in a debug session we don't want the repr to blow up.\n        args = []\n        try:\n            args.append(\"'%s'\" % self.url)\n            args.append('[%s]' % self.method)\n        except Exception:\n            args.append('(invalid WSGI environ)')\n\n        return '<%s %s>' % (\n            self.__class__.__name__,\n            ' '.join(args)\n        )", "label": 1}
{"function": "    def url_charset(self):\n        \"\"\"The charset that is assumed for URLs.  Defaults to the value\n        of :attr:`charset`.\n\n        .. versionadded:: 0.6\n        \"\"\"\n        return self.charset", "label": 1}
{"function": "    def from_values(cls, *args, **kwargs):\n        \"\"\"Create a new request object based on the values provided.  If\n        environ is given missing values are filled from there.  This method is\n        useful for small scripts when you need to simulate a request from an URL.\n        Do not use this method for unittesting, there is a full featured client\n        object (:class:`Client`) that allows to create multipart requests,\n        support for cookies etc.\n\n        This accepts the same options as the\n        :class:`~werkzeug.test.EnvironBuilder`.\n\n        .. versionchanged:: 0.5\n           This method now accepts the same arguments as\n           :class:`~werkzeug.test.EnvironBuilder`.  Because of this the\n           `environ` parameter is now called `environ_overrides`.\n\n        :return: request object\n        \"\"\"\n        from werkzeug.test import EnvironBuilder\n        charset = kwargs.pop('charset', cls.charset)\n        kwargs['charset'] = charset\n        builder = EnvironBuilder(*args, **kwargs)\n        try:\n            return builder.get_request(cls)\n        finally:\n            builder.close()", "label": 1}
{"function": "    def application(cls, f):\n        \"\"\"Decorate a function as responder that accepts the request as first\n        argument.  This works like the :func:`responder` decorator but the\n        function is passed the request object as first argument and the\n        request object will be closed automatically::\n\n            @Request.application\n            def my_wsgi_app(request):\n                return Response('Hello World!')\n\n        :param f: the WSGI callable to decorate\n        :return: a new WSGI callable\n        \"\"\"\n        #: return a callable that wraps the -2nd argument with the request\n        #: and calls the function with all the arguments up to that one and\n        #: the request.  The return value is then called with the latest\n        #: two arguments.  This makes it possible to use this decorator for\n        #: both methods and standalone WSGI functions.\n        def application(*args):\n            request = cls(args[-2])\n            with request:\n                return f(*args[:-2] + (request,))(*args[-2:])\n        return update_wrapper(application, f)", "label": 1}
{"function": "    def _get_file_stream(self, total_content_length, content_type, filename=None,\n                        content_length=None):\n        \"\"\"Called to get a stream for the file upload.\n\n        This must provide a file-like class with `read()`, `readline()`\n        and `seek()` methods that is both writeable and readable.\n\n        The default implementation returns a temporary file if the total\n        content length is higher than 500KB.  Because many browsers do not\n        provide a content length for the files only the total content\n        length matters.\n\n        :param total_content_length: the total content length of all the\n                                     data in the request combined.  This value\n                                     is guaranteed to be there.\n        :param content_type: the mimetype of the uploaded file.\n        :param filename: the filename of the uploaded file.  May be `None`.\n        :param content_length: the length of this file.  This value is usually\n                               not provided because webbrowsers do not provide\n                               this value.\n        \"\"\"\n        return default_stream_factory(total_content_length, content_type,\n                                      filename, content_length)", "label": 1}
{"function": "    def want_form_data_parsed(self):\n        \"\"\"Returns True if the request method carries content.  As of\n        Werkzeug 0.9 this will be the case if a content type is transmitted.\n\n        .. versionadded:: 0.8\n        \"\"\"\n        return bool(self.environ.get('CONTENT_TYPE'))", "label": 1}
{"function": "    def _set_retry_after(self, value):\n        if value is None:\n            if 'retry-after' in self.headers:\n                del self.headers['retry-after']\n            return\n        elif isinstance(value, datetime):\n            value = http_date(value)\n        else:\n            value = str(value)\n        self.headers['Retry-After'] = value", "label": 1}
{"function": "    def _set_property(name, doc=None):\n        def fget(self):\n            def on_update(header_set):\n                if not header_set and name in self.headers:\n                    del self.headers[name]\n                elif header_set:\n                    self.headers[name] = header_set.to_header()\n            return parse_set_header(self.headers.get(name), on_update)\n        def fset(self, value):\n            if not value:\n                del self.headers[name]\n            elif isinstance(value, string_types):\n                self.headers[name] = value\n            else:\n                self.headers[name] = dump_header(value)\n        return property(fget, fset, doc=doc)", "label": 1}
{"function": "    def www_authenticate(self):\n        \"\"\"The `WWW-Authenticate` header in a parsed form.\"\"\"\n        def on_update(www_auth):\n            if not www_auth and 'www-authenticate' in self.headers:\n                del self.headers['www-authenticate']\n            elif www_auth:\n                self.headers['WWW-Authenticate'] = www_auth.to_header()\n        header = self.headers.get('www-authenticate')\n        return parse_www_authenticate_header(header, on_update)", "label": 1}
{"function": "        def application(*args):\n            request = cls(args[-2])\n            with request:\n                return f(*args[:-2] + (request,))(*args[-2:])", "label": 1}
{"function": "        def on_update(cache_control):\n            if not cache_control and 'cache-control' in self.headers:\n                del self.headers['cache-control']\n            elif cache_control:\n                self.headers['Cache-Control'] = cache_control.to_header()", "label": 1}
{"function": "        def on_update(rng):\n            if not rng:\n                del self.headers['content-range']\n            else:\n                self.headers['Content-Range'] = rng.to_header()", "label": 1}
{"function": "        def on_update(d):\n            self.headers['Content-Type'] = \\\n                dump_options_header(self.mimetype, d)", "label": 1}
{"function": "        def fget(self):\n            def on_update(header_set):\n                if not header_set and name in self.headers:\n                    del self.headers[name]\n                elif header_set:\n                    self.headers[name] = header_set.to_header()\n            return parse_set_header(self.headers.get(name), on_update)", "label": 1}
{"function": "        def fset(self, value):\n            if not value:\n                del self.headers[name]\n            elif isinstance(value, string_types):\n                self.headers[name] = value\n            else:\n                self.headers[name] = dump_header(value)", "label": 1}
{"function": "        def on_update(www_auth):\n            if not www_auth and 'www-authenticate' in self.headers:\n                del self.headers['www-authenticate']\n            elif www_auth:\n                self.headers['WWW-Authenticate'] = www_auth.to_header()", "label": 1}
{"function": "    def createTempUrl(self, content, suffix, path=None, relativeTo=None,\n            pythonUrl=False):\n        \"\"\"\n        Specialized function. Creates a file, fills it with content \n        (byte string), closes it and returns its URL.\n        relativeTo -- path relative to which the URL should be or None\n            for preferred relativeTo path or \"\" for absolute URL\n        \"\"\"\n        fullPath = self.createTempFile(content, suffix, path, \"\")\n        \n        return self.getRelativeUrl(relativeTo, fullPath, pythonUrl=pythonUrl)", "label": 1}
{"function": "    def __init__(self, serialport):\n        self.serialport = serialport\n        self.logger = logging.getLogger('umap2')\n        self.reset()", "label": 1}
{"function": "    def halt(self):\n        self.serialport.setRTS(1)\n        self.serialport.setDTR(1)", "label": 1}
{"function": "    def reset(self, count=10):\n        self.logger.info('Facedancer resetting...')\n        for i in range(count):\n            self.halt()\n            self.serialport.setDTR(0)\n            rsp_data = self.read(1024)\n            if len(rsp_data) < 4:\n                continue\n            app, verb, n = struct.unpack('<BBH', rsp_data[:4])\n            if verb == 0x7f and n == (len(rsp_data) - 4):\n                self.logger.debug(\"No buffer any more\")\n                self.logger.info(\"Facedancer reset\")\n                return\n        raise Exception(\"Facedancer reset fault.\")", "label": 1}
{"function": "    def read(self, n):\n        '''Read raw bytes.'''\n        b = self.serialport.read(n)\n        self.logger.verbose('Facedancer received %s bytes; %s bytes remaining' % (len(b), self.serialport.inWaiting()))\n        self.logger.verbose('Facedancer Rx: %s' % hexlify(b))\n        return b", "label": 1}
{"function": "    def readcmd(self):\n        '''Read a single command.'''\n\n        b = self.read(4)\n        app, verb, n = struct.unpack('<BBH', b)\n\n        if n > 0:\n            data = self.read(n)\n        else:\n            data = b''\n\n        if len(data) != n:\n            raise ValueError('Facedancer expected %d bytes but received only %d' % (n, len(data)))\n        cmd = FacedancerCommand(app, verb, data)\n        self.logger.verbose('Facedancer Rx command: %s' % cmd)\n        return cmd", "label": 1}
{"function": "    def write(self, b):\n        '''Write raw bytes.'''\n        self.logger.verbose('Facedancer Tx: %s' % hexlify(b))\n        self.serialport.write(b)", "label": 1}
{"function": "    def writecmd(self, c):\n        '''Write a single command.'''\n        self.write(c.as_bytestring())\n        self.logger.verbose('Facedancer Tx command: %s' % c)", "label": 1}
{"function": "    def __init__(self, app=None, verb=None, data=None):\n        self.app = app\n        self.verb = verb\n        self.data = data", "label": 1}
{"function": "    def __str__(self):\n        s = 'app 0x%02x, verb 0x%02x, len %d' % (self.app, self.verb, len(self.data))\n\n        if len(self.data) > 0:\n            s += ', data %s' % hexlify(self.data)\n\n        return s", "label": 1}
{"function": "  def _log_prob(self, x):\n    x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if\n                                           self.validate_args else [], x)\n    contrib_tensor_util.assert_same_float_dtype(tensors=[x],\n                                                dtype=self.dtype)\n    return (self.alpha * math_ops.log(self.beta) +\n            (self.alpha - 1.) * math_ops.log(x) -\n            self.beta * x -\n            math_ops.lgamma(self.alpha))", "label": 1}
{"function": "  def _prob(self, x):\n    return math_ops.exp(self._log_prob(x))", "label": 1}
{"function": "  def _log_cdf(self, x):\n    x = control_flow_ops.with_dependencies([check_ops.assert_positive(x)] if\n                                           self.validate_args else [], x)\n    contrib_tensor_util.assert_same_float_dtype(tensors=[x], dtype=self.dtype)\n    # Note that igamma returns the regularized incomplete gamma function,\n    # which is what we want for the CDF.\n    return math_ops.log(math_ops.igamma(self.alpha, self.beta * x))", "label": 1}
{"function": "  def _cdf(self, x):\n    return math_ops.igamma(self.alpha, self.beta * x)", "label": 1}
{"function": "  def _entropy(self):\n    return (self.alpha -\n            math_ops.log(self.beta) +\n            math_ops.lgamma(self.alpha) +\n            (1. - self.alpha) * math_ops.digamma(self.alpha))", "label": 1}
{"function": "  def _mean(self):\n    return self.alpha / self.beta", "label": 1}
{"function": "  def _variance(self):\n    return self.alpha / math_ops.square(self.beta)", "label": 1}
{"function": "  def _std(self):\n    return math_ops.sqrt(self.alpha) / self.beta", "label": 1}
{"function": "  def _mode(self):\n    mode = (self.alpha - 1.) / self.beta\n    if self.allow_nan_stats:\n      nan = np.array(np.nan, dtype=self.dtype.as_numpy_dtype())\n      return math_ops.select(\n          self.alpha >= 1.,\n          mode,\n          array_ops.fill(self.batch_shape(), nan, name=\"nan\"))\n    else:\n      return control_flow_ops.with_dependencies([\n          check_ops.assert_less(\n              array_ops.ones((), self.dtype),\n              self.alpha,\n              message=\"mode not defined for components of alpha <= 1\"),\n          ], mode)", "label": 1}
{"function": "  def __init__(self,\n               alpha,\n               beta,\n               validate_args=False,\n               allow_nan_stats=True,\n               name=\"GammaWithSoftplusAlphaBeta\"):\n    with ops.name_scope(name, values=[alpha, beta]) as ns:\n      super(GammaWithSoftplusAlphaBeta, self).__init__(\n          alpha=nn.softplus(alpha),\n          beta=nn.softplus(beta),\n          validate_args=validate_args,\n          allow_nan_stats=allow_nan_stats,\n          name=ns)", "label": 1}
{"function": "    def table_psf_in_energy_band(self, energy_band, spectrum=None, n_bins=11, **kwargs):\n        \"\"\"Average PSF in a given energy band.\n\n        Expected counts in sub energy bands given the given exposure\n        and spectrum are used as weights.\n\n        Parameters\n        ----------\n        energy_band : `~astropy.units.Quantity`\n            Energy band\n        spectrum : `~gammapy.modeling.models.SpectralModel`\n            Spectral model used for weighting the PSF. Default is a power law\n            with index=2.\n        n_bins : int\n            Number of energy points in the energy band, used to compute the\n            weigthed PSF.\n\n        Returns\n        -------\n        psf : `TablePSF`\n            Table PSF\n        \"\"\"\n        from gammapy.modeling.models import PowerLawSpectralModel, TemplateSpectralModel\n\n        if spectrum is None:\n            spectrum = PowerLawSpectralModel()\n\n        exposure = TemplateSpectralModel(self.energy, self.exposure)\n\n        e_min, e_max = energy_band\n        energy = MapAxis.from_energy_bounds(e_min, e_max, n_bins).edges\n\n        weights = spectrum(energy) * exposure(energy)\n        weights /= weights.sum()\n\n        psf_value = self.evaluate(energy=energy)\n        psf_value_weighted = weights[:, np.newaxis] * psf_value\n        return TablePSF(self.rad, psf_value_weighted.sum(axis=0), **kwargs)", "label": 1}
{"function": "    def containment_radius(self, energy, fraction=0.68):\n        \"\"\"Containment radius.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        fraction : float\n            Containment fraction.\n\n        Returns\n        -------\n        rad : `~astropy.units.Quantity`\n            Containment radius in deg\n        \"\"\"\n        # upsamle for better precision\n        rad_max = Angle(np.linspace(0, self.rad[-1].value, 10 * len(self.rad)), \"rad\")\n        containment = self.containment(energy=energy, rad_max=rad_max)\n\n        # find nearest containment value\n        fraction_idx = np.argmin(np.abs(containment - fraction), axis=1)\n        return rad_max[fraction_idx].to(\"deg\")", "label": 1}
{"function": "    def containment(self, energy, rad_max):\n        \"\"\"Compute containment of the PSF.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energy\n        rad_max : `~astropy.coordinates.Angle`\n            Maximum offset angle.\n\n        Returns\n        -------\n        fraction : array_like\n            Containment fraction (in range 0 .. 1)\n        \"\"\"\n        energy = np.atleast_1d(u.Quantity(energy))[:, np.newaxis]\n        rad_max = np.atleast_1d(u.Quantity(rad_max))\n        return self._interpolate_containment((energy, rad_max))", "label": 1}
{"function": "    def info(self):\n        \"\"\"Print basic info\"\"\"\n        print(str(self))", "label": 1}
{"function": "    def plot_psf_vs_rad(self, energies=None, ax=None, **kwargs):\n        \"\"\"Plot PSF vs radius.\n\n        Parameters\n        ----------\n        energy : `~astropy.units.Quantity`\n            Energies where to plot the PSF.\n        **kwargs : dict\n            Keyword arguments pass to `~matplotlib.pyplot.plot`.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        if energies is None:\n            energies = [100, 1000, 10000] * u.GeV\n\n        ax = plt.gca() if ax is None else ax\n\n        for energy in energies:\n            psf_value = np.squeeze(self.evaluate(energy=energy))\n            label = f\"{energy:.0f}\"\n            ax.plot(\n                self.rad.to_value(\"deg\"),\n                psf_value.to_value(\"sr-1\"),\n                label=label,\n                **kwargs,\n            )\n\n        ax.set_yscale(\"log\")\n        ax.set_xlabel(\"Offset (deg)\")\n        ax.set_ylabel(\"PSF (1 / sr)\")\n        plt.legend()\n        return ax", "label": 1}
{"function": "    def plot_containment_vs_energy(\n        self, ax=None, fractions=[0.68, 0.8, 0.95], **kwargs\n    ):\n        \"\"\"Plot containment versus energy.\"\"\"\n        import matplotlib.pyplot as plt\n\n        ax = plt.gca() if ax is None else ax\n\n        for fraction in fractions:\n            rad = self.containment_radius(self.energy, fraction)\n            label = f\"{100 * fraction:.1f}% Containment\"\n            ax.plot(self.energy.value, rad.value, label=label, **kwargs)\n\n        ax.semilogx()\n        ax.legend(loc=\"best\")\n        ax.set_xlabel(\"Energy (GeV)\")\n        ax.set_ylabel(\"Containment radius (deg)\")", "label": 1}
{"function": "    def plot_exposure_vs_energy(self):\n        \"\"\"Plot exposure versus energy.\"\"\"\n        import matplotlib.pyplot as plt\n\n        plt.figure(figsize=(4, 3))\n        plt.plot(self.energy, self.exposure, color=\"black\", lw=3)\n        plt.semilogx()\n        plt.xlabel(\"Energy (MeV)\")\n        plt.ylabel(\"Exposure (cm^2 s)\")\n        plt.xlim(1e4 / 1.3, 1.3 * 1e6)\n        plt.ylim(0, 1.5e11)\n        plt.tight_layout()", "label": 1}
{"function": "    def stack(self, psf):\n        \"\"\"Stack two EnergyDependentTablePSF objects.s\n\n        Parameters\n        ----------\n        psf : `EnergyDependentTablePSF`\n            PSF to stack.\n\n        Returns\n        -------\n        stacked_psf : `EnergyDependentTablePSF`\n            Stacked PSF.\n\n        \"\"\"\n        exposure = self.exposure + psf.exposure\n        psf_value = self.psf_value.T * self.exposure + psf.psf_value.T * psf.exposure\n\n        with np.errstate(invalid=\"ignore\"):\n            # exposure can be zero\n            psf_value = np.nan_to_num(psf_value / exposure)\n\n        return self.__class__(\n            energy=self.energy, rad=self.rad, psf_value=psf_value.T, exposure=exposure\n        )", "label": 1}
{"function": "    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url)\n        video_id = mobj.group('id')\n        display_id = mobj.group('display_id') or video_id\n\n        video = self._download_json(\n            'https://api.byutv.org/api3/catalog/getvideosforcontent',\n            display_id, query={\n                'contentid': video_id,\n                'channel': 'byutv',\n                'x-byutv-context': 'web$US',\n            }, headers={\n                'x-byutv-context': 'web$US',\n                'x-byutv-platformkey': 'xsaaw9c7y5',\n            })\n\n        ep = video.get('ooyalaVOD')\n        if ep:\n            return {\n                '_type': 'url_transparent',\n                'ie_key': 'Ooyala',\n                'url': 'ooyala:%s' % ep['providerId'],\n                'id': video_id,\n                'display_id': display_id,\n                'title': ep.get('title'),\n                'description': ep.get('description'),\n                'thumbnail': ep.get('imageThumbnail'),\n            }\n\n        info = {}\n        formats = []\n        for format_id, ep in video.items():\n            if not isinstance(ep, dict):\n                continue\n            video_url = url_or_none(ep.get('videoUrl'))\n            if not video_url:\n                continue\n            ext = determine_ext(video_url)\n            if ext == 'm3u8':\n                formats.extend(self._extract_m3u8_formats(\n                    video_url, video_id, 'mp4', entry_protocol='m3u8_native',\n                    m3u8_id='hls', fatal=False))\n            elif ext == 'mpd':\n                formats.extend(self._extract_mpd_formats(\n                    video_url, video_id, mpd_id='dash', fatal=False))\n            else:\n                formats.append({\n                    'url': video_url,\n                    'format_id': format_id,\n                })\n            merge_dicts(info, {\n                'title': ep.get('title'),\n                'description': ep.get('description'),\n                'thumbnail': ep.get('imageThumbnail'),\n                'duration': parse_duration(ep.get('length')),\n            })\n        self._sort_formats(formats)\n\n        return merge_dicts(info, {\n            'id': video_id,\n            'display_id': display_id,\n            'title': display_id,\n            'formats': formats,\n        })", "label": 1}
{"function": "   def init(self):\n      self.DATALEN = 256 * 2**10\n      self.PAYLOAD = \"\\x00\\xfe\\x23\\xfa\\xf0\"\n      self.WAITSECS = 10\n      self.reportTime = True", "label": 1}
{"function": "    def test_from_array_int_event(surv_arrays):\n        event, time = surv_arrays\n\n        expected = numpy.empty(dtype=[('event', bool), ('time', float)], shape=100)\n        expected['event'] = event.astype(bool)\n        expected['time'] = time\n\n        y = Surv.from_arrays(event, time)\n        assert_array_equal(y, expected)", "label": 1}
{"function": "    def test_from_array_int_time(surv_arrays):\n        event, time = surv_arrays\n        time += 1\n        time *= time\n\n        expected = numpy.empty(dtype=[('event', bool), ('time', float)], shape=100)\n        expected['event'] = event.astype(bool)\n        expected['time'] = time.astype(int)\n\n        y = Surv.from_arrays(event.astype(bool), time.astype(int))\n        assert_array_equal(y, expected)", "label": 1}
{"function": "    def test_from_array_float(surv_arrays):\n        event, time = surv_arrays\n\n        expected = numpy.empty(dtype=[('event', bool), ('time', float)], shape=100)\n        expected['event'] = event.astype(bool)\n        expected['time'] = time\n\n        y = Surv.from_arrays(event.astype(float), time)\n        assert_array_equal(y, expected)", "label": 1}
{"function": "    def test_from_array_shape_mismatch(surv_arrays):\n        event, time = surv_arrays\n\n        msg = \"Found input variables with inconsistent numbers of samples\"\n        with pytest.raises(ValueError, match=msg):\n            Surv.from_arrays(event[1:], time)\n\n        with pytest.raises(ValueError, match=msg):\n            Surv.from_arrays(event, time[1:])", "label": 1}
{"function": "    def test_from_array_event_value_wrong_1(surv_arrays):\n        event, time = surv_arrays\n        event += 1\n\n        with pytest.raises(ValueError,\n                           match=\"non-boolean event indicator must contain 0 and 1 only\"):\n            Surv.from_arrays(event, time)", "label": 1}
{"function": "    def test_from_array_event_value_wrong_2(surv_arrays):\n        event, time = surv_arrays\n        event -= 1\n\n        with pytest.raises(ValueError,\n                           match=\"non-boolean event indicator must contain 0 and 1 only\"):\n            Surv.from_arrays(event, time)", "label": 1}
{"function": "    def test_from_array_event_value_wrong_3(surv_arrays):\n        event, time = surv_arrays\n        event[event == 0] = 3\n\n        with pytest.raises(ValueError,\n                           match=\"non-boolean event indicator must contain 0 and 1 only\"):\n            Surv.from_arrays(event, time)", "label": 1}
{"function": "    def test_from_array_event_value_wrong_4(surv_arrays):\n        event, time = surv_arrays\n        event[1] = 3\n\n        with pytest.raises(ValueError,\n                           match=\"event indicator must be binary\"):\n            Surv.from_arrays(event, time)", "label": 1}
{"function": "    def test_from_array_event_value_wrong_5(surv_arrays):\n        event, time = surv_arrays\n        event = numpy.arange(event.shape[0])\n\n        with pytest.raises(ValueError,\n                           match=\"event indicator must be binary\"):\n            Surv.from_arrays(event, time)", "label": 1}
{"function": "    def test_from_array_names_match(surv_arrays):\n        event, time = surv_arrays\n\n        with pytest.raises(ValueError,\n                           match=\"name_time must be different from name_event\"):\n            Surv.from_arrays(event, time,\n                             name_event='time_and_event', name_time='time_and_event')", "label": 1}
